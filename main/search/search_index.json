{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Azimuth Documentation!","text":"<p>Azimuth is an open source application that helps AI practitioners and data scientists better understand their dataset and model predictions by performing thorough dataset and error analyses. The application leverages different tools, including robustness tests, semantic similarity analysis and saliency maps, unified by concepts such as smart tags and proposed actions.</p> <p>While this version of Azimuth focuses on NLP classification problems, the tool could easily be adapted to apply to other data types and models, e.g. vision or tabular use cases. However, the current focus is on text classification.</p>"},{"location":"#youtube-playlist","title":"YouTube Playlist","text":""},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li> Getting Started contains all the instructions to    install and launch the app. It also details the changelog of our releases.</li> <li> Key Concepts explains the different concepts and   analyses that are provided in Azimuth to perform dataset and error analysis.</li> <li> User Guide goes screen per screen to explain the   different   interactions and visualizations available.</li> <li> Reference details the config file and the custom objects   which allow configuring Azimuth with different datasets and pipelines.</li> <li> Development guides on how to develop and contribute to   the repo.</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>Join   the Slack channel    to ask questions and engage with the community.</li> <li>File issues in   our GitHub repo .</li> <li>Learn how to contribute   in CONTRIBUTING.md.</li> </ul>"},{"location":"about-us/","title":"About Us","text":"<p>We started developing Azimuth  at the beginning of 2021, as an internal tool at ServiceNow. We are open source since April 2022. Our motivation in developing this tool is to go beyond the traditional model quality assessment based solely on high-level metrics. We strongly believe that much more can be discovered and understood from our dataset and models if we have the right tools, visualizations, and interfaces. We hope this can be helpful to you, and that you enjoy Azimuth! \ud83d\ude03</p>"},{"location":"about-us/#the-team","title":"The Team","text":""},{"location":"about-us/#main-contributors","title":"Main Contributors","text":""},{"location":"about-us/#frederic-branchaud-charron","title":"Fr\u00e9d\u00e9ric Branchaud-Charron","text":"<p>Fred works as an Applied Research Scientist specializing in Bayesian deep learning, active learning and uncertainty estimation. In addition to maintaining Azimuth, he also maintains BaaL, a Bayesian active learning library.</p>"},{"location":"about-us/#gabrielle-gauthier-melancon","title":"Gabrielle Gauthier-Melan\u00e7on","text":"<p>Gab is an Applied Research Scientist, interested in explainability, uncertainty and topics related to building trust in AI. Since the beginning of Azimuth, she is leading the product ideation, while being involved in designing the user interface, documenting it, and maintaining the back end.</p>"},{"location":"about-us/#joseph-marinier","title":"Joseph Marinier","text":"<p>Joseph is a full-stack developer who enjoys designing and engineering solutions to a large variety of problems. His main contribution to Azimuth has been leading the frontend development.</p>"},{"location":"about-us/#lindsay-brin","title":"Lindsay Brin","text":"<p>Lindsay is an Applied Research Scientist working in explainability and NLU, who loves the concept of representing language in mathematical space. Her journey through data analysis, modeling, and visualization started with research in biogeochemistry and ecosystem ecology.</p>"},{"location":"about-us/#chris-tyler","title":"Chris Tyler","text":"<p>Chris is a software developer with a background in physics and an interest in everything. He has contributed to the design and development of the Azimuth user interface.</p>"},{"location":"about-us/#nandhini-babu","title":"Nandhini Babu","text":"<p>Nandhini is a developer at ServiceNow who joined Azimuth in January 2022. She's mostly contributing on the front end. Her overall experience is around web development.</p>"},{"location":"about-us/#designers","title":"Designers","text":"<p>We can also count on the support of amazing designers.</p> <p></p> <p></p> <p></p>"},{"location":"about-us/#di-le","title":"Di Le","text":"<p>Di Le is an AI/ML design strategist, and a contributor to the intelligent automation of enterprise software at ServiceNow. Her work focus surrounds human-centered AI design and creating systems that augment, automate and accelerate how people work.</p>"},{"location":"about-us/#karine-grande","title":"Karine Grande","text":"<p>Karine is a Product Designer working on experiences that use AI/ML technology. In addition of helping on the design side of Azimuth, she worked on projects about data labelling, classification, summarization and forecasting, and is currently working on document processing and data extraction.</p>"},{"location":"about-us/#nikola-simic","title":"Nikola Simic","text":"<p>Nikola is a Senior Product Designer working on products that leverage responsible and ethical AI and Machine Learning across the enterprise space. He created the Azimuth visual identity and is developing Azimuth's branding and video materials.</p>"},{"location":"about-us/#other-contributors","title":"Other Contributors","text":""},{"location":"about-us/#orlando-marquez","title":"Orlando Marquez","text":"<p>Orlando is a Lead Applied Research Scientist with a strong background in software engineering. One of his passions is shipping state-of-the-art NLP to end-users through rigorous and careful experimentation. His early contributions to Azimuth revolved around saliency maps for NLP and similarity-based analysis.</p>"},{"location":"about-us/#michael-lanoie","title":"Michael Lanoie","text":"<p>Michael is a technical writer working on product documentation for AI/ML software at ServiceNow. He contributed as a content editor and collaborator, helping improve the UX and the documentation.</p>"},{"location":"about-us/#sean-hughes","title":"Sean Hughes","text":"<p>Sean leads cross-functional AI ecosystem strategy and engagement at ServiceNow, bringing his experience in AI developer community development to help the team launch and drive adoption of Azimuth.</p>"},{"location":"about-us/#special-thanks","title":"Special Thanks","text":"<p>Thank you Sethu Meiyappan, and Francis Ruel for helping us on the path to open sourcing!</p>"},{"location":"development/","title":"Development","text":"<p>This section documents how to get started when setting up your environment before contributing to Azimuth.</p> <p>Please see our  CONTRIBUTING.md for guidelines on how to contribute.</p> <p>Just want to launch the app locally without Docker?</p> <p>You will need <code>poetry</code> and <code>yarn</code> installed, as explained in  Initial Setup. Then go the  Launching section.</p>"},{"location":"development/#development-documentation-structure","title":"Development Documentation Structure","text":"<ul> <li> Initial Setup<ul> <li>Steps to install Backend and Frontend dependencies.</li> </ul> </li> <li> Launching<ul> <li>Steps to launch Azimuth for development.</li> </ul> </li> <li> Development Practices<ul> <li>Information on our best practices regarding branch names, testing and pull-requests.</li> </ul> </li> </ul>"},{"location":"development/dev-practices/","title":"Development Pratices","text":""},{"location":"development/dev-practices/#git-branches","title":"Git Branches","text":"<ul> <li><code>main</code>: Integrates new features, documentation can be lacking. Users will run this branch if they run locally. If they use Docker, they will use our latest release and won't be affected by the latest changes on <code>main</code>.</li> <li><code>release/*</code>: Branches that can be created to release a version, whenever <code>main</code> might contain additional commits which should not be part of the immediate release. <code>release/*</code> should be merged in <code>main</code> after.</li> </ul> <p>We don't have a convention for the names of regular branches for developing features and fixes.</p> <p>Each version has its own tag (<code>v2.0.0</code> for example) and release package on Github.</p>"},{"location":"development/dev-practices/#committing","title":"Committing","text":"<p>We try to commit regularly, so it is easy to revert partial changes.</p>"},{"location":"development/dev-practices/#front-end-types","title":"Front End Types","text":"<p>If you played with types and routes in the back end, remember to regenerate the front-end types, using the following command from the webapp folder while the back end is running (see how to launch here). <pre><code>cd webapp\nyarn types   # while the back end is runnnig\n</code></pre></p>"},{"location":"development/dev-practices/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>When you commit, the pre-commit hooks will automatically be run to format the code included in the commit. We have different hooks which will run <code>mypy</code>, reorder imports, clean unnecessary white-spacing, and perform typical formatting with <code>black</code>, <code>flake8</code> and <code>prettier</code>.</p> <ul> <li>In some cases, you might need to format using <code>make format</code>. However, in most cases, pre-commit hooks will make the necessary changes automatically.</li> <li><code>mypy</code> errors will never prevent a commit, but it will print the errors so that you can fix     them if you introduced a new one.</li> </ul> <p>How to temporarily disable pre-commits?</p> <p>For a specific commit, you can avoid running the pre-commits with the flag <code>--no-verify</code>. If you do this, make sure to run <code>pre-commit run --all-files</code> before opening a PR, which will run the pre-commit on all files.</p>"},{"location":"development/dev-practices/#testing","title":"Testing","text":"<p>It might be that all tests don't pass at each commit, and this is normal. Tests can take a while to run, and so you might want to run them only at specific moments, such as before opening a new PR. When ready to run tests, you can do the following commmands. <pre><code>make format   # It will fix formatting issues.\nmake test     # It will check for formatting issues, run mypy and all unit tests.\n</code></pre></p> <p>You can also only run specific tests, using <code>poetry run pytest {path_to_test.py}</code>.</p> <p>Clean cache for routers tests</p> <p>For tests in <code>tests/test_routers</code>, we run the startup task once and save the result in <code>/tmp/azimuth_test_cache</code>. When modifying Modules, you need to clean it manually or by running <code>make clean</code>.</p> <p>Regularly launch the app</p> <p>Even when all unit tests pass, it may happen that some new issues arise when launching the app. When doing important code changes, it is crucial to launch the app and see if the behavior is as expected. Details on launching the app are available here.</p>"},{"location":"development/dev-practices/#opening-a-pull-request-pr","title":"Opening a Pull Request (PR)","text":"<p>We have a template for when you open a PR. Follow instructions and check the boxes to acknowledge that you have done the required steps. Regarding the <code>CHANGELOG.md</code>, you will need to add your changes only if it is visible for the end-users. This helps us when building the release notes.</p>"},{"location":"development/launching/","title":"Launching","text":"<p>The app can be launched on different data and models, which are defined in a config file. Different config files are readily available in the repo, under <code>config/</code>.</p> <p>TLDR to launch the app locally</p> <p>Assuming you already installed <code>poetry</code> and <code>yarn</code> (See  Initial Setup), you can run the app by performing: <pre><code>make local_configs  # modify all configs for local usage\nmake CFG_PATH={path to config file} launch.local  # Start the backend.\n</code></pre></p> <p><code>{path to config file}</code> should be the path in <code>local_configs</code>. Ex for clinc dummy: <code>local_configs/development/clinc_dummy/conf.json</code></p> <p>In another terminal: <pre><code>cd webapp &amp;&amp; yarn start # Starts the frontend.\n</code></pre></p>"},{"location":"development/launching/#launching-locally","title":"Launching locally","text":"<p>When debugging, it is easier to launch the back end and the front end separately, as shown in the next two sections. However, it is also possible to launch both together using <code>Docker</code>, as shown in the third section below. It will just take longer, and does not allow for fast debugging.</p>"},{"location":"development/launching/#back-end","title":"Back End","text":"<ul> <li>Config files need to be regenerated for local development.   <pre><code>make local_configs\n</code></pre>   This will create a new folder <code>/local_configs</code> under the project root and will perform the necessary config updates to run locally.</li> <li> <p>You can then launch the back end using:   <pre><code>make CFG_PATH={path to config file} launch.local\n</code></pre></p> <ul> <li>For <code>{path to config file}</code>, use the path to the file in <code>local_configs</code> rather than <code>configs</code>. With <code>clinc_dummy</code>, this will be:   <pre><code>make CFG_PATH=local_configs/development/clinc_dummy/conf.json launch.local\n</code></pre></li> </ul> Error in the Back End? <p>If you get an error while launching the back end, common causes can be that the <code>poetry</code> has new dependencies or the configs were changed. Be sure to run: <pre><code>poetry install --extras cpu\nmake local_configs\n</code></pre></p> If you get <code>No module named '_lzma'</code>... <p>You may be missing the <code>xz</code> library (https://tukaani.org/xz/). That would result in a <code>ModuleNotFoundError: No module named '_lzma'</code> when you try to run locally. This may be corrected by using homebrew to install it. The following instructions are inspired from those. <pre><code>brew install xz\npyenv uninstall 3.8.9  # If you already had it - otherwise directly go to the steps\npyenv install 3.8.9  # Reinstall (now with the lzma lib available)\npyenv local 3.8.9  # Set this version to always run in this directory\n</code></pre></p> </li> </ul> <p>From this point, the back end will launch and compute the start-up task.</p> <ul> <li>You can consult the openapi documentation at <code>localhost:8091/docs</code>. From there, you can consult the API documentation and try out the different endpoints. This can be useful for debugging.</li> <li>Note that the back end will not reload automatically based on code changes.</li> </ul> <p>Cleaning the Cache</p> <p>If you make changes to back-end modules that result in different module responses, you will need to delete the cache, using: <pre><code>make clean\n</code></pre> As soon as you have an error in the start-up task, a good reflex is to use this command. Optionally, you can clean a single project by supplying the project name as an argument. <pre><code>make TARGET=CLINC clean\n</code></pre> will delete all cache folders beginning with <code>CLINC</code>.</p>"},{"location":"development/launching/#front-end","title":"Front End","text":"<p>You can then launch the front end using: <pre><code>cd webapp\nyarn        # Only needed when dependencies were updated.\nyarn start\n</code></pre></p> <ul> <li>The front-end app will launch and automatically connect to the back-end API, assuming you launched it already.</li> <li>The front end will hot reload automatically based on code changes.</li> </ul> Error in the Front End? <p>If you get an error while launching the front end, make sure that the dependencies were updated using <code>yarn</code>.</p>"},{"location":"development/launching/#launch-using-docker","title":"Launch Using Docker","text":"<ul> <li>Docker compose will build the images and connect them, using the following command.   <pre><code>make CFG_PATH=/config/examples/.../conf.json launch\n</code></pre> Note that the path starts with <code>/config</code> as we mount <code>./config:/config</code>.</li> </ul>"},{"location":"development/launching/#launch-multiple-instances","title":"Launch multiple instances","text":"<p>You can launch multiple instances of Azimuth (e.g., to compare effects of code changes) by specifying front-end and back-end ports.</p> <p>To specify the back-end port, start the back end using:   <pre><code>make CFG_PATH={path to config file} PORT={back-end port} launch.local\n</code></pre></p> <p>To connect the front end to this back end, start the front end using:   <pre><code>REACT_APP_BACKEND_PORT={back-end port} PORT={front-end port} yarn start\n</code></pre></p> <p>Use appropriate port numbers</p> <p>Make sure that your back-end ports match for both commands, and that you specify ports that are not already in use. When ports are not specified, the defaults are 8091 for the back end and 3000 for the front end.</p>"},{"location":"development/setup/","title":"Initial Setup","text":""},{"location":"development/setup/#install-dependencies","title":"Install Dependencies","text":""},{"location":"development/setup/#install-pyenv-macos","title":"Install <code>pyenv</code> (MacOS)","text":"<p>We use <code>python 3.8.9</code> for the back end in our repo, but <code>python &gt;= 3.8</code> should work in general. You should use <code>pyenv</code> to set up the right <code>python</code> version for the project.</p> <ul> <li>Install <code>brew</code> (if not already installed).</li> <li>Install the <code>xz</code> library (if not already installed).     <pre><code>brew install xz\n</code></pre></li> <li>Then install <code>pyenv</code> and set the right <code>python</code> version.     <pre><code>brew install pyenv\npyenv install 3.8.9\npyenv local 3.8.9      # You can use global or local.\n</code></pre></li> <li>Add this to your <code>~/.zshrc</code>:     <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre></li> <li>Run <code>zsh</code> to open a new shell (with the updated <code>~/.zshrc</code>).     <pre><code>zsh\n</code></pre></li> <li> <p>Check that the version of <code>python</code> is the same as you set to <code>pyenv local</code>.     <pre><code>python --version\n</code></pre></p> If it doesn't match... <p>It might be due to a wrong ordering in your <code>$PATH</code>. Print it and make sure that <code>$PYENV_ROOT/bin</code> is at the beginning. Edit <code>~/.zshrc</code> accordingly. <pre><code>echo $PATH\n</code></pre></p> </li> </ul>"},{"location":"development/setup/#install-poetry","title":"Install <code>Poetry</code>","text":"<p>We use <code>poetry</code> as our dependency manager for the back end.</p> <ul> <li>Install <code>poetry</code>.</li> <li> <p>Check that the version of python that <code>poetry</code> uses is the same as you set to <code>pyenv local</code>.     <pre><code>poetry run python --version\n</code></pre></p> If it prints a warning mentioning another python version... <p>You might need to force <code>poetry</code> to use the right python environment. <pre><code>poetry env use $(which python)\n</code></pre></p> </li> <li> <p>Install the dependencies.     <pre><code>poetry install --extras cpu\n</code></pre></p> </li> </ul>"},{"location":"development/setup/#docker-optional","title":"Docker (Optional)","text":"<p>Docker is needed for different tasks such as releasing and updating the documentation. However, it won't be needed at first to develop and launch the app. You can skip this step and wait until you are required to use it.</p> <ul> <li>Install <code>Docker Desktop</code>. If you are using a   Mac, check \"Apple logo\" &gt; \"About This Mac\" to know if you have a <code>Mac with Intel Chip</code> or   a <code>Mac with Apple Chip</code>.</li> <li>Set the memory to at least 9 GB in Docker &gt; Preference &gt; Resources.</li> </ul>"},{"location":"development/setup/#front-end","title":"Front End","text":"<ul> <li> <p>Install <code>Node.js</code> version 16. If you need to use different versions of Node on your machine, you may be interested in NVM. Otherwise, the simplest way to install Node on you Mac is to use <code>Homebrew</code> (and follow directions to add this version   to your PATH).</p> <pre><code>brew install node@16\n</code></pre> </li> <li> <p>Once you've installed <code>Node</code>, you can install <code>yarn</code>.</p> <pre><code>npm install -g yarn\n</code></pre> </li> <li> <p>You can then install front-end dependencies, from the webapp folder, using:</p> <pre><code>cd webapp\nyarn\n</code></pre> </li> </ul>"},{"location":"development/setup/#setting-up-the-development-environment","title":"Setting up the development environment","text":""},{"location":"development/setup/#install-ide","title":"Install IDE","text":"<ul> <li>If developing in back end, we recommend installing PyCharm:<ul> <li>Install PyCharm.</li> <li>In PyCharm preferences, you can add your existing python virtual environment as the \"Python Interpreter\", pointing where what <code>poetry run which python</code> prints.</li> </ul> </li> <li>If developing in front end, we recommend installing Visual Studio Code:<ul> <li>Install Visual Studio Code</li> <li>A pre-defined configuration is available in the repo, to help development. Two things to do:<ul> <li>View &gt; Extensions &gt; search for <code>esbenp.prettier-vscode</code> and install it. That's the official <code>Code formatter using prettier</code> by publisher <code>Prettier</code>.</li> <li>File &gt; Open Workspace from File &gt; select <code>azimuth.code-workspace</code>, which will set up two folders: <code>webapp</code> and <code>.</code> (for the rest). In the <code>webapp</code> folder, <code>webapp/.vscode/settings.json</code> will configure Prettier to format your files on save.</li> </ul> </li> </ul> </li> </ul>"},{"location":"development/setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We installed pre-commit hooks which will format automatically your code with each commit. The first time, you need to run the following command. <pre><code>poetry run pre-commit install\n</code></pre></p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section goes through all the steps to get from installing the repo to launching Azimuth on a dataset and models.</p> <ol> <li>Installation details how to install the requirements.</li> <li>Learn Basics describes the application repo and the different steps needed based    on your dataset and model format.</li> <li>Run on Your Use Case goes through the steps on how to prepare your config file and    run Azimuth on a simple use case.</li> </ol>"},{"location":"getting-started/a-install/","title":"A. Installation","text":"<p>Follow the steps to install the necessary items, and download Azimuth.</p>"},{"location":"getting-started/a-install/#requirements","title":"Requirements","text":"<ol> <li>Install Docker .<ul> <li>Docker allows to use our application without installing all the dependencies.</li> <li>You can also use Azimuth without <code>Docker</code> by installing our dependencies with <code>poetry</code>   and <code>yarn</code>. It is slightly more complex, but it can be faster to launch Azimuth. The   instructions are in Development.</li> </ul> </li> <li>[Mac Only] Setup Docker so it has enough memory to run Azimuth.<ol> <li>Right-click on the Docker icon  on the toolbar of your computer.</li> <li>Go to Resources and set the \u201cMemory\u201d slider to 9Gb or more.</li> <li>Click on \u201cApply and Restart\u201d.</li> </ol> </li> <li>Download Azimuth's latest release.<ol> <li>On GitHub , git clone our repo.</li> </ol> </li> </ol> <p>Installation completed</p> <p>Congratulations! You have installed the necessary items and installed Azimuth. Proceed to Learn Basics.</p>"},{"location":"getting-started/b-basics/","title":"B. Learn Basics","text":""},{"location":"getting-started/b-basics/#understanding-the-application-folder","title":"Understanding the Application Folder","text":"<p>In the downloaded Azimuth folder from step A, the following structure exists:</p> <pre><code>azimuth  # Root directory\n\u251c\u2500\u2500 azimuth\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 # Back End # (7)\n\u251c\u2500\u2500 azimuth_shr\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 # User dataset, models, and code # (1)\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 # User configs # (2)\n\u251c\u2500\u2500 webapp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 # Front End # (8)\n\u251c\u2500\u2500 .app_env  # (3)\n\u251c\u2500\u2500 docker-compose.yml  # (4)\n\u251c\u2500\u2500 Makefile  # (5)\n\u2514\u2500\u2500 README.md # (6)\n</code></pre> <ol> <li>Where to put your data, model and code, if relevant. <code>azimuth_shr</code> stands for    azimuth shared, because it contains user artifacts that are shared with the application. Azimuth    provides default artifacts already to load common dataset and models.</li> <li>Where to put all configs. Example configs are provided.</li> <li>Default values for env vars.</li> <li>Where the config and Docker images are specified.</li> <li>Available commands to use Azimuth.</li> <li>Instructions to launch the application.</li> <li>Only available when cloning the repo.</li> <li>Only available when cloning the repo.</li> </ol> <p>Where to put your data, code and configs?</p> <p><code>config</code> and <code>azimuth_shr</code> are two folders where you will put different artifacts before you can run Azimuth on your dataset and models. They get mounted automatically on the Docker image.</p>"},{"location":"getting-started/b-basics/#the-config-file","title":"The Config File","text":"<p>The Azimuth config file contains all the information to launch and customize Azimuth. It specifies which dataset and pipelines to load in the app, as well as other variables that allow for customization of the app. Most fields have default values and don't need to be explicitly defined in each config. The  Configuration reference details all available fields.</p> <p>Different configuration examples are provided in the repo under <code>config/examples</code>, leveraging pretrained models from HuggingFace. The next step,  C. Run on Your Use Case, will detail how to adapt an existing config to create your own.</p>"},{"location":"getting-started/b-basics/#clearing-the-cache","title":"Clearing the Cache","text":"<p>Azimuth keeps all artifacts in caching folders so that if you close the app and re-launch, it will load quickly. Once you are done with your analysis, you can delete the cache by running:</p> <pre><code>make clean\n</code></pre>"},{"location":"getting-started/b-basics/#run-our-demo-to-verify-your-setup","title":"Run Our Demo to Verify Your Setup","text":"<p>Out-of-the-box, Azimuth can run on different demo data and models from HuggingFace (HF). Verify that your setup is working correctly by running a demo.</p> <ol> <li> <p>In the terminal, from the <code>azimuth</code> folder (the root directory), execute the following commands.    The first one installs the Google Drive downloading library. The second command downloads from    Google Drive the demo data and model. Our demo is using a subset of    the <code>clinc_oos</code> dataset from HuggingFace, with only    16 classes.     <pre><code>pip install gdown\nmake download_demo\n</code></pre></p> You cannot install <code>gdown</code>? <p>Look at the following  Discussion to download the data manually.</p> </li> <li> <p>Run our dummy or full demo (option a. or b.), based on how much time you have. If it is the    first time that you are running the command, it will take additional time to download the Docker    image (~15 min).</p> <ol> <li>If you don't have a lot of time and just want to verify your setup, you can run our dummy    CLINC demo (~2min):    <pre><code>make CFG_PATH=/config/development/clinc_dummy/conf.json launch\n</code></pre></li> <li>If you have a bit more time, run our full CLINC demo (~10min):    <pre><code>make CFG_PATH=/config/development/clinc/conf.json launch\n</code></pre></li> </ol> </li> <li> <p>The app will be accessible at http://localhost:8080 after a few minutes    of waiting. The screen will indicate that the start-up tasks have started. When it is completed,    the application will be loaded.</p> </li> <li>Skim the  Key Concepts section to get a high-level    understanding of some concepts used throughout the application. If you are unsure what each    screen allows you to do, the  User Guide section walks    you through all the available interactions on each screen.</li> </ol> <p>Successful demo</p> <p>Now that the demo is working, you can adapt the config to make it work on your dataset and model. Proceed to  C. Run on Your Use Case.</p>"},{"location":"getting-started/c-run/","title":"C. Run on Your Use Case","text":"<p>This page guides you through the process of running the app on your data and pipelines, using Docker. Different dataset and text classification models can be supported in Azimuth.</p> <p>Launch Azimuth with no pipeline, or with multiple pipelines</p> <p>Azimuth supports specifying no pipelines, to only perform dataset analysis. It also supports supplying mulitple pipelines, to allow for quick comparison. However, only one dataset per config is allowed.</p> <p>The simplest scenario is if you have a HuggingFace (HF) dataset and model. For the sake of simplicity, we explain the instructions to run the app with this scenario. However, you will quickly need to learn about the  Configuration details and  Custom Objects to launch more complex use cases.</p>"},{"location":"getting-started/c-run/#1-prepare-the-config-file","title":"1. Prepare the Config File","text":"<p>Run our demo first</p> <p>You haven't run our demo yet? You might want to verify your setup before feeding your own model and dataset. Go back to B. Learn Basics.</p> <p>Start from an existing config  and edit the relevant fields to adapt it to your dataset and models. Examples with an HuggingFace (HF) dataset and model are available in <code>config/examples</code> (<code>CLINC</code> is also shown below).</p> <ol> <li>Put your model checkpoint (results    of .save_pretained())    under the folder <code>azimuth_shr</code>.</li> <li>In <code>config</code>, copy <code>config/examples/clinc_oos/conf.json</code> to a new folder with your project    name. Ex: <code>config/my_project/conf.json</code>.</li> <li> <p>Edit the config:</p> <ol> <li><code>name</code>: put your project name.</li> <li><code>dataset.args</code>: specify the args required to load your dataset    with <code>datasets.load_dataset</code>.</li> <li>Edit <code>columns</code> and <code>rejection_class</code> based on the dataset.</li> <li><code>pipelines.models.kwargs.checkpoint_path</code>: put your own checkpoint path to your model. The    path should start with <code>/azimuth_shr</code>, since this folder will be mounted on Docker.</li> <li>Edit the <code>saliency_layer</code> so it is the name of the input layer of the model. It should be set    to <code>null</code> if your model is not from PyTorch or without a word-embedding layer.</li> </ol> <p>Links to full reference</p> <p>If you need more details on some of these fields:</p> <ul> <li>The  Project Config explains in     more details <code>name</code>, <code>dataset</code>, <code>columns</code> and <code>rejection_class</code>.</li> <li>The  Model Contract Config     details how to define <code>pipelines</code>, <code>model_contract</code> and <code>saliency_layer</code>.</li> </ul> </li> </ol> <pre><code>{\n\"name\": \"CLINC150\", # (1)\n\"dataset\": {\n\"class_name\": \"datasets.load_dataset\", # (2)\n\"args\": [ # (3)\n\"clinc_oos\",\n\"imbalanced\"\n]\n},\n\"columns\": { # (4)\n\"text_input\": \"text\",\n\"label\": \"intent\"\n},\n\"rejection_class\": \"oos\", # (5)\n\"model_contract\": \"hf_text_classification\", # (6)\n\"pipelines\": [ # (7)\n{\n\"model\": {\n\"class_name\": \"loading_resources.load_hf_text_classif_pipeline\", # (8)\n\"remote\": \"/azimuth_shr\", # (9)\n\"kwargs\": { # (10)\n\"checkpoint_path\": \"transformersbook/\ndistilbert-base-uncased-distilled-clinc\"\n}\n}\n}\n],\n\"saliency_layer\": \"distilbert.embeddings.word_embeddings\", # (11)\n}\n</code></pre> <ol> <li>Name for your project. Shown in the application to identify your config.</li> <li>If the dataset is a <code>HF</code> dataset, use this <code>class_name</code>.</li> <li><code>kwargs</code> to send to the <code>class_name</code>.</li> <li>Specify the name of the dataset columns, such as the column with the utterance and the label.</li> <li>Specify the value if a rejection option is present in the classes.</li> <li>If the pipeline is a <code>HF</code> pipeline, use this <code>model_contract</code>.</li> <li>Multiples ML pipelines can be listed to be available in the webapp.</li> <li>If this a <code>HF</code> pipeline, use this <code>class_name</code>.</li> <li>Change only if <code>class_name</code> is not found in <code>/azimuth_shr</code>.</li> <li><code>kwargs</code> to send to the class. Only <code>checkpoint_path</code> if you use the class above.</li> <li>Name of the layer on which to compute saliency maps.</li> </ol>"},{"location":"getting-started/c-run/#2-running-the-app","title":"2. Running the App","text":"<ol> <li>In the terminal, go to the <code>azimuth</code> root directory.</li> <li>Set <code>CFG_PATH=/config/my_project/conf.json</code> with the location of the config.<ul> <li>The initial <code>/</code> is required as your local config folder will be mounted on the Docker   container at the root.</li> </ul> </li> <li>Execute the following command:     <pre><code>make launch\n</code></pre></li> <li>The app will be accessible at <code>http://localhost:8080</code> after a few minutes of waiting. The    start-up tasks will start.</li> </ol>"},{"location":"getting-started/c-run/#advanced-settings","title":"Advanced Settings","text":""},{"location":"getting-started/c-run/#additional-config-fields","title":"Additional Config Fields","text":"<p>The   Configuration reference details all additional fields that can be set, such as changing how behavioral tests are executed, the similarity analysis encoder, the batch size and so on.</p>"},{"location":"getting-started/c-run/#environment-variables","title":"Environment variables","text":"<p>No matter where you launch the app from, you can always configure some options through environment variables. They are all redundant with the config attributes, so you can set them in either place. They are the following:</p> <ul> <li>Specify the threshold of your model by passing <code>TH</code> (ex: <code>TH=0.6</code> or <code>NaN</code> if there is no   threshold) in the command. If multiple pipelines are defined, the threshold will apply to all.</li> <li>Similarly, pass <code>TEMP=Y</code> (ex: <code>TEMP=3</code>) to set the temperature of the model.</li> <li>Disable behavioral tests and similarity by passing respectively <code>BEHAVIORAL_TESTING=null</code> and   <code>SIMILARITY=null</code>.</li> <li>Specify the name of the project, passing <code>NAME</code>.</li> <li>You can specify the device on which to run Azimuth, with <code>DEVICE</code> being one of <code>auto</code>, <code>gpu</code> or <code>cpu</code>. If   none is provided, <code>auto</code> will be used. Ex: <code>DEVICE=gpu</code>.</li> <li>Specify <code>READ_ONLY_CONFIG=1</code> to lock the config once Azimuth is launched.</li> </ul> <p>Config file prevails over environment variables</p> <p>Remember that the values above are defined in the config too. If conflicting values are defined, values from the config file will prevail.</p>"},{"location":"getting-started/changelog/","title":"Releases","text":""},{"location":"getting-started/changelog/#252-2022-12-20","title":"[2.5.2] - 2022-12-20","text":""},{"location":"getting-started/changelog/#fixed","title":"Fixed","text":"<ul> <li>Show long utterances fully on hover in similar and perturbed utterances tables.</li> <li>Fixed webapp crash when there is no pipeline (with <code>\"pipeline\": null</code> configured).</li> <li>Fixed sort utterance table by confidence or prediction without post-processing.</li> <li>Fixed crash on Safari and iOS browsers as they don't support lookbehind in regular expressions.</li> </ul>"},{"location":"getting-started/changelog/#251-2022-12-05","title":"[2.5.1] - 2022-12-05","text":""},{"location":"getting-started/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Ignore <code>language</code> environment variable as we have seen conflicts with it on certain machines.</li> </ul>"},{"location":"getting-started/changelog/#250-2022-12-05","title":"[2.5.0] - 2022-12-05","text":""},{"location":"getting-started/changelog/#added","title":"Added","text":"<ul> <li>Support for french: Azimuth now works on French datasets (and pipelines)! Language can be selected in the config, and language-specific defaults for syntax-tagging and behavioral tests (neutral tokens) will be set dynamically (or can be altered manually). See Reference.</li> <li>Class Overlap: New class overlap detection section. More details are available in the User Guide and in the Key Concepts.</li> <li>Pre/Post-processing Steps: The details of the pipeline pre/post-processing steps are now visible in the utterance details page.</li> <li>Accuracy metric: Accuracy was added as a default metric.</li> </ul>"},{"location":"getting-started/changelog/#changed","title":"Changed","text":"<ul> <li>The default port for the front end will now be 3000, the React default, instead of 3005.</li> </ul>"},{"location":"getting-started/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>ONNX models on GPU.</li> <li>Preserve white spaces in utterances. That includes <code>\\n</code>s, <code>\\t</code>s, and consecutive spaces.</li> <li>Fix predictions with 100% confidence not showing up in the confidence histogram and in the ECE.</li> </ul>"},{"location":"getting-started/changelog/#241-2022-10-21","title":"[2.4.1] - 2022-10-21","text":""},{"location":"getting-started/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed Smart Tab Analysis table<ul> <li>Column headers exceeding plots' width</li> <li>Links to Prediction Overview</li> <li><code>Total</code> row squished when transposed and with a high number of rows</li> </ul> </li> </ul>"},{"location":"getting-started/changelog/#240-2022-10-20","title":"[2.4.0] - 2022-10-20","text":""},{"location":"getting-started/changelog/#added_1","title":"Added","text":"<ul> <li>New dataset warning: Added new class imbalance warnings.</li> <li>Pipeline Comparison: Added a new pipeline comparison mode in the pipeline metrics table to compare the metrics on different pipelines.</li> <li>New Smart Tag Analysis: Added a new plot where smart tag patterns over classes can be easily examined in one view.</li> <li>New loading screen: When the start-up tasks are running, a new loading screen displays the status of all the tasks.</li> </ul>"},{"location":"getting-started/changelog/#changed_1","title":"Changed","text":"<ul> <li>Renaming: Some sections were renamed in the UI, such as:<ul> <li>Dataset Class Distribution Analysis -&gt; Dataset Warnings</li> <li>Performance Analysis -&gt; Pipeline Metrics by Data Subpopulation</li> <li>Performance Overview -&gt; Prediction Overview</li> </ul> </li> <li>Proposed actions: We added a new action, <code>merge_classes</code>, and renamed <code>consider_new_class</code> to <code>define_new_class</code>.</li> <li>Improved Confusion Matrix: The order of the classes in the confusion matrix is now smarter: classes where the model gets similarly confused will be closer to one another. The rejection class is always the last row/column in the confusion matrix. A toggle allows the user to keep the original order from the dataset if preferred.</li> <li>Refactoring: We improved the <code>MetricsPerFilter</code> module (which generates the pipeline metrics by data subpopulation table). It now takes ~5 times less time to compute.</li> <li>New config fields: The memory of the dask cluster can now be set to large (12GB) for bigger models. The config can also be in read-only mode, to prevent users from changing its values.</li> <li>Offline Mode: Azimuth can now be launched without internet.</li> </ul>"},{"location":"getting-started/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed an issue related to HuggingFace where filtering on an empty dataset would result in an error.</li> </ul>"},{"location":"getting-started/changelog/#230-2022-08-17","title":"[2.3.0] - 2022-08-17","text":""},{"location":"getting-started/changelog/#added_2","title":"Added","text":"<ul> <li>Rows of Performance Analysis table now link to exploration page with filters applied, when the user clicks.</li> <li>Added visual bars to the similarity column in the semantically similar utterances.</li> <li>New attribute in the config allows users to change the thresholds that determine a short or long sentence.</li> </ul>"},{"location":"getting-started/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed utterances table poorly showing ids greater than 9999.</li> <li>Fixed filtering of aggregation modules without post-processing.</li> <li>Fixed high_epistemic_uncertainty smart tag which wasn't showing in the UI.</li> <li>Fixed crash when hitting <code>See more</code> when it would show over 100 rows in Performance Analysis table.</li> </ul>"},{"location":"getting-started/changelog/#223-2022-07-25","title":"[2.2.3] - 2022-07-25","text":""},{"location":"getting-started/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fixed losing hidden columns from Performance Analysis table when changing view (Label, Prediction, etc.).</li> <li>Fixed utterances table poorly showing ids greater than 99, now supporting up to 9999.</li> </ul>"},{"location":"getting-started/changelog/#222-2022-07-19","title":"[2.2.2] - 2022-07-19","text":""},{"location":"getting-started/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed losing hidden columns when sorting Performance Analysis table, clicking <code>See more</code>/<code>less</code>, or switching dataset split or pipeline.</li> </ul>"},{"location":"getting-started/changelog/#221-2022-07-15","title":"[2.2.1] - 2022-07-15","text":""},{"location":"getting-started/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Fixed pipeline comparison smart tag family not working as a filter on the Exploration space.</li> </ul>"},{"location":"getting-started/changelog/#220-2022-07-08","title":"[2.2.0] - 2022-07-08","text":""},{"location":"getting-started/changelog/#added_3","title":"Added","text":"<ul> <li>Contextual information has been added to most functionalities, linking directly to the relevant documentation.</li> <li>The F1 metric was added as a default metric.</li> </ul>"},{"location":"getting-started/changelog/#changed_2","title":"Changed","text":"<ul> <li>Smart tags are now grouped in families, so it is easier to digest them. This also impacts how filters are displayed.</li> <li>The confusion matrix can be shown with raw values (not just normalized).</li> <li>We now show the top 20, instead of 10, most similar examples in the utterances details.</li> <li>Renamed our docker images to use <code>servicenowdocker</code> registry on Docker Hub.</li> </ul>"},{"location":"getting-started/changelog/#performance-overview-table","title":"Performance Overview Table","text":"<ul> <li>Columns can be temporarily hidden.</li> <li>Use outcomes' icons as headers instead of the outcomes' full names.</li> <li>Added visual bars, so it is easier to analyze the model's performance.</li> </ul>"},{"location":"getting-started/changelog/#211-2022-06-06","title":"[2.1.1] - 2022-06-06","text":""},{"location":"getting-started/changelog/#changed_3","title":"Changed","text":"<ul> <li>Our Docker images are now available through Docker Hub<ul> <li>Renamed our docker images to use <code>servicenowdocker</code> registry on Docker Hub.</li> <li>Users can now use <code>make launch</code> instead of <code>make compose</code>.</li> </ul> </li> <li>Add option <code>DEVICE=auto</code> to automatically run Azimuth on GPU if available. As such, <code>DEVICE=cpu</code>   or <code>DEVICE=gpu</code> does not need to be specified.</li> <li>We added analytics in our documentation, which will add a pop-up to accept cookies when users   first access the page.</li> </ul>"},{"location":"getting-started/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Fixed issue where the performance analysis table showed empty results for some smart tags.</li> <li>Fixed <code>BEHAVIORAL_TESTING</code> environment variable.</li> <li>Fixed unexpected borders in Performance Analysis table.</li> <li>Fixed proposed actions which couldn't be applied on an utterance in the latest release.</li> </ul>"},{"location":"getting-started/changelog/#210-2022-05-27","title":"[2.1.0] - 2022-05-27","text":"<ul> <li>Ability to get predictions without postprocessing in the exploration space. See section \"Excluding   Post-Processing\" here.</li> <li>New Smart Tags <code>pipeline_disagreement</code> and <code>incorrect_for_all_pipelines</code> as a first step for   pipeline comparison. See section \"Pipeline Comparison\" here.</li> <li>Links on top words to filter utterances that contain it. See the section \"Word   Clouds\" here.</li> </ul>"},{"location":"getting-started/changelog/#200-2022-04-12","title":"[2.0.0] - 2022-04-12","text":"<p>First public release.</p>"},{"location":"key-concepts/","title":"Key Concepts","text":"<p>Azimuth leverages different analyses and concepts to enhance the process of dataset analysis and error analysis.</p> <p>The notion of smart tags is the most important concept, as it unifies most of the other analyses.</p>"},{"location":"key-concepts/#smart-tags","title":"Smart Tags","text":"<p>Smart tags are assigned to utterances by Azimuth when the app is launched. They can be seen as meta-data on the utterance and/or its prediction. The goal is to guide the error analysis process, identifying interesting data samples which may require further action and investigation. Different families of smart tags exist, based on the different types of analyses that Azimuth provides.</p> <p>Smart tag examples</p> <p>Examples of smart tag families:</p> <ul> <li>partial syntax: identifies utterances with a partial syntax, e.g. missing a verb.</li> <li>behavioral testing: identifies utterances which failed at least one behavioral test.</li> </ul> <p>Examples of individual smart tags:</p> <ul> <li><code>long_sentences</code> identifies utterances with more than X tokens.</li> <li><code>failed_punctuation</code> identifies utterances that failed at least one punctuation test.</li> </ul> <p>The full list of smart tags is available in  Smart Tags.</p>"},{"location":"key-concepts/#proposed-actions","title":"Proposed Actions","text":"<p>While smart tags are computed automatically and cannot be changed, proposed actions are annotations that can be added by the user to identify a proposed action that should be done on a specific data sample.</p> <p>Proposed action examples</p> <ul> <li><code>relabel</code> to identify data samples whose labels should be changed.</li> <li><code>remove</code> to identify data samples that should be removed from the dataset.</li> </ul> <p>A dedicated page on  Proposed Actions gives the full list of available actions.</p>"},{"location":"key-concepts/#prediction-outcomes","title":"Prediction Outcomes","text":"<p>Another key concept used through the application is the notion of prediction outcomes. It acts as a metric of success for a given prediction. More details are available in  Prediction outcomes.</p> <ul> <li> Correct &amp; Predicted</li> <li> Correct &amp; Rejected</li> <li> Incorrect &amp; Rejected</li> <li> Incorrect &amp; Predicted</li> </ul>"},{"location":"key-concepts/#analyses","title":"Analyses","text":"<p>In Azimuth, different types of analysis are provided. Each analysis has a dedicated section in the documentation. Almost all of them (except saliency maps) are linked to smart tags.</p> <ol> <li> Saliency Maps</li> <li> Syntax Analysis</li> <li> Similarity Analysis</li> <li> Behavioral Testing</li> <li> Uncertainty Estimation</li> </ol>"},{"location":"key-concepts/behavioral-testing/","title":"Behavioral Testing","text":""},{"location":"key-concepts/behavioral-testing/#what-is-it","title":"What is it?","text":"<p>Performing behavioral testing on ML models was first introduced in the checklist paper (Ribeiro, Marco Tulio, et al., 20201). Behavioral tests provide an assessment of the model robustness to small modifications to the input. Proper behavioral testing can help in detecting bias or other potential harmful aspects of the model that may not be otherwise obvious.</p>"},{"location":"key-concepts/behavioral-testing/#where-is-this-used-in-azimuth","title":"Where is this used in Azimuth?","text":"<p>In Azimuth, behavioral tests are automatically executed when launching the tool, using the provided dataset and model.</p> <ul> <li>The details of all the tests that were computed for a given utterance are shown in the    Utterance Details.</li> <li>A summary of each test for all utterances in both dataset splits (training and evaluation) is   available in the    Behavioral Testing Summary.</li> <li>Finally, a  Smart Tag is generated for each utterance for which at   least one test of each family has failed.</li> </ul>"},{"location":"key-concepts/behavioral-testing/#how-is-it-computed","title":"How is it computed?","text":"<p>The tests are deterministic for reproducibility purposes.</p>"},{"location":"key-concepts/behavioral-testing/#test-failing-criteria","title":"Test Failing Criteria","text":"<p>The tests can fail for two reasons.</p> <ul> <li> <p>The test will fail if the predicted class for the modified utterance is different from the   predicted class of the original utterance.</p> Failing examples Original Utterance Predicted Class Azimuth is the best tool <code>positive</code> Modified Utterance Predicted Class Test fails? Hello Azimuth is the best tool <code>positive</code> NO Azimuth is the best tool!!! <code>negative</code> YES </li> <li> <p>The test will fail if the confidence associated with the predicted class of the modified   utterance is too different (based on a threshold) from the confidence of the original   utterance. By default, the threshold is set to 1, meaning the tests will never fail due to a   change in confidence for the same predicted class.</p> Failing examples Original Utterance Predicted Class Confidence Azimuth is the best tool <code>positive</code> 95% <p>Threshold set to 0.1:</p> Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool <code>positive</code> 82% YES <p>Threshold set to 1:</p> Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool <code>positive</code> 82% NO </li> </ul>"},{"location":"key-concepts/behavioral-testing/#available-tests","title":"Available Tests","text":"<p>All tests are invariant (the modification should not change the predicted class) and assess the robustness of the model.</p> <ul> <li>The tool currently has 2 families of tests: <code>Fuzzy Matching</code> and <code>Punctuation</code>.</li> <li>For each test, different modification types can be applied (<code>Insertion</code>, <code>Deletion</code>, etc.)<ul> <li>For certain tests, all modification types are applied to each utterance (e.g., <code>Typos</code> and   <code>Neutral Token</code>).</li> <li>For others, only one modification type is applied based on the presence of a certain pattern   in the utterance (e.g., <code>Punctuation</code> and <code>Contractions</code> tests).</li> </ul> </li> </ul>"},{"location":"key-concepts/behavioral-testing/#fuzzy-matching","title":"Fuzzy Matching","text":"<ul> <li> <p><code>Typos</code>: For this test, we simulate common typos that might happen when typing an utterance. By   default, the test creates one typo per utterance. The different types of simulated typos (   modification types) are:</p> <ul> <li><code>Swap</code>: Random swap of two adjacent characters in a word.</li> <li><code>Deletion</code>: Deletion of random characters in a word.</li> <li><code>Replacement</code>: Keyboard proximity-based typos inserted in a word.</li> </ul> </li> <li> <p><code>Neutral Token</code>: Default neutral tokens are added to the utterance.</p> <ul> <li><code>PreInsertion</code>: One string from a list of prefixes is added at the beginning of the   utterance. The English default is [\"pls\", \"please\", \"hello\", \"greetings\"].</li> <li><code>PostInsertion</code>: One of string from a list of suffixes is added at the end of an utterance.   The English default is [\"pls\", \"please\", \"thank you\", \"appreciated\"].</li> </ul> </li> <li> <p><code>Contractions</code>: This test is applied only when the utterance contains a relevant expression that   can be contracted or expanded. The list is taken from   NL-Augmenter   .</p> <ul> <li><code>Contraction</code>: Contract relevant expressions, if present.</li> <li><code>Expansion</code>: Expand relevant expressions, if present.</li> </ul> </li> </ul>"},{"location":"key-concepts/behavioral-testing/#punctuation","title":"Punctuation","text":"<ul> <li> <p><code>Question Mark</code>: Adds/Deletes/Replaces question marks.</p> <ul> <li><code>Deletion</code>: Removes the ending question mark, if present.</li> <li><code>Replacement</code>: Replaces the ending punctuation sign ('.', '!', ','), if present, by a question   mark.</li> <li><code>PostInsertion</code>: Adds an ending question mark when the utterance does not end with a   punctuation sign.</li> </ul> </li> <li> <p><code>Ending period</code>: Same logic as the <code>Question Mark</code> test, with a period.</p> <ul> <li><code>Deletion</code>: Removes the ending period, if present.</li> <li><code>Replacement</code>: Replaces the ending punctuation sign ('?', '!', ','), if present, by a period.</li> <li><code>PostInsertion</code>: Adds an ending period when the utterance does not end with a punctuation   sign.</li> </ul> </li> <li> <p><code>Inner Comma</code>: Adds/Deletes comma inside the utterance (not at the end).</p> <ul> <li><code>Deletion</code>: Removes all commas inside the utterance, if present.</li> <li><code>Insertion</code>: Adds a comma near the middle of the utterance.</li> </ul> </li> <li> <p><code>Inner Period</code>: Same logic as the <code>Inner Comma</code> test, with a period.</p> <ul> <li><code>Deletion</code>: Removes all periods inside the utterance, if present.</li> <li><code>Insertion</code>: Adds a period near the middle of the utterance.</li> </ul> </li> </ul>"},{"location":"key-concepts/behavioral-testing/#configuration","title":"Configuration","text":"<p> Behavioral Testing Configuration details how to change some parameters, such as the lists of neutral tokens, the number of typos and the threshold confidence delta above which the tests should fail.</p> <ol> <li> <p>Ribeiro, Marco Tulio, et al. \"Beyond accuracy: Behavioral testing of NLP models with CheckList.\" Association for Computational Linguistics (ACL), 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"key-concepts/outcomes/","title":"Prediction Outcomes","text":"<p>Based on the predicted class and the label, Azimuth provides Outcomes, an assessment of the correctness of a given prediction. Outcomes are determined for individual utterances as a metric of success, and are also used across the application as an overall metric. These outcomes align with some business outcomes: not all errors (or correct predictions) are equal.</p>"},{"location":"key-concepts/outcomes/#rejection-class","title":"Rejection Class","text":"<p>Many ML problems have the notion of a rejection class, sometimes called the abstention class. This is the class that indicates the absence of a prediction. In some use cases, the model can predict it directly as one of its classes, and in other scenarios, it is only predicted when the  confidence of a model is below a given threshold. In Azimuth, the rejection class needs to be defined in the config, as explained in the  Project Configuration.</p> <p>From a business outcome, it is usually less costly to not predict anything than to predict the wrong class. This is why the notion of rejection is present in the outcomes, as explained below.</p>"},{"location":"key-concepts/outcomes/#outcomes","title":"Outcomes","text":"<p>Azimuth defines 4 different outcomes:</p> <ul> <li> Correct &amp; Predicted<ul> <li>When the predicted class matches the label, and is not the rejection class.</li> <li>These are the predictions that add the most value.</li> </ul> </li> <li> Correct &amp; Rejected<ul> <li>When the predicted class matches the label, but is the rejection class.</li> <li>These predictions are correct, but do not necessarily bring a lot of value, given that   the data sample does not come from one of the more meaningful classes.</li> </ul> </li> <li> Incorrect &amp; Rejected<ul> <li>When the predicted class does not match the label, but the rejection class has been   predicted.</li> <li>These predicted are incorrect, but not as costly as when the wrong class is predicted.</li> </ul> </li> <li> Incorrect &amp; Predicted<ul> <li>When the predicted class does not match the label, and is not the rejection class.</li> <li>These predicted are incorrect and are the most costly.</li> </ul> </li> </ul> <p>The colors of the outcomes are used throughout the application to show how many utterances of each of the outcomes are present in different aggregations.</p>"},{"location":"key-concepts/proposed-actions/","title":"Proposed Actions","text":"<p>In the utterance table or the utterance details, annotations can be added to indicate whether an action should be taken for each data sample. The annotations can be exported with the dataset from the  Utterances Table.</p> <p> </p> Proposed Actions in the Utterances Table of the Exploration Space <p>Five proposed actions are currently supported:</p> <ul> <li>Relabel: The label for this utterance should be changed.</li> <li>Augment with Similar: This utterance lacks nearby utterances in the same class, and new,   similarly labeled utterances should be added.</li> <li>Define New Class: This data point may belong to a new class that doesn't exist in the   currently defined classes. Based on the number of data points identified with this action, a user   may choose to add a new class.</li> <li>Merge Classes: The label and the predicted class for this data point may be too similar. Based on the number of data points identified with this action, a user may choose to merge the two classes.</li> <li>Remove: This data point is irrelevant or otherwise problematic, and should be removed from the   dataset.</li> <li>Investigate: The action is not yet clear, but the data point should be further   investigated/discussed, within Azimuth or outside the tool.</li> </ul>"},{"location":"key-concepts/saliency/","title":"Saliency Maps","text":""},{"location":"key-concepts/saliency/#what-is-it","title":"What is it?","text":"<p>A saliency map is a feature-based explainability (XAI) method that is available for gradient-based ML models. Its role is to estimate how much each variable contributes to the model prediction for a given data sample. In the case of NLP, variables are usually tokens or words.</p>"},{"location":"key-concepts/saliency/#literature","title":"Literature","text":"<p>In NLP, the current state-of-the-art models (Transformers) are black boxes. It\u2019s not trivial to understand why they make mistakes or why they are right.</p> <p>Saliency maps first became popular for computer vision problems, where the results would be a heatmap of the most contributing pixels. There has been recent growing interest in saliency maps as an XAI technique for NLP:</p> <ul> <li>Wallace et al.1 in their AllenNLP toolkit implement vanilla   gradient, integrated gradients, and smoothGrad for several NLP tasks and mostly pre-BERT models.</li> <li>Han et al.2 use gradient-based saliency maps for sentiment   analysis and NLI on BERT.</li> <li>Atanasova et al.3 evaluate different saliency techniques on   a variety of models including BERT-based models.</li> <li>Bastings and Filippova4 argue for using saliency maps over   attention-based explanations when determining the input tokens most relevant to a prediction.   Their end-user is a model developer, rather than a user of the system.</li> </ul> <p>Other XAI techniques</p> <p>Apart from saliency maps, other feature-based XAI techniques exist, such as SHAP or LIME.</p>"},{"location":"key-concepts/saliency/#where-is-this-used-in-azimuth","title":"Where is this used in Azimuth?","text":"<p>In Azimuth, we display a saliency map over a specific utterance to show the importance of each token to the model prediction. When available, it is both displayed in the Utterance Details and in the Utterances Table.</p> <p> </p> Saliency map of a specific utterance. <p>Saliency example</p> <p>In this example, <code>bill</code> is the word that contributes the most to the prediction <code>bill_balance</code>.</p>"},{"location":"key-concepts/saliency/#how-is-it-computed","title":"How is it computed?","text":"<p>We use the technique <code>Vanilla Gradient</code>, shown to satisfy input invariance in Kindermans et al.5 We simply backpropagate the gradient to the input layer of the network: in our case, the word-embedding layer. We then take the L2 norm to aggregate the gradients across all dimensions of the layer to determine the saliency value for each token.</p> <p>Saliency maps are only supported for certain models</p> <p>Saliency maps are only available for models that have gradients. Additionally, their input layer needs to be a token-embedding layer, so that the gradients can be computed per token. For example, a sentence embedder cannot back-propagate the gradients with sufficient granularity in the utterance.</p>"},{"location":"key-concepts/saliency/#configuration","title":"Configuration","text":"<p>Assuming the model architecture allows for saliency maps, the name of the input layer needs to be defined in the config file, as detailed in Model Contract Configuration.</p> <ol> <li> <p>Wallace, Eric, et al. \"Allennlp interpret: A framework for explaining predictions of nlp models.\" arXiv preprint arXiv:1909.09251 (2019).\u00a0\u21a9</p> </li> <li> <p>Han, Xiaochuang, Byron C. Wallace, and Yulia Tsvetkov. \"Explaining black box predictions and unveiling data artifacts through influence functions.\" arXiv preprint arXiv:2005.06676 (2020).\u00a0\u21a9</p> </li> <li> <p>Atanasova, Pepa, et al. \"A diagnostic study of explainability techniques for text classification.\" arXiv preprint arXiv:2009.13295 (2020).\u00a0\u21a9</p> </li> <li> <p>Bastings, Jasmijn, and Katja Filippova. \"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?.\" arXiv preprint arXiv:2010.05607 (2020).\u00a0\u21a9</p> </li> <li> <p>Kindermans, Pieter-Jan, et al. \"The (un) reliability of saliency methods.\" Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280.\u00a0\u21a9</p> </li> </ol>"},{"location":"key-concepts/similarity/","title":"Similarity Analysis","text":""},{"location":"key-concepts/similarity/#what-is-it","title":"What is it?","text":"<p>Similarity analysis is based on the relative locations of utterances in embedding space. This analysis can be quite powerful given that no trained ML model is needed; only a dataset needs to be supplied.</p> <p>Within Azimuth, different similarity analyses are provided to determine how similar utterances are within a class, between classes, and so on. This can help indicate whether classes are well-defined, or whether changes should be made to improve the dataset, such as by redefining classes, relabeling or omitting data, or augmenting the dataset.</p>"},{"location":"key-concepts/similarity/#where-is-this-used-in-azimuth","title":"Where is this used in Azimuth?","text":"<p>In Azimuth, the similarity analysis is used to derive  Smart Tags, and also to show the most similar utterances in both dataset splits on the  Utterances Details (see below).</p> <p>Similarity is also used for class overlap, which assesses the semantic overlap between pairs of classes. Class overlap is presented in the Class Overlap Dashboard Section as well as the  Class Overlap page.</p> <p> </p> Similar utterances in the Utterance Details."},{"location":"key-concepts/similarity/#how-is-it-computed","title":"How is it Computed?","text":""},{"location":"key-concepts/similarity/#similarity-computation","title":"Similarity Computation","text":"<p>To get utterance embeddings, Azimuth uses a sentence encoder (from sentence-transformers) based on a BERT architecture (Reimers and Gurevych, 20191). It then computes the cosine similarity (via a dot product on normalized embeddings) between each utterance in the dataset and all other utterances in both dataset splits (training and evaluation).</p> <p>On the  Utterances Details, the most similar examples are presented in descending order (i.e., most similar first), along with the cosine similarity to the selected utterance. A cosine similarity of 1 indicates that the utterance is identical, while 0 indicates that it is unrelated.</p>"},{"location":"key-concepts/similarity/#smart-tag-family-dissimilar","title":"Smart Tag Family: Dissimilar","text":""},{"location":"key-concepts/similarity/#no-close-tags","title":"No Close Tags","text":"<p>Some utterances may have no close neighbors in a dataset split - that is, their most similar utterances have low cosine similarities. When the cosine similarity of an utterance's closest neighbor is below a threshold (default = 0.5), the utterance gets tagged with <code>no_close_train</code> and/or <code>no_close_eval</code>, according to the dataset split being assessed (training or evaluation). Note that this tag is class label-agnostic.</p>"},{"location":"key-concepts/similarity/#conflicting-neighbors-tags","title":"Conflicting Neighbors Tags","text":"<p>It can be useful to assess whether the most similar data samples to an utterance (its neighbors) come from the same or different classes. When most of its neighboring utterances are from a different class, it might indicate a mislabeling issue, overlapping classes, data drift, or simply a difficult utterance to predict.</p> <p>Two  Smart Tags highlight these sorts of utterances, based on the label heterogeneity of the neighborhood in each dataset split (training or evaluation). If 90% or more of an utterance's most similar data samples (neighbors) in a dataset split belong to a different class, it will be tagged as <code>conflicting_neighbors_train</code> and/or <code>conflicting_neighbors_eval</code>, based on which dataset split is being examined. (E.g., an utterance in the test set will be compared to its neighbors in both the training and evaluation dataset splits.)</p>"},{"location":"key-concepts/similarity/#class-overlap","title":"Class Overlap","text":""},{"location":"key-concepts/similarity/#class-overlap-value","title":"Class Overlap Value","text":"<p>Class overlap is calculated using utterance embeddings, which are computed as described above.</p> <p>Class overlap for class Ci (source class) with class Cj (target class) is defined as the area of the feature (embedding) space in which an utterance in class Ci has a greater probability of being in class Cj than in class Ci.</p> <p>To approximate this probability, we make use of the <code>spectral-metric</code> package (Branchaud-Charron, 20192). The probability of a sample being in a specified class is determined based on the representation of this class in the sample's 5 nearest neighbors, as well as the hypervolume containing these neighbors (Parzen window). Class overlap for the Ci with the Cj is calculated as the mean probability across all samples in Ci. The similarity matrix S from <code>spectral-metric</code> contains these probabilities for all class pairs. Note that probabilities are normalized by the source class, to sum to 1.</p>"},{"location":"key-concepts/similarity/#samples-with-overlap","title":"Samples with overlap","text":"<p>Individual samples from a source class are determined to have overlap with a target class when their probability of being in the target class is greater than 0, which is the same as saying that at least one of their 5 nearest neighbors are from the target class. This is a conservative metric, on which we anticipate iterating in the future.</p>"},{"location":"key-concepts/similarity/#configuration","title":"Configuration","text":"<p> Similarity Analysis Configuration presents language-based defaults for the encoder used for the embeddings on which similarity is computed, and details how to change this encoder as well as the two thresholds used to determine the smart tags.</p> <ol> <li> <p>Reimers, Nils, and Iryna Gurevych. \"Sentence-bert: Sentence embeddings using siamese bert-networks.\" arXiv preprint arXiv:1908.10084 (2019).\u00a0\u21a9</p> </li> <li> <p>Branchaud-Charron, Frederic, Andrew Achkar, and Pierre-Marc Jodoin. \"Spectral metric for dataset complexity assessment.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"key-concepts/smart-tags/","title":"Smart Tags","text":"<p>When Azimuth is launched, smart tags are computed automatically (or loaded from the cache) on all utterances, on both training and evaluation sets.</p> <p>Conceptually, smart tags are meta-data on the utterance and/or its prediction. They help to narrow down data samples to identify those that may require further action and investigation. Different families of smart tags exist, based on the different types of analyses that Azimuth provides.</p> <p>The current list of supported smart tag, and their families, is detailed below.</p>"},{"location":"key-concepts/smart-tags/#extreme-length","title":"Extreme Length","text":"<p> Syntax Analysis gives more details on how the syntactic information is computed.</p> <ul> <li><code>multiple_sentences</code>: The number of sentences is above 1. All other syntactic smart tags will be   disabled when this is the case.</li> <li><code>long_sentence</code>: The number of tokens is greater than or equal to the defined threshold (default is 16).</li> <li><code>short_sentence</code>: The number of tokens is less than or equal to the defined threshold (default is 3).</li> </ul>"},{"location":"key-concepts/smart-tags/#partial-syntax","title":"Partial Syntax","text":"<p> Syntax Analysis gives more details on how the syntactic information is computed.</p> <ul> <li><code>missing_subj</code>: The sentence is missing a subject.</li> <li><code>missing_verb</code>: The sentence is missing a verb.</li> <li><code>missing_obj</code>: The sentence is missing an object.</li> </ul>"},{"location":"key-concepts/smart-tags/#dissimilar","title":"Dissimilar","text":"<p> Similarity Analysis provides more information on how similarity is computed.</p> <ul> <li><code>conflicting_neighbors_train</code>: The utterance has very few (or no) neighbors from the same class   in the training set.</li> <li><code>conflicting_neighbors_eval</code>: The utterance has very few (or no) neighbors from the same class in   the evaluation set.</li> <li><code>no_close_train</code>: The closest utterance in the training set has a cosine similarity below a   threshold (default = 0.5).</li> <li><code>no_close_eval</code>: The closest utterance in the evaluation set has a cosine similarity below a   threshold (default = 0.5).</li> </ul>"},{"location":"key-concepts/smart-tags/#almost-correct","title":"Almost Correct","text":"<p>These smart tags do not come from a particular analysis. They are computed based on the predictions and the labels.</p> <ul> <li><code>correct_top_3</code>: The top 1 prediction is not the right one, but the right one is in the top 3.</li> <li><code>correct_low_conf</code>: The top 1 prediction was the right one, but its confidence is below the   threshold, and thus the rejection class was predicted.</li> </ul>"},{"location":"key-concepts/smart-tags/#behavioral-testing","title":"Behavioral Testing","text":"<p> Behavioral Testing lists all the tests that are executed.</p> <ul> <li><code>failed_fuzzy_matching</code>: At least one fuzzy matching test failed.</li> <li><code>failed_punctuation</code>: At least one punctuation test failed.</li> </ul>"},{"location":"key-concepts/smart-tags/#pipeline-comparison","title":"Pipeline Comparison","text":"<p>Smart tags that are computed based on the difference between pipelines predictions.</p> <ul> <li><code>incorrect_for_all_pipelines</code>: When all pipelines give the wrong prediction.</li> <li><code>pipeline_disagreement</code>: When at least one of the pipelines disagrees with the others.</li> </ul>"},{"location":"key-concepts/smart-tags/#uncertain","title":"Uncertain","text":"<p> Uncertainty Quantification provides more details on how the uncertainty is estimated.</p> <ul> <li><code>high_epistemic_uncertainty</code>: If an uncertainty config was defined, this tag will highlight   predictions with high epistemic uncertainty.</li> </ul>"},{"location":"key-concepts/syntax-analysis/","title":"Syntax Analysis","text":""},{"location":"key-concepts/syntax-analysis/#what-is-it","title":"What is it?","text":"<p>The syntax of an utterance usually refers to its structure, such as how ideas and words are ordered in a sentence. Azimuth provides smart tags based on the length and syntactic structure of utterances.</p>"},{"location":"key-concepts/syntax-analysis/#where-is-this-used-in-azimuth","title":"Where is this used in Azimuth?","text":"<p>Based on the syntax of each utterance, Azimuth computes syntactic  Smart Tags (Extreme Length and Partial Syntax families). Additionally, the length mismatch plot in the  Dataset Class Distribution Analysis compares the length of the utterances in the training set and the evaluation set.</p>"},{"location":"key-concepts/syntax-analysis/#how-is-it-computed","title":"How is it computed?","text":""},{"location":"key-concepts/syntax-analysis/#pos-tags","title":"POS Tags","text":"<p>Part-of-speech (POS) tagging is a common technique to tag each word in a given text as belonging to a category according to its grammatical properties. Examples could be 'verb', or 'direct object'.</p> <p>Azimuth uses spaCy, an open-source library, to perform part-of-speech (POS) and dependency tagging on each token of an utterance. It is set up for all languages supported by Azimuth. Azimuth then computes the smart tags <code>missing_subj</code>, <code>missing_verb</code>, and <code>missing_obj</code> based on the presence of certain tags. Subjects and objects are identified by dependency tags that are language-dependent and specified in the  Syntax Analysis Config, whereas verbs are identified by POS tags that are consistent across languages (shown below). The smart tag <code>multiple_sentences</code> is based on a spaCy sentencizer:</p> <pre><code>import spacy\nfrom spacy.lang.en import English\n# Part of Speech\nverb_tags = [\"VERB\", \"AUX\"]\n# Sentencizer; English() should work for other languages that have similar sentence conventions.\nspacy_pipeline = English()\nspacy_pipeline.add_pipe(\"sentencizer\")\n</code></pre>"},{"location":"key-concepts/syntax-analysis/#token-count","title":"Token Count","text":"<p>To compute the number of tokens per utterance, the following tokenizer is used:</p> <pre><code>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n</code></pre> <p>Based on the token count, the <code>long_sentence</code> and <code>short_sentence</code> smart tags are computed.</p>"},{"location":"key-concepts/syntax-analysis/#configuration","title":"Configuration","text":"<p> Syntax Analysis Config explains how to edit the thresholds to determine what is considered a short or long sentence, the tags used to detect subjects and objects, and the spaCy model used to parse utterances.</p>"},{"location":"key-concepts/uncertainty/","title":"Uncertainty Estimation","text":""},{"location":"key-concepts/uncertainty/#what-is-it","title":"What is it?","text":"<p>Uncertainty estimation is a field of machine learning that aims to determine when a model is uncertain of its prediction. By nature, deep learning models are overconfident and their predictions are not trustworthy1. It is possible for a model to output a prediction with high confidence and still be uncertain.</p> <p>Uncertainty is split into two distinct sources: data uncertainty and model uncertainty. Data uncertainty, also called aleatoric uncertainty, comes from mislabelled examples, noisy data, overlapping classes, etc. Model uncertainty, also called epistemic uncertainty, relates to uncertainty around the model's parameters. As such, epistemic uncertainty is going to be high when a data sample is close to the model's decision boundary.</p> <p>It is important to know that while epistemic uncertainty can be reduced with more data, the intrinsic noise that produces aleatoric uncertainty can't be reduced with more data. More information on uncertainty estimation can be found in the BAAL documentation.</p>"},{"location":"key-concepts/uncertainty/#where-is-it-used-in-azimuth","title":"Where is it used in Azimuth?","text":"<p>Azimuth has some simple uncertainty estimation capabilities. If an uncertainty configuration is provided in the config file, Azimuth assigns a  Smart Tag for utterances with high epistemic uncertainty. These utterances tend to be outliers or mislabeled examples.</p>"},{"location":"key-concepts/uncertainty/#how-is-it-computed","title":"How is it computed?","text":""},{"location":"key-concepts/uncertainty/#epistemic-uncertainty-smart-tag","title":"Epistemic Uncertainty Smart Tag","text":"<p>On Pytorch models, Azimuth leverages MC-Dropout (Gal et al. 2015)2. MC-Dropout draws multiple sets of weights from the model's posterior distribution, effectively creating a Bayesian ensemble. This type of ensemble is weaker than a regular ensemble, as there is a high correlation between each member of the ensemble4.</p> <p>It uses this weak ensemble to estimate the epistemic uncertainty using BALD ( Houlsby et al. 2013). The maximum BALD value is <code>log(C)</code> where <code>C</code> is the number of classes. Predictions with high epistemic uncertainty are data points for which slight changes in the model parameters can cause significant changes in the predictions. On models that do not have any Dropout layers, this has no effect.</p>"},{"location":"key-concepts/uncertainty/#configuration","title":"Configuration","text":"<p> Model Contract Configuration offers some attributes to enable uncertainty quantification, by defining the number of iterations for MC-Dropout, and the threshold value for the smart tag.</p> <ol> <li> <p>Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction. Scalia et al. J. Chem. Inf. Model, 2020\u00a0\u21a9</p> </li> <li> <p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Gal and Ghahramani, ICML, 2020\u00a0\u21a9</p> </li> <li> <p>Bayesian active learning for classification and preference learning. Houlsby et al. arXiv preprint arXiv:1112.5745, 2011\u00a0\u21a9</p> </li> <li> <p>The power of ensembles for active learning in image classification. Beluch et al. CVPR, 2018\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"Reference","text":"<p>Our reference is split in two sections:</p> <ol> <li> <p> Configuration provides detailed information about the    different fields in the Azimuth configuration that allow to define the dataset, the pipelines and    different customization.</p> </li> <li> <p> Custom Objects describes how to create custom objects    that allow to add your own datasets, pipelines and metrics.</p> </li> </ol>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>The Azimuth configuration allows defining different fields, some mandatory, that will customize Azimuth.</p> <p>We grouped the fields in config scopes based on what they control. All classes inherit from one another in a chain, the last one being <code>AzimuthConfig</code>, which contains all fields.</p> <p>To help with detecting the mandatory fields in the config, the following legend is shown throughout the reference.</p> <ul> <li>\ud83d\udd34 : Mandatory fields in the config.</li> <li>\ud83d\udfe0 : Only mandatory if Azimuth is used to analyze models, and not just a   dataset.</li> <li>\ud83d\udfe1 : Usually mandatory fields, but default values exist that may work for some   use cases.</li> <li>\ud83d\udd35 : The default values should work for most use cases.</li> </ul>"},{"location":"reference/configuration/#config-scopes","title":"Config Scopes","text":"<ol> <li> Project Config \ud83d\udd34<ul> <li>Mainly mandatory fields to define the name of the project and the dataset information.</li> </ul> </li> <li> Language Config \ud83d\udfe1<ul> <li>Specifies the model and dataset language used to set several language-based defaults.</li> </ul> </li> <li> Model Contract Config \ud83d\udfe0<ul> <li>Defines how Azimuth interacts with the models/pipelines.</li> </ul> </li> <li> Common Fields Config \ud83d\udd35<ul> <li>Fields that are common to many applications (batch size for example).</li> </ul> </li> <li> Customize Azimuth Analyses:    In these sections, different analyses in Azimuth can be configured.<ol> <li> Behavioral Testing Config \ud83d\udd35<ul> <li>Defines how the behavioral tests are generated.</li> </ul> </li> <li> Similarity Analysis Config \ud83d\udd35<ul> <li>Modifies the similarity analysis.</li> </ul> </li> <li> Dataset Warnings Config \ud83d\udd35<ul> <li>Configure the dataset warnings.</li> </ul> </li> <li> Syntax Analysis Config \ud83d\udd35<ul> <li>Modify the defaults for the syntax analysis.</li> </ul> </li> </ol> </li> </ol>"},{"location":"reference/configuration/common/","title":"Common Fields Config","text":"<p>These fields are generic and can be adapted based on the user's machine.</p> Class DefinitionConfig Example <pre><code>class CommonFieldsConfig(ProjectConfig, extra=Extra.ignore):\n\"\"\"\"\"\"\nartifact_path: str = \"/cache\"\nbatch_size: int = 32\nuse_cuda: Union[Literal[\"auto\"], bool] = \"auto\"\nlarge_dask_cluster: bool = False\nread_only_config: bool = False\n</code></pre> <p>Example to append to the config to override the default <code>batch_size</code>.</p> <pre><code>{\n\"batch_size\": 64,\n}\n</code></pre>"},{"location":"reference/configuration/common/#artifact-path","title":"Artifact Path","text":"<p>\ud83d\udd35 Default value: <code>/cache</code></p> <p>Where to store the caching artifacts (<code>HDF5</code> files and HF datasets). The value needs to be available inside Docker (see <code>docker-compose.yml</code>). <code>/cache</code> is available by default on the docker image.</p> <p>Not using Docker?</p> <p>If Azimuth is run without Docker, the cache needs to be a path with write access (<code>/cache</code> will not work).</p>"},{"location":"reference/configuration/common/#batch-size","title":"Batch Size","text":"<p>\ud83d\udd35 Default value: 32</p> <p>Batch size to use during inference. A higher batch size will make computation faster, depending on the memory available on your machine.</p>"},{"location":"reference/configuration/common/#use-cuda","title":"Use Cuda","text":"<p>\ud83d\udd35 Default value: <code>auto</code></p> <p>If cuda is available on your machine, set to <code>true</code>, otherwise <code>false</code>. Can also be set to \"auto\" and let the user-code take care of it.</p>"},{"location":"reference/configuration/common/#large-dask-cluster","title":"Large Dask Cluster","text":"<p>\ud83d\udd35 Default value: False</p> <p>The memory of the dask cluster is usually 6GB. If your models are big or if you encounter garbage collection errors,  you can set the memory to 12GB by setting <code>large_dask_cluster</code> to <code>True</code>.</p>"},{"location":"reference/configuration/common/#read-only-config","title":"Read-Only Config","text":"<p>\ud83d\udd35 Default value: False</p> <p>This field allows to block the changes to the config when set to <code>True</code>. This can be useful in certain context, such as when hosting a demo.</p>"},{"location":"reference/configuration/language/","title":"Language Config","text":"<p>The language configuration specifies the language of the dataset and model, for the purpose of determining default values used for  Syntax Analysis,  Similarity Analysis, and  Behavioral Testing.</p> <p>For language-specific defaults, see  Syntax Analysis Config,  Similarity Analysis Config, and  Behavioral Testing Config.</p>"},{"location":"reference/configuration/language/#config-scopes","title":"Config Scopes","text":"Class DefinitionConfig Example <pre><code>from enum import Enum\nclass SupportedLanguage(str, Enum):\nen = \"en\"\nfr = \"fr\"\nclass LanguageConfig:\nlanguage: SupportedLanguage = SupportedLanguage.en\n</code></pre> <p>For example, to change the language to French:</p> <pre><code>{\n\"language\": \"fr\"\n}\n</code></pre>"},{"location":"reference/configuration/model_contract/","title":"Model Contract Config","text":"<p>Fields from this scope defines how Azimuth interacts with the ML pipelines and the metrics.</p> Class DefinitionConfig Example <pre><code>from typing import Dict, List, Optional\nfrom azimuth.config import MetricDefinition, PipelineDefinition,\nUncertaintyOptions\nfrom azimuth.types import SupportedModelContract\nclass ModelContractConfig:\nmodel_contract: SupportedModelContract = SupportedModelContract.hf_text_classification # (1)\npipelines: Optional[List[PipelineDefinition]] = None # (2)\nuncertainty: UncertaintyOptions = UncertaintyOptions() # (3)\nsaliency_layer: Optional[str] = None # (4)\nmetrics: Dict[str, MetricDefinition] = { # (5)\n\"Precision\": MetricDefinition(\nclass_name=\"datasets.load_metric\",\nkwargs={\"path\": \"precision\"},\nadditional_kwargs={\"average\": \"weighted\"},\n),\n\"Recall\": MetricDefinition(\nclass_name=\"datasets.load_metric\",\nkwargs={\"path\": \"recall\"},\nadditional_kwargs={\"average\": \"weighted\"},\n),\n\"F1\": MetricDefinition(\nclass_name=\"datasets.load_metric\",\nkwargs={\"path\": \"f1\"},\nadditional_kwargs={\"average\": \"weighted\"},\n),\n}\n</code></pre> <ol> <li><code>model_contract</code> needs to be chosen based on the model type.</li> <li>List of pipelines. Can also be set to <code>null</code> to launch Azimuth with a dataset only.</li> <li>Enable uncertainty quantification.</li> <li>Layer name where to calculate the gradients, normally the word embeddings layer.    Only available for Pytorch models.</li> <li>HuggingFace Metrics.</li> </ol> <pre><code>{\n\"model_contract\": \"hf_text_classification\",\n\"pipelines\": [\n{\n\"model\": {\n\"class_name\": \"loading_resources.load_hf_text_classif_pipeline\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"ckpt_path\": \"distilbert-base-uncased-finetuned-sst-2-english\"\n}\n}\n}\n]\n}\n</code></pre>"},{"location":"reference/configuration/model_contract/#model-contract","title":"Model Contract","text":"<p>\ud83d\udfe0 Mandatory field with an ML pipeline.</p> <p>Default value: <code>hf_text_classification</code></p> <p>The model contract will be determined based on the model type. More details on what model contract to select based on the model is available in  Define a Model.</p> <ul> <li><code>hf_text_classification</code> supports <code>transformers.Pipeline</code> from HF. Examples are provided in the   repo under <code>config/examples</code>.</li> <li><code>custom_text_classification</code> supports any <code>Callable</code> and is more generic. Some features from   Azimuth will be unavailable, such as saliency maps.</li> <li><code>file_based_text_classification</code> supports reading the predictions from a file. A lot of features   from Azimuth will be unavailable, such as behavioral testing and saliency maps.</li> </ul>"},{"location":"reference/configuration/model_contract/#pipelines","title":"Pipelines","text":"<p>\ud83d\udfe0 Mandatory field with an ML pipeline.</p> <p>Default value: <code>None</code></p> <p>In Azimuth, we define an ML pipeline as the combination of a model and postprocessors. This field accepts a list, since multiple pipelines can be loaded in Azimuth. If set to <code>null</code>, Azimuth will be launched without any pipeline.</p> Pipeline DefinitionConfig ExampleNo Pipelines <pre><code>from typing import List, Optional, Union\nfrom pydantic import BaseSettings, Field\nfrom azimuth.config import CustomObject, TemperatureScaling, ThresholdConfig\nclass PipelineDefinition(BaseSettings):\nname: str # (1)\nmodel: CustomObject # (2)\npostprocessors: Optional[ # (3)\nList[Union[TemperatureScaling, ThresholdConfig, CustomObject]]\n] = Field([ThresholdConfig(threshold=0.5)], nullable=True)\n</code></pre> <ol> <li>Add a name to the pipeline to easily recognize it from the webapp. Ex: <code>distilbert-base-uncased-th-0.9</code></li> <li>Azimuth offers a helper function for HF pipelines. See the config example.</li> <li>The default postprocessors in Azimuth is a temperature of 1 and a threshold of 0.5. They can be changed (Ex: <code>postprocessors: [{\"temperature\": 3}]</code>), disabled (<code>postprocessors: null</code>), or replaced with new ones defined with custom objects.</li> </ol> <p>If using a <code>transformers.Pipeline</code> from HF, the configuration below should work.</p> <pre><code>{\n\"pipelines\": [\n{\n\"name\": \"distilbert-base-uncased-th-0.9\"\n\"model\": {\n\"class_name\": \"loading_resources.load_hf_text_classif_pipeline\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"ckpt_path\": \"distilbert-base-uncased-finetuned-sst-2-english\"\n}\n\"postprocessors\": [\n{\n\"threshold\": 0.9\n}\n]\n}\n]\n}\n</code></pre> <p>This will launch Azimuth without any pipelines. Dataset Analysis is still available.</p> <pre><code>{\n\"pipelines\": null\n}\n</code></pre> <p>Both the model and the postprocessors are defined with  Custom Objects.</p> <ul> <li><code>model</code> allows to define a model. The model can be a HuggingFace pipeline, or any callable.   The model can even include its own   post-processing.  Defining a Model   details how to do that with custom objects.</li> <li><code>postprocessors</code> defines the postprocessors. Azimuth offers some default values for   temperature scaling and thresholding. Users can also provide their own postprocessor functions, or   disabled them (<code>postprocessors: null</code>).    Defining Processors details the   different use cases.</li> </ul> <p>Beginner users should start with simple use cases</p> <p>Beginner users should aim to use Azimuth's default supported <code>postprocessors</code>: temperature scaling and thresholding. We provide shortcuts to override the default values. Ex: <code>{\"postprocessors\": [{\"temperature\": 3, \"threshold\": 0.9}]}</code>. You can also use environment variable <code>TEMP</code> and <code>TH</code>. Ex: <code>TH=0.6</code>.</p>"},{"location":"reference/configuration/model_contract/#uncertainty","title":"Uncertainty","text":"<p>\ud83d\udd35 Default value: <code>UncertaintyOptions()</code></p> <p>Azimuth has some simple uncertainty estimation capabilities. By default, they are disabled given that it can be computationally expensive.</p> <p>On any model, we can provide the entropy of the predictions which is an approximation of the predictive uncertainty. In addition, we can tag high epistemic items above a threshold. More information on how we compute uncertainty is available in  Uncertainty Estimation.</p> Class DefinitionConfig Example <pre><code>from pydantic import BaseModel\nclass UncertaintyOptions(BaseModel):\niterations: int = 1  # (1)\nhigh_epistemic_threshold: float = 0.1  # (2)\n</code></pre> <ol> <li>Number of MC sampling to do. The default is 1, which disables BMA.</li> <li>Threshold to determine high epistemic items.</li> </ol> <pre><code>{\n\"uncertainty\": {\n\"iterations\": 20\n}\n}\n</code></pre>"},{"location":"reference/configuration/model_contract/#saliency-layer","title":"Saliency Layer","text":"<p>\ud83d\udfe1 Default value: <code>None</code></p> <p>If using a Pytorch model,  Saliency Maps can be available. Specify the name of the embedding layer on which to compute them.</p> <p>Example: <code>distilbert.embeddings.word_embeddings</code>.</p>"},{"location":"reference/configuration/model_contract/#metrics","title":"Metrics","text":"<p>\ud83d\udd35 Default value: Accuracy, Precision, Recall and F1. See in the config example below.</p> <p>By default, Azimuth will compute the metrics listed above. <code>metrics</code> leverages custom objects, with an additional field which allow defining <code>kwargs</code> to be sent to the metric <code>compute()</code> function.</p> <p>You can add metrics available from the HuggingFace Metric Hub, or create your own, as detailed in  Defining Metrics .</p> Metric DefinitionConfig Example <pre><code>from typing import Dict\nfrom pydantic import Field\nfrom azimuth.config import CustomObject\nclass MetricDefinition(CustomObject):\nadditional_kwargs: Dict = Field(\ndefault_factory=dict,\ntitle=\"Additional kwargs\",\ndescription=\"Keyword arguments supplied to `compute`\",\n)\n</code></pre> <p>These are the default values. This example shows how this could be adapted to other metrics.</p> <pre><code>{\n\"metrics\": {\n\"Accuracy\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"accuracy\"\n}\n},\n\"Precision\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"precision\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n},\n\"Recall\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"recall\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n},\n\"F1\": {\n\"class_name\":\"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"f1\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n}\n}\n}\n</code></pre>"},{"location":"reference/configuration/project/","title":"Project Config","text":"<p>The project configuration contains mainly mandatory fields that specify the dataset to load in Azimuth and details about the way it is handled by the app.</p> Class DefinitionConfig Example <pre><code>from typing import Optional\nfrom pydantic import BaseSettings, Field\nfrom azimuth.config import ColumnConfiguration, CustomObject\nclass ProjectConfig(BaseSettings):\nname: str = Field(\"New project\", env=\"NAME\")\ndataset: CustomObject\ncolumns: ColumnConfiguration = ColumnConfiguration()\nrejection_class: Optional[str] = \"REJECTION_CLASS\"\n</code></pre> <pre><code>{\n\"name\": \"Banking77 Model v4\",\n\"dataset\": {\n\"class_name\": \"datasets.load_dataset\",\n\"args\": [\n\"banking77\"\n]\n},\n\"columns\": {\n\"text_input\": \"text\",\n\"label\": \"target\"\n},\n\"rejection_class\": \"NA\",\n}\n</code></pre>"},{"location":"reference/configuration/project/#name","title":"Name","text":"<p>\ud83d\udfe1 Default value: <code>New project</code></p> <p>Environment Variable: <code>NAME</code></p> <p>Any name can be set for the config. For example, it can represent the name of the dataset and/or the model. Ex: <code>Banking77 Model v4</code>.</p>"},{"location":"reference/configuration/project/#dataset","title":"Dataset","text":"<p>\ud83d\udd34 Mandatory field</p> <p>To define which dataset to load in the application, Azimuth uses  Custom Objects.</p> <p>If the dataset is already on HuggingFace, you can use the <code>datasets.load_dataset</code> from HF, as shown in the example below. If you have your own dataset, you will need to create your own custom object, as explained in  Defining Dataset.</p> Custom Object DefinitionConfig Example with HF <pre><code>from typing import Any, Dict, List, Optional, Union\nfrom pydantic import BaseModel, Field\nclass CustomObject(BaseModel):\nclass_name: str = Field(..., title=\"Class name to load\")\nargs: List[Union[\"CustomObject\", Any]] = []\nkwargs: Dict[str, Union[\"CustomObject\", Any]] = {}\nremote: Optional[str] = None # (1)\n</code></pre> <ol> <li>Absolute path to <code>class_name</code>.</li> </ol> <p>Example to load <code>banking77</code> from HF.</p> <pre><code>{\n\"dataset\": {\n\"class_name\": \"datasets.load_dataset\",\n\"args\": [\n\"banking77\"\n]\n}\n}\n</code></pre>"},{"location":"reference/configuration/project/#columns","title":"Columns","text":"<p>\ud83d\udfe1 Default value: <code>ColumnConfiguration()</code></p> <p>All dataset column names are configurable. The mandatory columns and their descriptions are as follows:</p> Field name Default Description <code>text_input</code> <code>utterance</code> The preprocessed utterance. <code>label</code> <code>label</code> The class label for the utterance, as type <code>datasets.ClassLabel</code>. <code>persistent_id</code> <code>persistent_id</code> A unique identifier for each utterance, as type <code>datasets.Value(\"int16\")</code> or <code>datasets.Value(\"string\")</code>. Class DefinitionConfig Example <pre><code>from pydantic import BaseModel\nclass ColumnConfiguration(BaseModel):\ntext_input: str = \"utterance\" # (1)\nraw_text_input: str = \"utterance_raw\" # (2)\nlabel: str = \"label\" # (3)\nfailed_parsing_reason: str = \"failed_parsing_reason\" # (4)\npersistent_id: str = \"row_idx\" # (5)\n</code></pre> <ol> <li>Column for the text input that will be send to the pipeline.</li> <li>Optional column for the raw text input (before any pre-processing). Unused at the moment.</li> <li>Features column for the label</li> <li>Optional column to specify whether an example has failed preprocessing. Unused at the moment.</li> <li>Unique identifier for every example that should be persisted if the dataset is modified, such as if new examples are added or if examples are modified or removed.</li> </ol> <p>Example to override the default column values.</p> <pre><code>{\n\"columns\": {\n\"text_input\": \"text\",\n\"label\": \"target\"\n}\n}\n</code></pre>"},{"location":"reference/configuration/project/#rejection-class","title":"Rejection class","text":"<p>\ud83d\udfe1 Default value: <code>REJECTION_CLASS</code></p> <p>The field <code>rejection_class</code> requires the class to be present in the dataset. If your dataset doesn't have a rejection class, set the value to <code>null</code>. More details on the rejection class are available in Prediction Outcomes.</p>"},{"location":"reference/configuration/analyses/","title":"Analyses Customization","text":"<p>Four analyses can be configured in Azimuth. Go to each relevant section to learn more about the different attributes that can be defined.</p> Class DefinitionConfig Example <pre><code>class PerturbationTestingConfig(ModelContractConfig):\nbehavioral_testing: Optional[BehavioralTestingOptions] = Field(\nBehavioralTestingOptions(), env=\"BEHAVIORAL_TESTING\"\n)\nclass SimilarityConfig(CommonFieldsConfig):\nsimilarity: Optional[SimilarityOptions] = Field(\nSimilarityOptions(), env=\"SIMILARITY\")\nclass DatasetWarningConfig(CommonFieldsConfig):\ndataset_warnings: DatasetWarningsOptions = DatasetWarningsOptions()\nclass SyntaxConfig(CommonFieldsConfig):\nsyntax: SyntaxOptions = SyntaxOptions()\n</code></pre> <p>This will disable both behavioral testing and similarity analysis in Azimuth.</p> <pre><code>{\n\"behavioral_testing\": null,\n\"similarity\": null\n}\n</code></pre>"},{"location":"reference/configuration/analyses/behavioral_testing/","title":"Behavioral Testing Config","text":"<p>\ud83d\udd35 Default value: <code>BehavioralTestingOptions()</code></p> <p>Environment Variable: <code>BEHAVIORAL_TESTING</code></p> <p> Behavioral Testing in the Key Concepts section explains how the different configuration attributes will affect the tests results. Note that language-related defaults are dynamically selected based on the language specified in the  Language Config (default is English).</p> <p>If your machine does not have a lot of computing power, <code>behavioral_testing</code> can be set to <code>null</code>. It can be enabled later on in the application.</p> Class DefinitionConfig ExampleDisabling Behavioral TestingEnglish defaultsFrench defaults <pre><code>from typing import List\nfrom pydantic import BaseModel\nclass NeutralTokenOptions(BaseModel):\nthreshold: float = 1  # (5)\nsuffix_list: List[str] = []  # Language-based default value # (6)\nprefix_list: List[str] = []  # Language-based default value # (7)\nclass PunctuationTestOptions(BaseModel):\nthreshold: float = 1  # (4)\nclass FuzzyMatchingTestOptions(BaseModel):\nthreshold: float = 1  # (3)\nclass TypoTestOptions(BaseModel):\nthreshold: float = 1  # (2)\nnb_typos_per_utterance: int = 1  # (1)\nclass BehavioralTestingOptions(BaseModel):\nneutral_token: NeutralTokenOptions = NeutralTokenOptions()\npunctuation: PunctuationTestOptions = PunctuationTestOptions()\nfuzzy_matching: FuzzyMatchingTestOptions = FuzzyMatchingTestOptions()\ntypo: TypoTestOptions = TypoTestOptions()\nseed: int = 300\n</code></pre> <ol> <li>Ex: if <code>nb_typos_per_utterance</code> = 2, this will create 2 tests per utterance, one with 1 typo and another with 2 typos.</li> <li>Threshold that defines the confidence gap above which the test will fail.</li> <li>Threshold that defines the confidence gap above which the test will fail.</li> <li>Threshold that defines the confidence gap above which the test will fail.</li> <li>Threshold that defines the confidence gap above which the test will fail.</li> <li>Strings appended to end of utterances for neutral token tests.</li> <li>Strings prepended to beginning of utterances for neutral token tests.</li> </ol> <p>For example, to change the threshold for the punctuation test:</p> <pre><code>{\n\"behavioral_testing\": {\n\"punctuation\": {\n\"threshold\": 0.1\n}\n}\n}\n</code></pre> <pre><code>{\n\"behavioral_testing\": null\n}\n</code></pre> <pre><code># Neutral tokens\nsuffix_list = [\"pls\", \"please\", \"thank you\", \"appreciated\"]\nprefix_list = [\"pls\", \"please\", \"hello\", \"greetings\"]\n</code></pre> <pre><code># Neutral tokens\nsuffix_list = [\"svp\", \"s'il vous pla\u00eet\", \"merci\", \"super\"]\nprefix_list = [\"svp\", \"s'il vous pla\u00eet\", \"bonjour\", \"all\u00f4\"]\n</code></pre>"},{"location":"reference/configuration/analyses/dataset_warnings/","title":"Dataset Warnings Config","text":"<p>\ud83d\udd35 Default value: <code>DatasetWarningsOptions()</code></p> <p>Some thresholds can be set to modify the number of warnings in the  Dataset Warnings.</p> Class DefinitionConfig Example <pre><code>from pydantic import BaseModel\nclass DatasetWarningsOptions(BaseModel):\nmin_num_per_class: int = 20 # (1)\nmax_delta_class_imbalance: float = 0.5 # (2)\nmax_delta_representation: float = 0.05 # (3)\nmax_delta_mean_words: float = 3.0 # (4)\nmax_delta_std_words: float = 3.0 # (5)\n</code></pre> <ol> <li>Threshold for the first set of warnings (missing samples).</li> <li>Threshold for the second set of warnings (class imbalance).</li> <li>Threshold for the third set of warnings (representation mismatch).</li> <li>Threshold for the fourth set of warnings (length mismatch).</li> <li>Threshold for the fourth set of warnings (length mismatch).</li> </ol> <pre><code>{\n\"dataset_warnings\": {\n\"min_num_per_class\": 40\n}\n}\n</code></pre>"},{"location":"reference/configuration/analyses/similarity/","title":"Similarity Analysis Config","text":"<p>\ud83d\udd35 Default value: <code>SimilarityOptions()</code></p> <p>Environment Variable: <code>SIMILARITY</code></p> <p>In Key Concepts,  Similarity Analysis explains how the different configuration attributes will affect the analysis results. Note that language-related defaults are dynamically selected based on the language specified in the  Language Config (default is English).</p> <p>If your machine does not have a lot of computing power, <code>similarity</code> can be set to <code>null</code>. It can be enabled later on in the application.</p> Class DefinitionConfig ExampleDisabling Similarity AnalysisEnglish defaultsFrench defaults <pre><code>from pydantic import BaseModel\nclass SimilarityOptions(BaseModel):\nfaiss_encoder: str = \"\" # Language-based default value # (1)\nconflicting_neighbors_threshold: float = 0.9 # (2)\nno_close_threshold: float = 0.5 # (3)\n</code></pre> <ol> <li>Language model used for utterance embeddings for similarity analysis. The name of your encoder must be supported by sentence-transformers.</li> <li>Threshold to determine the ratio of utterances that should belong to another class for the smart tags <code>conflicting_neighbors_train</code>/<code>conflicting_neighbors_eval</code>.</li> <li>Threshold for cosine similarity for the smart tags <code>no_close_train</code>/<code>no_close_eval</code>.</li> </ol> <p>For example, to change the encoder used for utterance embeddings:</p> <pre><code>{\n\"similarity\": {\n\"faiss_encoder\": \"your_encoder\"\n}\n}\n</code></pre> <pre><code>{\n\"similarity\": null\n}\n</code></pre> <pre><code># Sentence encoder\nfaiss_encoder = \"all-MiniLM-L12-v2\"\n</code></pre> <pre><code># Sentence encoder\nfaiss_encoder = \"distiluse-base-multilingual-cased-v1\"\n</code></pre>"},{"location":"reference/configuration/analyses/syntax/","title":"Syntax Analysis Config","text":"<p>\ud83d\udd35 Default value: <code>SyntaxOptions()</code></p> <p>In the Syntax config, users can modify thresholds to determine what is considered a short or a long sentence, as well as select the spaCy model and the dependency tags used for certain syntax-related smart tags. More details are explained in  Syntax Analysis.</p> <p>Note that language-related defaults are dynamically selected based on the language specified in the  Language Config (default is English). As such, the spaCy model and dependency tag lists will generally not need to be modified.</p> Class DefinitionConfig ExampleEnglish defaultsFrench defaults <pre><code>from pydantic import BaseModel\nclass SyntaxOptions(BaseModel):\nshort_sentence_max_word int = 3 # (1)\nlong_sentence_min_word: int = 12 # (2)\nspacy_model: SupportedSpacyModels = SupportedSpacyModels.use_default  # Language-based default (3)\nsubj_tags: List[str] = []  # Language-based default value (4)\nobj_tags: List[str] = []  # Language-based default value (5)\n</code></pre> <ol> <li>Maximum number of tokens for a sentence to be tagged as short (e.g &lt;=3 for the default)</li> <li>Minimum number of tokens for a sentence to be tagged as long (e.g &gt;=16 for the default)</li> <li>spaCy model to use for syntax tagging.</li> <li>spaCy dependency tags used to determine whether a word is a subject (noun).</li> <li>spaCy dependency tags used to determine whether a word is an object (noun).</li> </ol> <pre><code>{\n\"syntax\": {\n\"short_sentence_max_word\": 5\n}\n}\n</code></pre> <pre><code>import spacy\n# Part of Speech\nsubj_tags = [\"nsubj\", \"nsubjpass\"]\nobj_tags = [\"dobj\", \"pobj\", \"dobj\"]\nspacy_model = spacy.load(\"en_core_web_sm\")\n</code></pre> <pre><code>import spacy\n# Part of Speech\nsubj_tags = [\"nsubj\", \"nsubj:pass\"]\nobj_tags = [\"obj\", \"iobj\", \"obl:arg\", \"obl:agent\", \"obl:mod\"]\nspacy_model = spacy.load(\"fr_core_news_md\")\n</code></pre>"},{"location":"reference/custom-objects/","title":"Custom Objects","text":"<p>Azimuth uses Custom Objects to define how to integrate with models, datasets and metrics. Custom Objects are instantiated at runtime to load the object; the ML pipeline function will return a pipeline, and the Dataset function will return the dataset.</p> <p>Our primary integration is through HuggingFace, but Azimuth supports any other type of dataset or model.</p> <ul> <li> Defining Datasets</li> <li> Defining a Model</li> <li> Defining Postprocessors</li> <li> Defining Metrics</li> </ul>"},{"location":"reference/custom-objects/#what-is-a-custom-object","title":"What is a Custom Object?","text":"<p>A Custom Object is simply a path to a function and its arguments. When users supply their functions and classes, they should be added to <code>azimuth_shr</code>, as it is already mounted at startup on the Docker image.</p> <pre><code>from typing import Any, Dict, List, Optional, Union\nfrom pydantic import BaseModel, Field\nclass CustomObject(BaseModel):\nclass_name: str = Field(..., title=\"Class name to load\")\nargs: List[Union[\"CustomObject\", Any]] = []\nkwargs: Dict[str, Union[\"CustomObject\", Any]] = {}\nremote: Optional[str] = None  # (1)\n</code></pre> <ol> <li>Absolute path to <code>class_name</code>.</li> </ol> <p><code>class_name</code> is the name of the function or class that is located in <code>remote</code>. <code>args</code> and <code>kwargs</code> will be sent to the function/class.</p>"},{"location":"reference/custom-objects/#example","title":"Example","text":"<p>Here is in example of two custom objects. In <code>azimuth_shr/loading_resources.py</code>, we will add two functions which will load a model and a dataset. The configuration file will then be defined as shown in config example.</p> <code>azimuth_shr/loading_resources.py</code>Config Example <pre><code>import transformers\nimport datasets\ndef my_model(ckpt_path) -&gt; transformers.Pipeline:\npipeline = ...  # Load the pipeline from ckpt_path\nreturn pipeline\ndef my_dataset(ckpt_path) -&gt; datasets.DatasetDict:\ndataset = ...  # Load the Dataset from ckpt_path\nreturn dataset\n</code></pre> <pre><code>{\n\"dataset\": {\n\"class_name\": \"loading_resources.my_dataset\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"ckpt_path\": \"/azimuth_shr/data/my_dataset\" # (1)\n}\n},\n\"pipelines\": [\n{\n\"model\": {\n\"class_name\": \"loading_resources.my_model\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"ckpt_path\": \"distilbert-base-uncased-finetuned-sst-2-english\"\n}\n}\n}\n]\n}\n</code></pre> <ol> <li>Path to the dataset. Should be put under <code>/azimuth_shr</code> so it is mounted on the Docker image automatically.</li> </ol>"},{"location":"reference/custom-objects/#using-the-config","title":"Using the Config","text":"<p>If the function has an argument named <code>azimuth_config</code>, Azimuth will supply the config file to the function automatically. This can be useful if some attributes from the config are needed.</p> <pre><code>from azimuth.config import AzimuthConfig\n# This is fine.\ndef my_model(num_classes, azimuth_config: AzimuthConfig):\npass\n# Also fine\ndef my_model(num_classes):\npass\n</code></pre>"},{"location":"reference/custom-objects/#install-dependencies-at-runtime","title":"Install dependencies at runtime","text":"<p>To avoid needing to modify the Docker image to include your dependencies, Azimuth supports installing dependencies at runtime.</p> <p>To do this, the field <code>remote</code> can be one of the following:</p> <ol> <li>A package on Pypi<ol> <li>Example: <code>remote: \"torchvision\"</code></li> </ol> </li> <li>A folder with <code>requirements.txt</code></li> <li>A folder with <code>setup.py</code></li> </ol> <p>If none of the above matches, we append <code>remote</code> to <code>PYTHONPATH</code>, but no dependency will be installed.</p>"},{"location":"reference/custom-objects/dataset/","title":"Defining Dataset","text":"<p>In  Project Config is described how a dataset needs to be defined with a  Custom Object in the config. This section details how to define the <code>class_name</code>, <code>args</code> and <code>kwargs</code> defined in the custom object.</p>"},{"location":"reference/custom-objects/dataset/#dataset-definition","title":"Dataset Definition","text":"<p>Azimuth supports the HuggingFace Dataset API. The loading function for the dataset must respect the following contract:</p> <pre><code>from datasets import DatasetDict\nfrom azimuth.config import AzimuthConfig\ndef load_your_dataset(azimuth_config: AzimuthConfig, **kwargs) -&gt; DatasetDict:\n...\n</code></pre> <p>Your don't have a HuggingFace <code>Dataset</code>?</p> <p>If your dataset is not a HuggingFace <code>Dataset</code>, you can convert it easily using the following resources from HuggingFace:</p> <ol> <li>from local files</li> <li>from in-memory data</li> </ol> <p>We suggest following this HuggingFace tutorial to know more about dataset loading using Huggingface.</p>"},{"location":"reference/custom-objects/dataset/#dataset-splits","title":"Dataset splits","text":"<p>Azimuth expects the <code>train</code> and one of <code>validation</code> or <code>test</code> splits to be available. If both <code>validation</code> and <code>test</code> are available, we will pick the former. The <code>train</code> is not mandatory for Azimuth to run.</p>"},{"location":"reference/custom-objects/dataset/#column-names-and-rejection-class","title":"Column names and rejection class","text":"<p>Go to the  Project Config to see other attributes that should be set along with the dataset.</p>"},{"location":"reference/custom-objects/dataset/#example","title":"Example","text":"<p>Using this API, we can load SST2, a sentiment analysis dataset.</p> <p>Note: in this case, we can omit <code>azimuth_config</code> from the definition because we don't need it.</p> azimuth_shr/loading_resources.pyConfiguration file <pre><code>from datasets import DatasetDict, load_dataset\ndef load_sst2_dataset(dataset_name: str) -&gt; DatasetDict:\ndatasets = load_dataset(\"glue\", dataset_name)\nreturn DatasetDict(\n{\"train\": datasets[\"train\"], \"validation\": datasets[\"validation\"]}\n)\n</code></pre> <pre><code>{\n\"dataset\": {\n\"class_name\": \"loading_resources.load_sst2_dataset\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"dataset_name\": \"sst2\"\n}\n},\n\"columns\": {\n\"text_input\": \"sentence\"\n},\n\"rejection_class\": null\n}\n</code></pre>"},{"location":"reference/custom-objects/metric/","title":"Defining Metrics","text":"<p>Azimuth builds upon the HuggingFace Metric API. Please refer to HuggingFace Metric Hub that details all available metrics that can be added to Azimuth.</p>"},{"location":"reference/custom-objects/metric/#metric-definition","title":"Metric definition","text":"<p>A metric definition is a simple Custom Object that loads a HuggingFace metric.</p> <p>The metric custom object has an extra-field <code>additional_kwargs</code> that will be pass to the <code>compute()</code> function of the HuggingFace metric. This will be required for some metrics, as shown in the example with precision, recall and F1 below.</p> Metric DefinitionConfig Example <pre><code>from typing import Dict\nfrom pydantic import Field\nfrom azimuth.config import CustomObject\nclass MetricDefinition(CustomObject):\nadditional_kwargs: Dict = Field(\ndefault_factory=dict,\ntitle=\"Additional kwargs\",\ndescription=\"Keyword arguments supplied to `compute`.\",\n)\n</code></pre> <pre><code>{\n\"metrics\": {\n\"Accuracy\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"accuracy\"\n}\n},\n\"Precision\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"precision\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n},\n\"Recall\": {\n\"class_name\": \"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"recall\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n},\n\"F1\": {\n\"class_name\":\"datasets.load_metric\",\n\"kwargs\": {\n\"path\": \"f1\"\n},\n\"additional_kwargs\": {\n\"average\": \"weighted\"\n}\n}\n}\n}\n</code></pre>"},{"location":"reference/custom-objects/metric/#custom-metrics","title":"Custom Metrics","text":"<p>For more experienced users, HuggingFace presents how to create custom metrics in this tutorial.</p> <p>For custom metrics, Azimuth supplies probabilities when possible. The <code>compute</code> method must follow this signature:</p> <pre><code>from typing import Any, Dict, List\nimport numpy as np\ndef _compute(\nself,\npredictions: List[int],  # (1)\nreferences: List[int],  # (2)\nprobabilities: np.ndarray,  # (3)\n) -&gt; Dict[str, Any]:  # (4)\n...\n</code></pre> <ol> <li>Predicted labels (1 class per sample).</li> <li>Ground truth labels.</li> <li>Probabilities computed by the pipeline.</li> <li>A dictionary with the computed values.</li> </ol>"},{"location":"reference/custom-objects/model/","title":"Defining a Model","text":"<p>A model is defined as a function that takes an utterance as input and outputs a <code>SupportedOutput</code> (defined below). Some models will output probabilities, whereas other models are \"pipelines\" which include pre-processing and/or post-processing steps. Azimuth supports both these use cases. The <code>model_contract</code> field from the config will determine how Azimuth will interface with the model, as detailed in this section.</p>"},{"location":"reference/custom-objects/model/#model-definition","title":"Model Definition","text":"<p>In the config, the custom object for the model defines the class that will load the model.</p> <pre><code>from typing import Union, Callable\nfrom transformers import Pipeline\nfrom azimuth.config import AzimuthConfig\ndef load_model(azimuth_config: AzimuthConfig, **kwargs) -&gt; Union[Pipeline,\nCallable]:\n...\n</code></pre> <p>The output of the function (<code>transformers.Pipeline</code> or <code>Callable</code>) will determine the <code>model_contract</code> that should be used.</p> <p>When a <code>transformers.Pipeline</code> from HF is returned, we expect it to contain <code>tokenizer: Tokenizer</code> and <code>model: nn.Module</code>.</p>"},{"location":"reference/custom-objects/model/#model-prediction-signature","title":"Model Prediction Signature","text":"<p>The inputs of the model prediction signature will change based on the <code>model_contract</code>. However, the supported outputs are the same for all model contracts.</p>"},{"location":"reference/custom-objects/model/#supported-output","title":"Supported Output","text":"<pre><code>from typing import Union\nimport numpy as np\nimport transformers\nfrom torch import Tensor  # (1)\nfrom azimuth.modules.model_contracts.text_classification import (\nPipelineOutputProtocol, PipelineOutputProtocolV2)\nSupportedOutput = Union[np.ndarray,\nTensor,\ntransformers.file_utils.ModelOutput,\nPipelineOutputProtocol,\nPipelineOutputProtocolV2]\n</code></pre> <ol> <li><code>Tensor</code> from <code>TensorFlow</code> is also supported.</li> </ol> <p>If the model does not have its own postprocessing, the supported output should be one of the 3 first outputs listed above, as shown in the example below.</p>"},{"location":"reference/custom-objects/model/#example","title":"Example","text":"<p>Assuming the following function, the table below shows how to transform <code>probs</code> in the 3 first supported outputs.</p> <pre><code>import numpy as np\nfrom scipy.special import softmax\nNUM_SAMPLES = 10\nNUM_CLASSES = 2\nprobs = softmax(np.random.rand(NUM_SAMPLES, NUM_CLASSES), -1)\n</code></pre> Supported Output Example Shape <code>np.ndarray</code> <code>probs</code> <code>[N, num_classes]</code> <code>Tensor</code> <code>torch.from_numpy(probs)</code> or  <code>tf.convert_to_tensor(probs)</code> <code>[N, num_classes]</code> <code>ModelOutput</code> <code>SequenceClassifierOutput(logits=torch.log(probs))</code> NA <p>If your model already includes post-processing, or if you decide to create your own post-processing in Azimuth (we already support thresholding and temperature scaling), it will need to output a <code>PipelineOutputProtocol</code> or <code>PipelineOutputProtocolV2</code>. More details can be found in Define Postprocessors.</p>"},{"location":"reference/custom-objects/model/#model-contracts","title":"Model contracts","text":"<p>To differentiate between the different types of models, Azimuth uses a field named <code>model_contract</code>.</p> <code>model_contract</code> Model type Framework <code>hf_text_classification</code> HuggingFace Pipeline Supported by HF <code>custom_text_classification</code> Callable Any <code>file_based_text_classification</code> Callable Any"},{"location":"reference/custom-objects/model/#huggingface-pipeline","title":"HuggingFace Pipeline","text":"<code>model_contract</code> Model type Framework <code>hf_text_classification</code> HuggingFace Pipeline Supported by HF* <p>*Saliency maps are only available for Pytorch models.</p> <p>This is our canonical API. It currently supports all of our features and includes some optimization for HuggingFace inference.</p>"},{"location":"reference/custom-objects/model/#model-definition_1","title":"Model Definition","text":"<p>In the config, the custom object for the model should return a <code>transformers.Pipeline</code>.</p> <pre><code>from transformers import Pipeline\nfrom azimuth.config import AzimuthConfig\ndef load_model(azimuth_config: AzimuthConfig, **kwargs) -&gt; Pipeline:\n...\n</code></pre> <p>For most use cases, <code>loading_resources.load_hf_text_classif_pipeline</code> will work.</p>"},{"location":"reference/custom-objects/model/#model-prediction-signature_1","title":"Model Prediction Signature","text":"<p>The prediction signature is the one from <code>transformers</code>. We will supply the following arguments:</p> <pre><code>from typing import List\nfrom azimuth.modules.model_contracts.text_classification import SupportedOutput\ndef __call__(utterances: List[str], num_workers: int,\nbatch_size: int) -&gt; SupportedOutput:\n...\n</code></pre>"},{"location":"reference/custom-objects/model/#example_1","title":"Example","text":"<p>This is how we would load a pretrained BERT model using this API:</p> azimuth_shr/loading_resources.pyConfiguration file <pre><code>from transformers import (\nAutoModelForSequenceClassification,\nAutoTokenizer,\nPipeline,\nTextClassificationPipeline,\n)\nfrom azimuth.config import AzimuthConfig\nfrom azimuth_shr.loading_resources import _should_use_cuda\ndef load_text_classif_pipeline(checkpoint_path: str,\nazimuth_config: AzimuthConfig) -&gt; Pipeline:\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_path, use_fast=False)\ndevice = 0 if _should_use_cuda(azimuth_config) else -1\nreturn TextClassificationPipeline(\nmodel=model, tokenizer=tokenizer, device=device,\nreturn_all_scores=True)  # (1)\n</code></pre> <ol> <li>We set <code>return_all_scores=True</code> to get all softmax outputs.</li> </ol> <pre><code>\"model_contract\": \"hf_text_classification\",\n\"pipelines\": [\n{\n\"model\": {\n\"class_name\": \"loading_resources.load_text_classif_pipeline\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"checkpoint_path\": \"/azimuth_shr/files/clinc/CLINC150_trained_model\"\n}\n}\n}\n]\n</code></pre> <p>Examples are also provided in the repo under <code>config/examples</code>.</p>"},{"location":"reference/custom-objects/model/#other-frameworks","title":"Other frameworks","text":"<code>model_contract</code> Model type Framework <code>custom_text_classification</code> Callable Any <p>This general purpose API can be used with any framework.</p>"},{"location":"reference/custom-objects/model/#model-definition_2","title":"Model Definition","text":"<p>The user-defined model is a <code>Callable</code> that returns predictions from a list of strings.</p> <pre><code>from typing import Callable\nfrom azimuth.config import AzimuthConfig\ndef load_model(azimuth_config: AzimuthConfig, **kwargs) -&gt; Callable:\n...\n</code></pre>"},{"location":"reference/custom-objects/model/#model-prediction-signature_2","title":"Model Prediction Signature","text":"<p>When the model is called with a list of utterances, it should return a prediction for each utterance.</p> <pre><code>from typing import List\nfrom azimuth.modules.model_contracts.text_classification import SupportedOutput\ndef __call__(utterances: List[str]) -&gt; SupportedOutput:\n...\n</code></pre>"},{"location":"reference/custom-objects/model/#disabled-features","title":"Disabled features","text":"<ol> <li>Saliency maps</li> <li>Epistemic uncertainty estimation</li> </ol>"},{"location":"reference/custom-objects/model/#example_2","title":"Example","text":"<p>For models coming from other frameworks, the loading function returns a <code>Callable</code>.</p> azimuth_shr/loading_resources.pyConfiguration file <pre><code>def load_keras_model(checkpoint_path: str,\nazimuth_config: AzimuthConfig) -&gt; Callable:\nmodel = tf.keras.models.load_model(checkpoint_path)\nreturn model\n</code></pre> <pre><code>\"model_contract\": \"custom_text_classification\",\n\"pipelines\": [\n{\n\"model\": {\n\"class_name\": \"loading_resources.load_keras_model\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"checkpoint_path\": \"/azimuth_shr/files/clinc/keras_clinc_model\"\n}\n}\n}\n]\n</code></pre>"},{"location":"reference/custom-objects/model/#file-based","title":"File-based","text":"<code>model_contract</code> Model type Framework <code>file_based_text_classification</code> Callable Any <p>Azimuth can also work without a model, but with predictions supplied in a file. To do so, when calling the prediction function, Azimuth will provide the row index of each utterance along with the sentence. The predictions should be after postprocessing.</p>"},{"location":"reference/custom-objects/model/#model-definition_3","title":"Model Definition","text":"<p>In <code>azimut_shr/models/file_based.FileBasedModel</code> is given the class that can be used to load file-based models.</p>"},{"location":"reference/custom-objects/model/#model-prediction-signature_3","title":"Model Prediction Signature","text":"<pre><code>from typing import Any, Dict\nfrom azimuth.modules.model_contracts.text_classification import SupportedOutput\nfrom azimuth.types import DatasetSplitName\ndef __call__(utterances: Dict[str, Any],\ndataset_split_name: DatasetSplitName) -&gt; SupportedOutput:\n...\n</code></pre> <p>where the <code>utterances</code> have the following keys:</p> <ol> <li><code>row_idx</code>: the indices for each row.</li> <li><code>utterance</code>: utterances as defined in <code>AzimuthConfig.columns.text_input</code>.</li> </ol>"},{"location":"reference/custom-objects/model/#disabled-features_1","title":"Disabled features","text":"<ol> <li>Saliency maps</li> <li>Epistemic uncertainty estimation</li> <li>Threshold Comparison</li> </ol>"},{"location":"reference/custom-objects/model/#example_3","title":"Example","text":"<pre><code>{\n\"model_contract\": \"file_based_text_classification\",\n\"pipelines\": [\n{\n\"model\": {\n\"class_name\": \"models.file_based.FileBasedModel\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\n\"test_path\": \"/azimuth_shr/path_to_your_prediction_file\"\n}\n}\n}\n]\n}\n</code></pre>"},{"location":"reference/custom-objects/processors/","title":"Defining Processors","text":"<ul> <li>Preprocessors cannot be defined in Azimuth, but if they are included in the user pipeline, the tool can display the results of the preprocessing steps.</li> <li>Postprocessors can be included as part of the user pipeline, similar to preprocessors, and defined in Azimuth, either by leveraging the default postprocessors, or defining new ones.</li> </ul>"},{"location":"reference/custom-objects/processors/#default-postprocessors","title":"Default Postprocessors","text":"<p>By default, Azimuth applies thresholding at 0.5. It also supports Temperature Scaling.</p> <p>Azimuth provides shortcuts to override the threshold and the temperature. Add this as postprocessors in the <code>PipelineDefinition</code>.</p> <ul> <li>For Temperature Scaling : <code>{\"postprocessors\": [{\"temperature\": 1}]}</code></li> <li>For Thresholding : <code>{\"postprocessors\": [{\"threshold\": 0.5}]}</code></li> </ul>"},{"location":"reference/custom-objects/processors/#pipeline-with-processing","title":"Pipeline with Processing","text":"<p>If your pipeline already call preprocessing and/or postprocessing steps, as explained in  Define a Model, the model prediction signature needs to have a specific output, <code>PipelineOutputProtocol</code> or <code>PipelineOutputProtocolV2</code>.</p> <p>Both classes need to contain the model and post-processed predictions. The difference between the two classes is the presence of two extra attributes in V2 to return the intermediate results of the preprocessing and postprocessing steps. Those will be displayed in the UI.</p> PipelineOutputProtocolPipelineOutputProtocolV2 <pre><code>from typing import Protocol\nfrom azimuth.utils.ml.postprocessing import PostProcessingIO\nclass PipelineOutputProtocol(Protocol):\n\"\"\"Class containing result of a batch\"\"\"\nmodel_output: PostProcessingIO  # (1)\npostprocessor_output: PostProcessingIO  # (2)\n</code></pre> <ol> <li>model output before passing through post-processing: texts, logits, probs, preds</li> <li>output after passing through post-processing: pre-processed texts, logits, probs, preds</li> </ol> <pre><code>from typing import Protocol\nfrom azimuth.utils.ml.postprocessing import PostProcessingIO\nclass PipelineOutputProtocolV2(PipelineOutputProtocol, Protocol):\n\"\"\"Class containing result of a batch with pre and postprocessing steps\"\"\"\npreprocessing_steps: List[Dict[str, Union[str, List[str]]]] # (1)\npostprocessing_steps: List[Dict[str, Union[str, PostProcessingIO]]] # (2)\n</code></pre> <ol> <li>list of preprocessing steps with the intermediate results. See Preprocessing Steps below.</li> <li>list of postprocessing steps with the intermediate results. See Postprocessing Steps below.</li> </ol> <p><code>PostProcessingIO</code> is defined as the following.</p> <pre><code>from typing import List\nfrom azimuth.types import AliasModel, Array\nclass PostProcessingIO(AliasModel):\ntexts: List[str]  # (1)\nlogits: Array[float]  # (2)\npreds: Array[int]  # (3)\nprobs: Array[float]  # (4)\n</code></pre> <ol> <li>The utterance text. Length of <code>N</code></li> <li>Logits of the model. Shape of <code>(N, C)</code></li> <li>Predicted class, <code>argmax</code>of probabilities. Shape of <code>(N, 1)</code></li> <li>Probabilities of the model # Shape of <code>(N, C)</code></li> </ol> <p>In your code, you don't have to extend <code>PipelineOutputProtocol</code> or <code>PostProcessingIO</code>; you can use your own library, and as long as the fields match, Azimuth will accept it. This is done so that our users don't have to add Azimuth as a dependency.</p> <p>Example</p> <pre><code>@dataclass\nclass PostprocessingData:\ntexts: List[str]  # len [num_samples]\nlogits: np.ndarray  # shape [num_samples, classes]\npreds: np.ndarray  # shape [num_samples]\nprobs: np.ndarray  # shape [num_samples, classes]\n### Valid\nclass MyPipelineOutput(BaseModel):  # Could be dataclass as well.\nmodel_output: PostprocessingData\npostprocessor_output: PostprocessingData\n### Invalid because the field names do not match\nclass MyPipelineOutput(BaseModel):\npredictions: MyPipelineOutput\npostprocessed_predictions: MyPipelineOutput\n</code></pre>"},{"location":"reference/custom-objects/processors/#in-the-config","title":"In the Config","text":"<p><code>{\"postprocessors\": null}</code> should then be added to the config, to avoid re-postprocessing in Azimuth.</p>"},{"location":"reference/custom-objects/processors/#pipelineoutputprotocolv2","title":"<code>PipelineOutputProtocolV2</code>","text":"<p>If using V2, two new fields need to be provided: preprocessing steps and postprocessing steps.</p> <p>The preprocessing steps need to be returned as a <code>List</code> of <code>Dict</code> with the following keys and types.</p> Dict Fields and TypesExample <pre><code>class_name: str  # (1)\ntext: List[str]  # (2)\n</code></pre> <ol> <li>Name of the pre-processing step, usually the name of the Python class.</li> <li>Text of the utterance after the pre-processing step.</li> </ol> <pre><code>[\n{\n'class_name': 'PunctuationRemoval',  # (1)\n'text': ['Test'],  # (2)\n},\n{\n'class_name': 'LowerCase',\n'text': ['test'],\n}\n]\n</code></pre> <p>The postprocessing steps also need to be returned as a <code>List</code> of <code>Dict</code> with the following fields and types.</p> Dict Fields and TypesExample <pre><code>from typing import List\nclass_name: str  # (1)\noutput: PostProcessingIO  # (2)\n</code></pre> <ol> <li>Name of the post-processing step, usually the name of the Python class.</li> <li>Prediction results after this step.</li> </ol> <pre><code>[\n{\n'class_name': 'TemperatureScaling',\n'output': PostprocessorOutput(\ntexts='',\nlogits=array([[-0.20875366 -0.25494186]], dtype=float32),\nprobs=array([[0.511545 0.488455]], dtype=float32),\npreds=array([0])),\n},\n{\n'class_name': 'Thresholding',\n'output': PostprocessorOutput(\ntexts=None,\nfeatures=None,\nlogits=array([[-0.20875366 -0.25494186]], dtype=float32),\nprobs=array([[0.511545 0.488455]], dtype=float32),\npreds=array([2])),\n}\n]\n</code></pre>"},{"location":"reference/custom-objects/processors/#user-defined-postprocessors","title":"User-Defined Postprocessors","text":"<p>Similarly to a model and a dataset, users can add their own postprocessors in Azimuth with custom objects. However, some typing needs to be respected for Azimuth to handle it.</p> <p>First, the post-processing class needs <code>PostProcessingIO</code>, as defined above, as both input and output. To get consistent results, all values need to be updated by the post-processors. For example, if a postprocessor modifies the <code>logits</code>, it must recompute <code>probs</code> as well.</p> <p>The API for a postprocessor is the following:</p> <pre><code>from azimuth.utils.ml.postprocessing import PostProcessingIO\ndef __call__(self, post_processing_io: PostProcessingIO) -&gt; PostProcessingIO:\n...\n</code></pre> <p>You can also extend <code>azimuth.utils.ml.postprocessing.Postprocessing</code> to write your own postprocessor.</p>"},{"location":"reference/custom-objects/processors/#example","title":"Example","text":"<p>Let's define a postprocessor that will do Temperature scaling:</p> azimuth_shr/loading_resources.pyConfiguration file <pre><code>from azimuth.functional.postprocessing import PostProcessingIO\nfrom scipy.special import expit, softmax\nclass TemperatureScaling:\ndef __init__(self, temperature):\nself.temperature = temperature\ndef __call__(self, post_processing_io: PostProcessingIO) -&gt; PostProcessingIO:\nnew_logits = post_processing_io.logits / self.temperature\nconfidences = (\nsoftmax(new_logits, axis=1) if post_processing_io.is_multiclass\nelse expit(new_logits)\n)\nreturn PostProcessingIO(\ntexts=post_processing_io.texts,\nlogits=new_logits,\npreds=post_processing_io.preds,\nprobs=confidences,\n)\n</code></pre> <pre><code>\"pipelines\": [\n{\n\"model\": ...,\n\"postprocessors\": [\n{\n\"class_name\": \"loading_resources.TemperatureScaling\",\n\"remote\": \"/azimuth_shr\",\n\"kwargs\": {\"temperature\": 3}\n}\n]\n}\n]\n</code></pre>"},{"location":"user-guide/","title":"Dashboard","text":"<p>Welcome to Azimuth!</p> <p>Explore the different analyses and tools of Azimuth using the dashboard. Navigate through the different sections to get a deeper understanding of the dataset and the pipeline.</p> <p>Use Azimuth with no pipeline, or with multiple pipelines</p> <p>Azimuth can be launched without any pipelines. All the information related to the pipelines (prediction, behavioral testing and so on) will then be unavailable on all screens. It can also be launched with multiple pipelines. Use the dropdown in the top banner to switch between pipelines.</p> <p></p>"},{"location":"user-guide/#top-banner","title":"Top Banner","text":"<p>The top banner contains useful information and links.</p> <ul> <li>The project name from the config file is shown.</li> <li>A dropdown  allows you to select the different pipelines   defined in the config. It also allows you to select no pipelines.</li> <li>The settings  allows you to edit the config.</li> <li>A link to the support Slack channel and to the documentation is available in the help    option.</li> </ul> <p></p> <p>Don't miss out on the exploration space!</p> <p> At the top, access the Exploration Space to explore and interact with the utterances and the predictions.</p>"},{"location":"user-guide/#dataset-warnings","title":"Dataset Warnings","text":"<p>The dataset warnings section highlights issues related to class size, class imbalance and dataset shift, i.e. differences between the data distributions of the training and the evaluation sets.</p> <p></p> <ul> <li>Missing samples: Verify if each intent has sufficient samples in both sets.</li> <li>Class imbalance: Flag when some classes suffer from imbalance in either split.</li> <li>Representation mismatch: Assess that the representation of each intent is similar in both   sets.</li> <li>Length mismatch: Verify that the utterances' length are similar for each intent in both sets.</li> </ul> <p>Select <code>View Details</code> to get to  Dataset Warnings.</p>"},{"location":"user-guide/#class-overlap","title":"Class Overlap","text":"<p>The Class Overlap section gives an overview of the semantic overlap between class pairs. In some cases, high overlap may be associated with poor class definitions, mislabelling, and/or model confusion.</p> <p>Select <code>View details</code> to access the  Class Overlap page with an interactive Class Overlap Plot.</p> <p></p>"},{"location":"user-guide/#table-content","title":"Table Content","text":"<p>The Class Overlap table presents class pairs that have nonzero class overlap or pipeline confusion.</p> <p>Each row shows a source class and a target class, where the source class is the class of the samples being analyzed (e.g., class label for pipeline confusion), and the target class is the class that the source class may look like, through the lens of the dataset or the model (e.g., prediction for pipeline confusion).</p> <ul> <li>Semantic Overlap Scores are calculated for class pairs in the training data based on the   locations of utterances in embedding space, as described in    Similarity Analysis. Because it is   determined on a dataset alone, it does not require a pipeline to be supplied.</li> <li>Utterances with Overlap indicates the percentage of samples in the source class that are   determined to overlap the target class, as well as the sample count with overlap   relative to the total sample count for the source class. This is also based on the training data.</li> <li>Pipeline Confusion is presented if a pipeline is provided. Pipeline confusion indicates   confusion of the source class for the target class (i.e., the confusion matrix cell   where the label is the source class and the prediction is the target class), for the   evaluation data.</li> </ul> <p>As class overlap indicates semantic similarity in embedding space, it may or may not be associated with pipeline confusion.</p> <p>For example, if high overlap is due to mislabeling or poor class differentiation, it may be associated with pipeline confusion. On the other hand, it's possible to have a pair of classes that are semantically very similar, but have a distinguishing feature that the model can learn, such as specific nouns. In these cases, class overlap may be expected, but not associated with model confusion.</p> <p>Thus, overlap values can help direct attention to dataset characteristics to explore to determine whether a change should be made (typically data augmentation or relabeling) or if the semantic overlap is acceptable.</p> <p>Training vs Evaluation Data</p> <p>Note that Class Overlap is calculated on the training data, whereas Pipeline Confusion is calculated on the evaluation data. Thus, the two analyses have different source class sample counts.</p> <p>Sorting</p> <p> The data is ordered by overlap value, in descending order. Click the overlap or confusion value headers to sort the values in ascending or descending order.</p>"},{"location":"user-guide/#pipeline-metrics-by-data-subpopulation","title":"Pipeline Metrics by Data Subpopulation","text":"<p>This section summarizes the quality of the predictions in terms of the prediction outcomes and metrics available in Azimuth, for different data subpopulations. Change the value in the dropdown  to see the metrics broken down per label, predicted class, or smart tag families. Use the toggle to alternate between the training set or on the evaluation set.</p> <p>The user can click on any row in the table to get directly to the exploration space with the corresponding filters applied. This allows for further investigation of errors. As an example, clicking on the row with the label <code>freeze_account</code> will bring the user to the exploration space with that same filter applied. This works with prediction classes and smart tags too.</p> <p>Click on <code>Compare pipelines</code> to display the table fullscreen and compare all metrics across pipelines, as explained in Pipeline Metrics Comparison.</p> <p>Sort the table and hide columns</p> <p> Click a column header to sort the values in ascending or descending order. The default order is descending by the number of utterances, except for <code>NO_PREDICTION</code>/<code>NO_SMART_TAGS</code> which will be first. <code>overall</code> always stay at the top.</p> <p> Beside each column header, click the vertical dots to hide the corresponding column, or multiple ones by selecting 'Show columns'. However, the table columns will reappear if the page is refreshed.</p> <p></p> <p>Go to the exploration space to interact with metrics</p> <p> The same metrics are available on the  Exploration Space, where you can filter by any combination of values, and see more information on what each metric represents.</p>"},{"location":"user-guide/#smart-tag-analysis","title":"Smart Tag Analysis","text":"<p>The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes, along with sample counts and prediction accuracies. Use the dropdown  to switch between values for labels or for predictions. Use the toggle to alternate between the training and evaluation sets.</p> <p>The <code>Transpose</code> toggle transposes the table and thus the axes for each bar plot. The default view aides analysis of each smart tag across all classes, whereas the transposed view makes it easier to investigate the smart tag pattern for a specific class.</p> <p>Select <code>View details</code> to get to  Smart Tag Analysis.</p> <p>Sort the table by bar plot columns</p> <p> Click a column header (or row label if transposed) to sort the values in ascending or descending order. This works for bar plot columns as well as numerical columns. The default order is descending by the number of utterances, except for the rejection class, which will be first.</p> <p></p> <p>Go to the exploration space to see samples</p> <p> Clicking on a bar takes you to the exploration space with corresponding filters applied, where you can further explore the tagged samples, including the specific smart tags applied.</p>"},{"location":"user-guide/#behavioral-testing","title":"Behavioral Testing","text":"<p>The Behavioral Testing section summarizes the behavioral testing performance. The failure rates on both the evaluation set and the training set highlight the ratio of failed tests to the total amount of tests.</p> <p>Click the failure rates to alternate between the performance on the training set or on the evaluation set. Select <code>View details</code> to get to  Behavioral Testing Summary, which provides more information on tests and the option to export the results.</p> <p>Scrollable table</p> <p> The data is ordered in descending order by failure rate. The table is scrollable.</p> <p></p> <p>File-based configurations</p> <p>With file-based configurations, the behavioral tests are generated and can be exported. However, since the tool does not have access to the model, predictions cannot be made for the modified utterances. As such, by default the tests have a failure rate of 0% (the new prediction is hard-coded to the original value).</p>"},{"location":"user-guide/#post-processing-analysis","title":"Post-processing Analysis","text":"<p>The Post-processing Analysis provides an assessment of the performance of one post-processing step: the thresholding. The visualization shows the prediction outcome count on the evaluation set for different thresholds. Click <code>View Details</code> to see the plot full screen in  Post-processing Analysis.</p> <p></p> <p>Only available for some configs</p> <p>This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at <code>null</code>.</p>"},{"location":"user-guide/behavioral-testing-summary/","title":"Behavioral Testing Summary","text":"<p>The Behavioral Testing Summary displays a report with the failure rates of each test, on both the evaluation set and the training set.  Invariant (the modification should not change the predicted class) tests are currently supported.</p> <p> Behavioral Testing in Key Concepts gives detailed information on the tests.</p> <p></p>"},{"location":"user-guide/behavioral-testing-summary/#table-content","title":"Table Content","text":"<ul> <li>Each test belongs to a family and a name. The test categorization is then further broken   down by modification type. The verb of the description is used to name the modification type.</li> <li>The failure rate (FR) indicates the number of total failed utterances, over all modified   utterances generated for each test. Hover over the <code>FR</code> to see the average delta (absolute   difference) in the prediction confidence, i.e. when the predicted class remains the same as the   original utterance.<ul> <li>The failure rate on the training set is the performance of the modified training set when   tested with the trained model. This might be useful to test the robustness of the model on   data points it has already seen, before taking the extra step of understanding robustness in   the presence of new data.</li> </ul> </li> <li>The last column shows one example from the dataset of the modifications that were made on the   original utterance.</li> </ul> <p>Sort the table</p> <p> Click the failure rate headers to sort the values in ascending or descending order.</p>"},{"location":"user-guide/behavioral-testing-summary/#download","title":"Download","text":"<p>A summary of the test results can be downloaded, as well as the modified sets generated for both the evaluation set and training set. Download the required file by clicking Export  in the upper right corner of the table and then selecting the appropriate item from the menu.</p> <p>Augment custom utterance</p> <p>See our  Custom Utterances page to learn how to augment additional data.</p>"},{"location":"user-guide/class-overlap/","title":"Class Overlap","text":"<p>Class overlap assesses the semantic overlap between pairs of classes. In some cases, high overlap may be associated with poor class definitions, mislabelling, and/or model confusion.</p> <p>Class overlap is determined with a dataset alone, based on the locations of utterances in embedding space, as described in  Similarity Analysis.</p>"},{"location":"user-guide/class-overlap/#class-overlap-plot","title":"Class Overlap Plot","text":"<p>The Class Overlap plot shows the extent to which source classes semantically overlap target classes, all in the training data. The source class is the class label, and the target class is the class that the source class may look like, based on its nearest neighbors. As such, flows between class nodes indicate whether samples in a source class are in neighborhoods typified by other classes (class overlap) or its own class (self overlap). For each source class, class overlap and self-overlap values sum to 1, unless values are scaled by class size.</p> <p>Overlap is displayed as flows from source class (nodes on the left) to target classes (right). Nodes are ordered with flows for greatest overlap values towards the top, so as to highlight these class pairs. Wider flows indicate greater overlap values. Colors group flows from the same source class. The plot is interactive, in that nodes can be moved and reordered via dragging.</p> <p></p>  Class Overlap plot on the Class Overlap page, accessed via the Dashboard."},{"location":"user-guide/class-overlap/#plot-options","title":"Plot options","text":"<ul> <li>Minimum displayed overlap value: This value determines which overlap flows will be displayed   on the plot. Vary this value to focus on class pairs with greatest overlap, or to see all   overlap to better understand the complexity of the dataset. The default value is set to the   tenth-highest class overlap value for ease of visualization alone, and will differ across   different datasets.</li> <li>Self-overlap: This toggle determines whether to show flows for overlap of a class with   itself, to get a sense of the relative magnitude (and possibly importance) of class overlap.</li> <li>Scale by class size: Overlap values are normalized by source class, such that the sum of   all class overlap and self-overlap values for a source class is 1. This toggle multiples overlap   values by class sample sizes, changing node size and flow width accordingly.</li> </ul>"},{"location":"user-guide/class-overlap/#suggested-workflow","title":"Suggested workflow","text":"<p>The plot options described above allow for exploration of different aspects of class overlap. To navigate them, we suggest the following workflow:</p>"},{"location":"user-guide/class-overlap/#1-default-view-self-overlap-off-scale-by-class-size-on","title":"1. Default view: <code>Self-overlap</code> off, <code>Scale by class size</code> on","text":"<ul> <li>Start here. This view shows you the class pairs with the greatest (scaled) semantic overlap   scores in the dataset. Vary the <code>Minimum displayed overlap value</code> to see all dataset overlap or   to focus on the class pairs with the greatest overlap scores.</li> <li>Because <code>Scale by class size</code> is on, this view will emphasize overlapping classes with greater   sample counts. This is useful if you are less concerned about class overlap from   source classes with few samples in the training data. However, if you want to further investigate   classes with high overlap values but fewer samples, either for better understanding your dataset   or because some classes might have high business value, then you can toggle <code>Scale by class size</code>   to off, as explained in step 2.</li> </ul>"},{"location":"user-guide/class-overlap/#2-toggle-scale-by-class-size-off","title":"2. Toggle <code>Scale by class size</code> off:","text":"<ul> <li>When <code>Scale by class size</code> is turned off, total flows (class overlap and self-overlap) sum to 1.   This view emphasizes class pairs with the greatest class overlap scores, regardless of   whether the source class has many samples in it.</li> <li>This is useful to further understand class overlap for classes that have relatively fewer   samples in them, which might not have been as visible during the analysis at step 1.</li> </ul>"},{"location":"user-guide/class-overlap/#3-toggle-self-overlap-on","title":"3. Toggle <code>Self-overlap</code> on:","text":"<ul> <li>For any given class, turning on <code>Self-overlap</code> lets you compare the extent to which its samples   semantically overlap other classes (class overlap) vs. samples of its own class (self-overlap).   For example, if self-overlap is much higher than class overlap, class overlap may be less   problematic for this class, and vice versa.</li> </ul> <p>Tip</p> <p> Click the reset button next to the overlap threshold value to reset to the default threshold.</p>"},{"location":"user-guide/custom-utterances/","title":"Custom utterances","text":"<p>Azimuth has limited support for \"custom utterances\", i.e. utterances not part of the initial dataset.</p> <p>Azimuth offers support through API routes accessible locally at <code>http://localhost:8091/docs#/Custom%20Utterances%20v1</code>.</p>"},{"location":"user-guide/custom-utterances/#data-augmentation","title":"Data augmentation","text":"<p>To augment a list of utterances, you can follow these intructions:</p> PythonOutput <pre><code>from pprint import pprint\nimport requests\nutterances = [\"welcome to Azimuth\",\n\"Don't forget to star our repo!\"]\nresponse = requests.get(\"http://localhost:8091/custom_utterances/perturbed_utterances\",\nparams={\"utterances\": utterances}).json()\npprint([r[\"perturbedUtterance\"] for r in response])\n</code></pre> <pre><code>&gt;&gt;&gt; [ 'pls welcome to Azimuth',\n      'please welcome to Azimuth',\n      ...,\n      'Do not forget to star our repo!'\n]\n</code></pre>"},{"location":"user-guide/custom-utterances/#saliency","title":"Saliency","text":"<p>If a pipeline allows it, you can get saliency of a custom utterance by doing the following:</p> PythonOutput <pre><code>from pprint import pprint\nimport requests\nutterances = [\"welcome to Azimuth\",\n\"Don't forget to star our repo!\"]\nresponse = requests.get(\"http://localhost:8091/custom_utterances/saliency\",\nparams={\"utterances\": utterances, \"pipeline_index\": 0}).json()\npprint(response)\n</code></pre> <pre><code>&gt;&gt;&gt; [{'saliency': [0.08587087690830231,\n                    ...\n                   ],\n      'tokens': ['[CLS]', 'welcome', 'to', 'az', '##im', '##uth', '[SEP]']},\n      ...\n     ]\n</code></pre>"},{"location":"user-guide/dataset-warnings/","title":"Dataset Warnings","text":"<p>Datasets can suffer from a variety of issues, such as class imbalance, classes with low sample counts, and dataset shift. These warnings help detect some of these issues.</p> <p> </p>"},{"location":"user-guide/dataset-warnings/#missing-samples","title":"Missing samples","text":"<p>In this first analysis, the application flags when a class has fewer than <code>X</code> (default is 20) samples in either the training or the evaluation set. The plot helps to visualize the values for each class.</p>"},{"location":"user-guide/dataset-warnings/#class-imbalance","title":"Class Imbalance","text":"<p>In this second analysis, Azimuth detects class imbalance issues. It raises a flag for all classes where the relative difference between the number of samples in that class and the mean sample count per class in a dataset split is above a certain threshold <code>Y</code>. The default is 50%.</p>"},{"location":"user-guide/dataset-warnings/#dataset-shift","title":"Dataset Shift","text":"<p>A discrepancy between the training and evaluation splits can cause problems with a model. For example, the model may not have a representative sample of examples to train on, making it generalize poorly in production.</p> <p>Alternatively, if your evaluation set does not come from the same data distribution as the data in production, measuring model performance on this evaluation set may not be a good indicator of the performance in production. Distribution analysis aims to give warnings when the training and evaluation sets look too different in some aspect of the data.</p>"},{"location":"user-guide/dataset-warnings/#representation-mismatch","title":"Representation mismatch","text":"<p>This analysis flags when a class is over-represented in the evaluation set (relative to other classes) or the training set. If the delta between the percentage of a class in each set is above <code>Z</code>% (default is 5%), the analysis flags it.</p>"},{"location":"user-guide/dataset-warnings/#length-mismatch","title":"Length mismatch","text":"<p>Length mismatch compares the number of tokens per utterance in both sets. The application flags a warning if the mean and/or standard deviation between the 2 distributions is above <code>A</code> and <code>B</code> ( default is 3 for both) respectively.</p>"},{"location":"user-guide/dataset-warnings/#configuration","title":"Configuration","text":"<p>All thresholds mentioned (<code>X</code>/<code>Y</code>/<code>Z</code>/<code>A</code>/<code>B</code>) can be modified in the config file, as explained in  Dataset Warnings Configuration.</p>"},{"location":"user-guide/pipeline-metrics-comparison/","title":"Pipeline Metrics Comparison","text":"<p>This table summarizes the quality of the predictions for different data subpopulations. When selecting a second model next to <code>Compare Baseline with</code>, the table will display the metrics for both the baseline and the second model, along with delta columns that show the difference for each metric and subpopulation. All interactions available from the Dashboard are also available here, including the ability to sort and hide columns, and to click on a row.</p>"},{"location":"user-guide/post-processing-analysis/","title":"Post-processing Analysis","text":""},{"location":"user-guide/post-processing-analysis/#threshold-comparison","title":"Threshold Comparison","text":"<p>With the Threshold Comparison page, you can compare the performance of the model on the evaluation set at different threshold values. The visualization shows the performance for threshold values between 0 and 95%, with increments of 5%.</p> <p>A suggested minimum amount of correct predictions, as well as a maximum amount of incorrect predictions, are displayed on the plot.</p> <p></p> <p>Only available for some configs</p> <p>This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at <code>null</code>.</p>"},{"location":"user-guide/settings/","title":"Settings","text":""},{"location":"user-guide/settings/#speed-up-the-start-up","title":"Speed-up the Start-Up","text":"<p>Disable behavioral testing and similarity analysis in the config file to increase the start-up speed, as explained in  Behavioral Testing Configuration and  Similarity Configuration. Enable them through settings later on without restarting the app.</p> <p></p>"},{"location":"user-guide/smart-tag-analysis/","title":"Smart Tag Analysis","text":"<p>The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes, along with sample counts and prediction accuracies.</p> <p>The analyses associated with each smart tag family may also be associated with a specific model behavior, failure mode, and/or approach to address any issues. For example, <code>Dissimilar</code> smart tags are associated with class definition issues, such as class overlap, and may require actions such as relabeling training samples or augmenting the training data. <code>Partial Syntax</code> smart tags can help you assess whether any syntactical patterns are associated with greater misclassification, which might warrant data augmentation.</p> <p>To further explore a pattern, including investigating individual samples and smart tags, click on a bar to go to the exploration space with the corresponding filters applied. This can help guide any actions required to address a problem. For example, examining misclassified samples tagged <code>no_close_train</code> can help direct targeted data augmentation, if necessary.</p> <p> Smart Tags in Key Concepts gives information on the smart tags included in each smart tag family, with links to more information on the analyses associated with each.</p> <p></p>"},{"location":"user-guide/smart-tag-analysis/#table-details","title":"Table Details","text":"<p>The table is described in the default view; in the transposed view, references to columns should be interpreted as references to rows.</p>"},{"location":"user-guide/smart-tag-analysis/#table-controls","title":"Table controls","text":"<ul> <li>Use the toggle to alternate between the training and evaluation sets.</li> <li>The <code>Transpose</code> toggle transposes the table and thus the axes for each bar   plot.<ul> <li>The default view aides analysis of each smart tag across all classes, which may be a   good starting point for assessing overall dataset and pipeline patterns.</li> <li>In contrast, the transposed view groups bars for each class on its own axis. This   makes it easier to investigate patterns for a specific class that has been identified   as needing further analysis, such as one with greater misclassification rates.</li> </ul> </li> </ul>"},{"location":"user-guide/smart-tag-analysis/#columns","title":"Columns","text":"<ul> <li>The first column shows the class variable for which other values are presented. Use the   dropdown  to switch between labels and predictions.</li> <li>The second and third columns show sample count and pipeline accuracy, which can help with   identifying or prioritizing classes to investigate. For example, you may want to sort by   accuracy in ascending order, to focus on classes for which the model had more difficulty.</li> <li>The remaining columns show bar plots across the selected class variable for each smart tag   family. Colors indicate prediction outcomes,   and column sorting acts on the full stacked bar.</li> </ul>"},{"location":"user-guide/exploration-space/","title":"Introduction to Exploration Space","text":"<p>The Exploration Space includes the datasets and predictions of your model in an interactive way. Explore the utterances and the predictions, spot patterns in errors, and annotate the data to indicate further work to improve the model's predictions.</p> <p></p> <p>Access from dashboard</p> <p>Access this space through the header of the dashboard.</p> <p></p>"},{"location":"user-guide/exploration-space/#views","title":"Views","text":"<p>Interact with the Exploration Space in different views. Filter the data on any view with the control panel. Both training and evaluation sets can be explored.</p> <p>Pipeline dropdown</p> <p>Don't forget the pipeline dropdown in the top banner to see how the performance of different pipelines compare. This space also exists without selecting any pipelines, to perform dataset analysis.</p>"},{"location":"user-guide/exploration-space/#prediction-overview","title":"Prediction Overview","text":"<ul> <li>Assess the quality of the metrics for any given subset of the data.</li> <li>Visualize the distribution of the confidence scores, according to prediction outcome.</li> <li>See the most important words from the utterances, according to prediction outcome.</li> </ul>"},{"location":"user-guide/exploration-space/#confusion-matrix","title":"Confusion Matrix","text":"<ul> <li>Visualize the model confusion between each pair of intents.</li> </ul>"},{"location":"user-guide/exploration-space/#utterance-table","title":"Utterance Table","text":"<ul> <li>Explore the utterances, with their labels, predictions, and smart tags.</li> <li>Access all utterance details, including the detailed prediction results, the behavioral tests, and   the most similar utterances.</li> <li>Annotate utterances to identify further work.</li> </ul>"},{"location":"user-guide/exploration-space/#control-panel","title":"Control Panel","text":"<p>All views are affected by the control panel.</p> <p>Share and save the filters</p> <p> The control panel selection can be saved and shared by using the url, which contains the selected filters and options.</p> <p> It can also be collapsed to leave more space for the rest of the content.</p>"},{"location":"user-guide/exploration-space/#evaluation-set-and-training-set","title":"Evaluation Set and Training Set","text":"<p>Explore the views on both the evaluation and the training set by clicking the toggle. Clicking on a tab will update the view.</p>"},{"location":"user-guide/exploration-space/#excluding-post-processing","title":"Excluding Post-Processing","text":"<p>By default, the predictions are shown after the ML pipeline, including the model prediction and the post-processing steps (temperature scaling, thresholding, or else). To analyze the model predictions only, you can exclude the post-processing steps in the exploration space. This will affect the displayed predictions, as well as related computations: the metrics, the confidence histogram and the confusion matrix. This will not affect the smart tags, as these would be too long to recompute.</p>"},{"location":"user-guide/exploration-space/#filters","title":"Filters","text":"<p>Filter the data on any view according to different dimensions of the data, such as the model's confidence, the utterances label, or the smart tags. The available categories and behaviors of filters are listed below.</p>"},{"location":"user-guide/exploration-space/#filter-categories","title":"Filter Categories","text":"<ul> <li>Search a particular string to filter utterances that contain it.</li> <li>Filter predictions based on their confidence value. You can specify a minimum and a maximum   value.</li> <li>Filter predictions according to their prediction outcomes.</li> <li>Filter by labeled class (the target).</li> <li>Filter by predicted class.</li> <li>Filter by smart tags.</li> <li>Filter by user-applied proposed actions.</li> </ul>"},{"location":"user-guide/exploration-space/#filters-behavior","title":"Filters Behavior","text":""},{"location":"user-guide/exploration-space/#bars-distribution","title":"Bars Distribution","text":"<ul> <li>The bar distributions to the right of the filters show the   prediction outcome count for a given filter. Selecting or   deselecting filters update the bars based on the current selection.   </li> </ul>"},{"location":"user-guide/exploration-space/#filters-selection","title":"Filters Selection","text":"<ul> <li>The checkbox beside the name of each filter category can be used to select or deselect all   corresponding filters.</li> <li>When selecting filters, the other filters that can no longer be selected will be greyed-out.<ul> <li>Example: selecting a smart tag filter greys out the <code>NO_SMART_TAGS</code> option.</li> </ul> </li> <li>The number of utterances currently selected by the filters is shown in parentheses at the top   of the panel (beside the title <code>Filters</code>).</li> <li>Click <code>Clear filters</code> to reset all filters to the default values.</li> </ul>"},{"location":"user-guide/exploration-space/#negative-filtering","title":"Negative filtering","text":"<ul> <li>For smart tag families and proposed actions, you can filter based on the absence of all available   filters within a category, respectively <code>NO_SMART_TAGS</code> and <code>NO_ACTION</code>.</li> </ul>"},{"location":"user-guide/exploration-space/#search","title":"Search","text":"<ul> <li>Use the search bar to find specific filters.</li> </ul>"},{"location":"user-guide/exploration-space/confusion-matrix/","title":"Confusion Matrix","text":"<p>The confusion matrix displays the model confusion between each pair of intents. The confusion is defined as the number of utterances with a given label that are predicted as another label.</p>"},{"location":"user-guide/exploration-space/confusion-matrix/#normalization","title":"Normalization","text":"<p>The toggle \"Normalize\" in the top right corner allows alternating between normalized and raw values. When normalized, the number of utterances is divided by to the total number of utterances with the given label.</p>"},{"location":"user-guide/exploration-space/confusion-matrix/#class-ordering","title":"Class Ordering","text":"<p>The default order for the rows and columns is determined based on the reverse Cuthill-Mckee algorithm, which will group as many classes as possible with similar confusion. The algorithm ignores all confusion values under 10%. The rejection class is also ignored and is always the last one in the order.</p> <p>Toggling off  \"Reorder classes\" disables the reordering and allows showing the confusion matrix according to the class order provided by the user.</p> <p>Outcome colors</p> <p>The prediction outcome colors are shown on the confusion matrix.</p> <p></p> <p>Example</p> <p>In this example, 45% of utterances labeled as <code>bill_due</code> were predicted as <code>bill_balance</code>.</p>"},{"location":"user-guide/exploration-space/prediction-overview/","title":"Prediction Overview","text":"<p>The Prediction Overview centralizes the metrics, the confidence histogram and two word clouds to show important words from the utterances.</p> <p></p>"},{"location":"user-guide/exploration-space/prediction-overview/#metrics","title":"Metrics","text":"<p>Assess the quality of the model with different metrics. Hover over the information icon to see more information on each metric.</p> <p></p> <ul> <li>The first tile corresponds to the performance based on prediction   outcomes:<ul> <li> Correct &amp; Predicted</li> <li> Correct &amp; Rejected</li> <li> Incorrect &amp; Rejected</li> <li> Incorrect &amp; Predicted</li> </ul> </li> <li>The second tile contains the precision, recall and F1.</li> <li>The last tile shows the Expected Calibration Error (ECE), which indicates the quality of the   model's calibration. An ECE of 0 means perfect calibration; A lower ECE is better. Hover the ECE   to show a plot that displays the breakdown of the ECE computation per bin.</li> </ul>"},{"location":"user-guide/exploration-space/prediction-overview/#confidence-histogram","title":"Confidence Histogram","text":"<p>The Confidence Histogram displays the distribution of model confidences, grouped-by prediction outcomes. The threshold, if set in the  Project Configuration, is displayed on the plot.</p> <p></p> <p>Assess the distribution</p> <p>Assess the confidence distribution by looking at the shape of the curve, the min and max values, and the ratio in each bin. Look at the histogram for any subset of the data by using the control panel.</p>"},{"location":"user-guide/exploration-space/prediction-overview/#word-clouds","title":"Word Clouds","text":"<p>To the right of the histogram is a word cloud showing the most important words for correct and incorrect predictions. Correct predictions include <code>Correct &amp; Predicted</code> and <code>Correct &amp; Rejected</code>, while incorrect predictions include <code>Incorrect &amp; Predicted</code> and <code>Incorrect &amp; Rejected</code>. The word clouds change depending on the filters and indicate the frequency of each important word across all filtered utterances.</p> <p>Clicking on a word will filter utterances that contain it. Clicking it a second time will clear that search filter, so does the \u2717 in the <code>Search utterances</code> filter.</p> <p></p> <p>What does the word cloud count?</p> <p>If saliency maps are available, the word clouds show the count of salient words. A word is considered salient in an utterance if the sum of its tokens' saliency values is greater than 60% of the largest saliency value in the utterance.</p> <p>If saliency maps are not available, the word clouds only show the most frequent words which are not part of a pre-defined list of stop words (from nltk.corpus).</p>"},{"location":"user-guide/exploration-space/utterance-details/","title":"Utterance Details","text":"<p>The utterance detail page shows more information about individual utterances. See the including saliency per token, semantically similar utterances, and model performance on behavioral tests automatically generated from the utterance.</p>"},{"location":"user-guide/exploration-space/utterance-details/#utterance-information","title":"Utterance Information","text":"<p>The top section shows the utterance details:</p> <ul> <li>Utterance: The utterance and its per-token saliency (if   available for the model's architecture). Hover over the tokens to the see the associated saliency   value.</li> <li>If pre-processing steps are returned by the pipeline, blue chevrons will be shown, allowing the utterance to be displayed after each pre-processing step, from the raw utterance to the processed version sent to the model.</li> <li>Label: The utterance's labeled class (the target).</li> <li>Predictions:<ul> <li>The top 3 predictions for this utterance are shown.</li> <li>The top prediction is highlighted using the prediction outcome color.<ul> <li>If the top prediction was converted to the rejection class based on the threshold, it will   be added as a 4th element at the top.</li> </ul> </li> <li>By clicking on the blue chevrons, the prediction results after each post-processing step can be viewed, including the model's output before any post-processing.</li> </ul> </li> <li>Smart tags (where applicable): An automatically computed tag highlighting a certain   characteristic of the utterance (e.g., long sentences, utterance is missing a verb, utterance   contains multiple sentences). For more information,   see Smart Tags.</li> <li>Proposed Action (editable): You can add a proposed action to identify further steps required   for an incorrectly predicted utterance. For more information,   see Proposed Actions.</li> </ul> <p></p>"},{"location":"user-guide/exploration-space/utterance-details/#semantically-similar-utterances","title":"Semantically Similar Utterances","text":"<p>The Semantically Similar Utterances tab shows the top 20 most similar utterances in the dataset as calculated using sentence embeddings. For more information, see  Similarity Analysis.</p> <ul> <li>The toggle buttons control whether to search for similar utterances in the evaluation set or the   training set.</li> <li>Utterances that are similar but have different labels or predictions can indicate possible   problems with the dataset or the model. An \u26a0 icon indicates utterances that are from a   different class than the base utterance.</li> <li>Clicking on the row of an utterance in the table will open the details page for that utterance.</li> </ul>"},{"location":"user-guide/exploration-space/utterance-details/#behavioral-tests","title":"Behavioral Tests","text":"<p>The Behavioral Tests tab displays the result of the behavioral tests that were run to test the  model's robustness to minor modifications. A failing test occurs when a modified utterance has a different predicted class or when prediction certainty is altered by more than a set threshold. For more information, see  Behavioral Tests.</p> <p></p> <p>Only available when a pipeline is defined</p> <p>Behavioral tests are only available when a pipeline is available. As such, the tab will be empty for file-based configurations.</p>"},{"location":"user-guide/exploration-space/utterance-table/","title":"Utterance Table","text":"<p>The utterance table view contains all the utterances with their predictions. The table also includes information such as smart tags and proposed actions, if applicable.</p> <p></p> <p>To see more details on an utterance, click any row, which will open the  Utterance Details page.</p> <p>Open in a new tab</p> <p> Open the utterance details in a new tab or in a new window using the Ctrl or Cmd + click or right-click menu.</p>"},{"location":"user-guide/exploration-space/utterance-table/#table-content","title":"Table Content","text":"<p>Sort the table</p> <p> Click a column header to sort the table by the column values. Each click rotates between ascending order, descending order, and no sorting.</p>"},{"location":"user-guide/exploration-space/utterance-table/#id","title":"ID","text":"<p>A unique ID for each utterance is created for referencing purposes. When exporting the utterances, the utterance ID refers to the column <code>row_idx</code>.</p>"},{"location":"user-guide/exploration-space/utterance-table/#utterance","title":"Utterance","text":"<p>Utterances appear as available in the dataset provided. Hover over the utterances to display the copy button .</p> <p>If available, the utterances are overlaid with saliency maps, highlighting the most important tokens for the model's prediction. You can see the raw saliency values when hovering on the utterance tokens in the  Utterance Details only. For more information on how these values are calculated see  Saliency Map.</p> <p></p>"},{"location":"user-guide/exploration-space/utterance-table/#prediction-information","title":"Prediction Information","text":"<p>The table shows the labels, predicted classes, and confidences for the utterances. If the confidence is below the prediction threshold, the table reads <code>NO_PREDICTION</code> and the original prediction in parentheses.</p>"},{"location":"user-guide/exploration-space/utterance-table/#smart-tags","title":"Smart Tags","text":"<p>The table shows smart tag families, as well as the specific smart tag values on hover. For more information, see Smart Tags.</p>"},{"location":"user-guide/exploration-space/utterance-table/#proposed-action","title":"Proposed Action","text":"<p>For each data point, the user can specify if an action needs to be taken. Proposed Actions are explained in the Key Concepts section. The actions are done outside the app. Export the proposed actions in a <code>.csv</code> file and use the list to resolve the utterance issues. The exported file also contains the smart tags.</p> <p>Apply in batch</p> <p>Proposed actions can be applied in batches by selecting multiple rows (or selecting all based on the current search) and applying the change.</p>"}]}