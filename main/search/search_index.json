{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Azimuth Documentation! Azimuth is an open source application that helps AI practitioners and data scientists better understand their dataset and model predictions by performing thorough dataset and error analyses . The application leverages different tools, including robustness tests, semantic similarity analysis and saliency maps, unified by concepts such as smart tags and proposed actions . While this version of Azimuth focuses on NLP classification problems , the tool could easily be adapted to apply to other data types and models, e.g. vision or tabular use cases. However, the current focus is on text classification. Documentation Structure Getting Started contains all the instructions to install and launch the app. It also details the changelog of our releases. Key Concepts explains the different concepts and analyses that are provided in Azimuth to perform dataset and error analysis. User Guide goes screen per screen to explain the different interactions and visualizations available. Reference details the config file and the custom objects which allow configuring Azimuth with different datasets and pipelines. Development guides on how to develop and contribute to the repo. Support Join the Slack channel to ask questions and engage with the community. File issues in our GitHub repo . Learn how to contribute in CONTRIBUTING.md .","title":"Welcome to the Azimuth Documentation!"},{"location":"#welcome-to-the-azimuth-documentation","text":"Azimuth is an open source application that helps AI practitioners and data scientists better understand their dataset and model predictions by performing thorough dataset and error analyses . The application leverages different tools, including robustness tests, semantic similarity analysis and saliency maps, unified by concepts such as smart tags and proposed actions . While this version of Azimuth focuses on NLP classification problems , the tool could easily be adapted to apply to other data types and models, e.g. vision or tabular use cases. However, the current focus is on text classification.","title":"Welcome to the Azimuth Documentation!"},{"location":"#documentation-structure","text":"Getting Started contains all the instructions to install and launch the app. It also details the changelog of our releases. Key Concepts explains the different concepts and analyses that are provided in Azimuth to perform dataset and error analysis. User Guide goes screen per screen to explain the different interactions and visualizations available. Reference details the config file and the custom objects which allow configuring Azimuth with different datasets and pipelines. Development guides on how to develop and contribute to the repo.","title":" Documentation Structure"},{"location":"#support","text":"Join the Slack channel to ask questions and engage with the community. File issues in our GitHub repo . Learn how to contribute in CONTRIBUTING.md .","title":" Support"},{"location":"about-us/","text":"About Us We started developing Azimuth at the beginning of 2021, as an internal tool at ServiceNow. We are open source since April 2022. Our motivation in developing this tool is to go beyond the traditional model quality assessment based solely on high-level metrics. We strongly believe that much more can be discovered and understood from our dataset and models if we have the right tools, visualizations, and interfaces. We hope this can be helpful to you, and that you enjoy Azimuth! The Team Main Contributors Fr\u00e9d\u00e9ric Branchaud-Charron Fred works as an Applied Research Scientist specializing in Bayesian deep learning, active learning and uncertainty estimation. In addition to maintaining Azimuth, he also maintains BaaL, a Bayesian active learning library. Gabrielle Gauthier-Melan\u00e7on Gab is an Applied Research Scientist, interested in explainability, uncertainty and topics related to building trust in AI. Since the beginning of Azimuth, she is leading the product ideation, while being involved in designing the user interface, documenting it, and maintaining the back end. Joseph Marinier Joseph is a full-stack developer who enjoys designing and engineering solutions to a large variety of problems. His main contribution to Azimuth has been leading the frontend development. Lindsay Brin Lindsay is an Applied Research Scientist working in explainability and NLU, who loves the concept of representing language in mathematical space. Her journey through data analysis, modeling, and visualization started with research in biogeochemistry and ecosystem ecology. Chris Tyler Chris is a software developer with a background in physics and an interest in everything. He has contributed to the design and development of the Azimuth user interface. Nandhini Babu Nandhini is a developer at ServiceNow who joined Azimuth in January 2022. She's mostly contributing on the front end. Her overall experience is around web development. Designers We can also count on the support of amazing designers. Di Le Di Le is an AI/ML design strategist, and a contributor to the intelligent automation of enterprise software at ServiceNow. Her work focus surrounds human-centered AI design and creating systems that augment, automate and accelerate how people work. Karine Grande Karine is a Product Designer working on experiences that use AI/ML technology. In addition of helping on the design side of Azimuth, she worked on projects about data labelling, classification, summarization and forecasting, and is currently working on document processing and data extraction. Nikola Simic Nikola is a Senior Product Designer working on products that leverage responsible and ethical AI and Machine Learning across the enterprise space. He created the Azimuth visual identity and is developing Azimuth's branding and video materials. Other Contributors Orlando Marquez Orlando is a Lead Applied Research Scientist with a strong background in software engineering. One of his passions is shipping state-of-the-art NLP to end-users through rigorous and careful experimentation. His early contributions to Azimuth revolved around saliency maps for NLP and similarity-based analysis. Michael Lanoie Michael is a technical writer working on product documentation for AI/ML software at ServiceNow. He contributed as a content editor and collaborator, helping improve the UX and the documentation. Sean Hughes Sean leads cross-functional AI ecosystem strategy and engagement at ServiceNow, bringing his experience in AI developer community development to help the team launch and drive adoption of Azimuth. Special Thanks Thank you Sethu Meiyappan, and Francis Ruel for helping us on the path to open sourcing!","title":"About Us"},{"location":"about-us/#about-us","text":"We started developing Azimuth at the beginning of 2021, as an internal tool at ServiceNow. We are open source since April 2022. Our motivation in developing this tool is to go beyond the traditional model quality assessment based solely on high-level metrics. We strongly believe that much more can be discovered and understood from our dataset and models if we have the right tools, visualizations, and interfaces. We hope this can be helpful to you, and that you enjoy Azimuth!","title":"About Us"},{"location":"about-us/#the-team","text":"","title":"The Team"},{"location":"about-us/#main-contributors","text":"","title":"Main Contributors"},{"location":"about-us/#frederic-branchaud-charron","text":"Fred works as an Applied Research Scientist specializing in Bayesian deep learning, active learning and uncertainty estimation. In addition to maintaining Azimuth, he also maintains BaaL, a Bayesian active learning library.","title":"Fr\u00e9d\u00e9ric Branchaud-Charron  "},{"location":"about-us/#gabrielle-gauthier-melancon","text":"Gab is an Applied Research Scientist, interested in explainability, uncertainty and topics related to building trust in AI. Since the beginning of Azimuth, she is leading the product ideation, while being involved in designing the user interface, documenting it, and maintaining the back end.","title":"Gabrielle Gauthier-Melan\u00e7on  "},{"location":"about-us/#joseph-marinier","text":"Joseph is a full-stack developer who enjoys designing and engineering solutions to a large variety of problems. His main contribution to Azimuth has been leading the frontend development.","title":"Joseph Marinier  "},{"location":"about-us/#lindsay-brin","text":"Lindsay is an Applied Research Scientist working in explainability and NLU, who loves the concept of representing language in mathematical space. Her journey through data analysis, modeling, and visualization started with research in biogeochemistry and ecosystem ecology.","title":"Lindsay Brin  "},{"location":"about-us/#chris-tyler","text":"Chris is a software developer with a background in physics and an interest in everything. He has contributed to the design and development of the Azimuth user interface.","title":"Chris Tyler  "},{"location":"about-us/#nandhini-babu","text":"Nandhini is a developer at ServiceNow who joined Azimuth in January 2022. She's mostly contributing on the front end. Her overall experience is around web development.","title":"Nandhini Babu  "},{"location":"about-us/#designers","text":"We can also count on the support of amazing designers.","title":"Designers"},{"location":"about-us/#di-le","text":"Di Le is an AI/ML design strategist, and a contributor to the intelligent automation of enterprise software at ServiceNow. Her work focus surrounds human-centered AI design and creating systems that augment, automate and accelerate how people work.","title":"Di Le "},{"location":"about-us/#karine-grande","text":"Karine is a Product Designer working on experiences that use AI/ML technology. In addition of helping on the design side of Azimuth, she worked on projects about data labelling, classification, summarization and forecasting, and is currently working on document processing and data extraction.","title":"Karine Grande "},{"location":"about-us/#nikola-simic","text":"Nikola is a Senior Product Designer working on products that leverage responsible and ethical AI and Machine Learning across the enterprise space. He created the Azimuth visual identity and is developing Azimuth's branding and video materials.","title":"Nikola Simic "},{"location":"about-us/#other-contributors","text":"","title":"Other Contributors"},{"location":"about-us/#orlando-marquez","text":"Orlando is a Lead Applied Research Scientist with a strong background in software engineering. One of his passions is shipping state-of-the-art NLP to end-users through rigorous and careful experimentation. His early contributions to Azimuth revolved around saliency maps for NLP and similarity-based analysis.","title":"Orlando Marquez  "},{"location":"about-us/#michael-lanoie","text":"Michael is a technical writer working on product documentation for AI/ML software at ServiceNow. He contributed as a content editor and collaborator, helping improve the UX and the documentation.","title":"Michael Lanoie "},{"location":"about-us/#sean-hughes","text":"Sean leads cross-functional AI ecosystem strategy and engagement at ServiceNow, bringing his experience in AI developer community development to help the team launch and drive adoption of Azimuth.","title":"Sean Hughes "},{"location":"about-us/#special-thanks","text":"Thank you Sethu Meiyappan, and Francis Ruel for helping us on the path to open sourcing!","title":"Special Thanks"},{"location":"development/","text":"Development This section documents how to get started when setting up your environment before contributing to Azimuth. Please see our CONTRIBUTING.md for guidelines on how to contribute. Just want to launch the app locally without Docker? You will need poetry and yarn installed, as explained in Initial Setup . Then go the Launching section. Development Documentation Structure Initial Setup Steps to install Backend and Frontend dependencies. Launching Steps to launch Azimuth for development. Development Practices Information on our best practices regarding branch names, testing and pull-requests.","title":"Development"},{"location":"development/#development","text":"This section documents how to get started when setting up your environment before contributing to Azimuth. Please see our CONTRIBUTING.md for guidelines on how to contribute. Just want to launch the app locally without Docker? You will need poetry and yarn installed, as explained in Initial Setup . Then go the Launching section.","title":"Development"},{"location":"development/#development-documentation-structure","text":"Initial Setup Steps to install Backend and Frontend dependencies. Launching Steps to launch Azimuth for development. Development Practices Information on our best practices regarding branch names, testing and pull-requests.","title":" Development Documentation Structure"},{"location":"development/dev-practices/","text":"Development Pratices Git Branches main : Integrates new features, documentation can be lacking. Users will run this branch if they run locally. If they use Docker, they will use our latest release and won't be affected by the latest changes on main . release/* : Branches that can be created to release a version, whenever main might contain additional commits which should not be part of the immediate release. release/* should be merged in main after. We don't have a convention for the names of regular branches for developing features and fixes. Each version has its own tag ( v2.0.0 for example) and release package on Github. Committing We try to commit regularly, so it is easy to revert partial changes. Front End Types If you played with types and routes in the back end, remember to regenerate the front-end types, using the following command from the webapp folder while the back end is running (see how to launch here ). cd webapp yarn types # while the back end is runnnig Pre-commit Hooks When you commit, the pre-commit hooks will automatically be run to format the code included in the commit. We have different hooks which will run mypy , reorder imports, clean unnecessary white-spacing, and perform typical formatting with black , flake8 and prettier . In some cases, you might need to format using make format . However, in most cases, pre-commit hooks will make the necessary changes automatically. mypy errors will never prevent a commit, but it will print the errors so that you can fix them if you introduced a new one. How to temporarily disable pre-commits? For a specific commit, you can avoid running the pre-commits with the flag --no-verify . If you do this, make sure to run pre-commit run --all-files before opening a PR, which will run the pre-commit on all files. Testing It might be that all tests don't pass at each commit, and this is normal. Tests can take a while to run, and so you might want to run them only at specific moments, such as before opening a new PR. When ready to run tests, you can do the following commmands. make format # It will fix formatting issues. make test # It will check for formatting issues, run mypy and all unit tests. You can also only run specific tests, using poetry run pytest {path_to_test.py} . Clean cache for routers tests For tests in tests/test_routers , we run the startup task once and save the result in /tmp/azimuth_test_cache . When modifying Modules, you need to clean it manually or by running make clean . Regularly launch the app Even when all unit tests pass, it may happen that some new issues arise when launching the app. When doing important code changes, it is crucial to launch the app and see if the behavior is as expected. Details on launching the app are available here . Opening a Pull Request (PR) We have a template for when you open a PR. Follow instructions and check the boxes to acknowledge that you have done the required steps. Regarding the CHANGELOG.md , you will need to add your changes only if it is visible for the end-users. This helps us when building the release notes.","title":"Development Pratices"},{"location":"development/dev-practices/#development-pratices","text":"","title":"Development Pratices"},{"location":"development/dev-practices/#git-branches","text":"main : Integrates new features, documentation can be lacking. Users will run this branch if they run locally. If they use Docker, they will use our latest release and won't be affected by the latest changes on main . release/* : Branches that can be created to release a version, whenever main might contain additional commits which should not be part of the immediate release. release/* should be merged in main after. We don't have a convention for the names of regular branches for developing features and fixes. Each version has its own tag ( v2.0.0 for example) and release package on Github.","title":"Git Branches"},{"location":"development/dev-practices/#committing","text":"We try to commit regularly, so it is easy to revert partial changes.","title":"Committing"},{"location":"development/dev-practices/#front-end-types","text":"If you played with types and routes in the back end, remember to regenerate the front-end types, using the following command from the webapp folder while the back end is running (see how to launch here ). cd webapp yarn types # while the back end is runnnig","title":"Front End Types"},{"location":"development/dev-practices/#pre-commit-hooks","text":"When you commit, the pre-commit hooks will automatically be run to format the code included in the commit. We have different hooks which will run mypy , reorder imports, clean unnecessary white-spacing, and perform typical formatting with black , flake8 and prettier . In some cases, you might need to format using make format . However, in most cases, pre-commit hooks will make the necessary changes automatically. mypy errors will never prevent a commit, but it will print the errors so that you can fix them if you introduced a new one. How to temporarily disable pre-commits? For a specific commit, you can avoid running the pre-commits with the flag --no-verify . If you do this, make sure to run pre-commit run --all-files before opening a PR, which will run the pre-commit on all files.","title":"Pre-commit Hooks"},{"location":"development/dev-practices/#testing","text":"It might be that all tests don't pass at each commit, and this is normal. Tests can take a while to run, and so you might want to run them only at specific moments, such as before opening a new PR. When ready to run tests, you can do the following commmands. make format # It will fix formatting issues. make test # It will check for formatting issues, run mypy and all unit tests. You can also only run specific tests, using poetry run pytest {path_to_test.py} . Clean cache for routers tests For tests in tests/test_routers , we run the startup task once and save the result in /tmp/azimuth_test_cache . When modifying Modules, you need to clean it manually or by running make clean . Regularly launch the app Even when all unit tests pass, it may happen that some new issues arise when launching the app. When doing important code changes, it is crucial to launch the app and see if the behavior is as expected. Details on launching the app are available here .","title":"Testing"},{"location":"development/dev-practices/#opening-a-pull-request-pr","text":"We have a template for when you open a PR. Follow instructions and check the boxes to acknowledge that you have done the required steps. Regarding the CHANGELOG.md , you will need to add your changes only if it is visible for the end-users. This helps us when building the release notes.","title":"Opening a Pull Request (PR)"},{"location":"development/launching/","text":"Launching The app can be launched on different data and models, which are defined in a config file. Different config files are readily available in the repo, under config/ . TLDR to launch the app locally Assuming you already installed poetry and yarn (See Initial Setup ), you can run the app by performing: make local_configs # modify all configs for local usage make CFG_PATH ={ path to config file } launch.local # Start the backend. {path to config file} should be the path in local_configs . Ex for clinc dummy: local_configs/development/clinc_dummy/conf.json In another terminal: cd webapp && yarn start # Starts the frontend. Launching locally When debugging, it is easier to launch the back end and the front end separately, as shown in the next two sections. However, it is also possible to launch both together using Docker , as shown in the third section below. It will just take longer, and does not allow for fast debugging. Back End Config files need to be regenerated for local development. make local_configs This will create a new folder /local_configs under the project root and will perform the necessary config updates to run locally. You can then launch the back end using: make CFG_PATH={path to config file} launch.local For {path to config file} , use the path to the file in local_configs rather than configs . With clinc_dummy , this will be: make CFG_PATH=local_configs/development/clinc_dummy/conf.json launch.local Error in the Back End? If you get an error while launching the back end, common causes can be that the poetry has new dependencies or the configs were changed. Be sure to run: poetry install make local_configs If you get No module named '_lzma' ... You may be missing the xz library (https://tukaani.org/xz/). That would result in a ModuleNotFoundError: No module named '_lzma' when you try to run locally. This may be corrected by using homebrew to install it. The following instructions are inspired from those . brew install xz pyenv uninstall 3 .8.9 # If you already had it - otherwise directly go to the steps pyenv install 3 .8.9 # Reinstall (now with the lzma lib available) pyenv local 3 .8.9 # Set this version to always run in this directory From this point, the back end will launch and compute the start-up task. You can consult the openapi documentation at localhost:8091/docs . From there, you can consult the API documentation and try out the different endpoints. This can be useful for debugging. Note that the back end will not reload automatically based on code changes. Cleaning the Cache If you make changes to back-end modules that result in different module responses, you will need to delete the cache, using: make clean As soon as you have an error in the start-up task, a good reflex is to use this command. Optionally, you can clean a single project by supplying the project name as an argument. make TARGET=CLINC clean will delete all cache folders beginning with CLINC . Front End You can then launch the front end using: cd webapp yarn # Only needed when dependencies were updated. yarn start The front-end app will launch and automatically connect to the back-end API, assuming you launched it already. The front end will hot reload automatically based on code changes. Error in the Front End? If you get an error while launching the front end, make sure that the dependencies were updated using yarn . Launch Using Docker Docker compose will build the images and connect them, using the following command. make CFG_PATH=/config/examples/.../conf.json launch Note that the path starts with /config as we mount ./config:/config .","title":"Launching"},{"location":"development/launching/#launching","text":"The app can be launched on different data and models, which are defined in a config file. Different config files are readily available in the repo, under config/ . TLDR to launch the app locally Assuming you already installed poetry and yarn (See Initial Setup ), you can run the app by performing: make local_configs # modify all configs for local usage make CFG_PATH ={ path to config file } launch.local # Start the backend. {path to config file} should be the path in local_configs . Ex for clinc dummy: local_configs/development/clinc_dummy/conf.json In another terminal: cd webapp && yarn start # Starts the frontend.","title":"Launching"},{"location":"development/launching/#launching-locally","text":"When debugging, it is easier to launch the back end and the front end separately, as shown in the next two sections. However, it is also possible to launch both together using Docker , as shown in the third section below. It will just take longer, and does not allow for fast debugging.","title":"Launching locally"},{"location":"development/launching/#back-end","text":"Config files need to be regenerated for local development. make local_configs This will create a new folder /local_configs under the project root and will perform the necessary config updates to run locally. You can then launch the back end using: make CFG_PATH={path to config file} launch.local For {path to config file} , use the path to the file in local_configs rather than configs . With clinc_dummy , this will be: make CFG_PATH=local_configs/development/clinc_dummy/conf.json launch.local Error in the Back End? If you get an error while launching the back end, common causes can be that the poetry has new dependencies or the configs were changed. Be sure to run: poetry install make local_configs If you get No module named '_lzma' ... You may be missing the xz library (https://tukaani.org/xz/). That would result in a ModuleNotFoundError: No module named '_lzma' when you try to run locally. This may be corrected by using homebrew to install it. The following instructions are inspired from those . brew install xz pyenv uninstall 3 .8.9 # If you already had it - otherwise directly go to the steps pyenv install 3 .8.9 # Reinstall (now with the lzma lib available) pyenv local 3 .8.9 # Set this version to always run in this directory From this point, the back end will launch and compute the start-up task. You can consult the openapi documentation at localhost:8091/docs . From there, you can consult the API documentation and try out the different endpoints. This can be useful for debugging. Note that the back end will not reload automatically based on code changes. Cleaning the Cache If you make changes to back-end modules that result in different module responses, you will need to delete the cache, using: make clean As soon as you have an error in the start-up task, a good reflex is to use this command. Optionally, you can clean a single project by supplying the project name as an argument. make TARGET=CLINC clean will delete all cache folders beginning with CLINC .","title":"Back End"},{"location":"development/launching/#front-end","text":"You can then launch the front end using: cd webapp yarn # Only needed when dependencies were updated. yarn start The front-end app will launch and automatically connect to the back-end API, assuming you launched it already. The front end will hot reload automatically based on code changes. Error in the Front End? If you get an error while launching the front end, make sure that the dependencies were updated using yarn .","title":"Front End"},{"location":"development/launching/#launch-using-docker","text":"Docker compose will build the images and connect them, using the following command. make CFG_PATH=/config/examples/.../conf.json launch Note that the path starts with /config as we mount ./config:/config .","title":"Launch Using Docker"},{"location":"development/setup/","text":"Initial Setup Install Dependencies Install pyenv (MacOS) We use python 3.8.9 for the back end in our repo, but python >= 3.8 should work in general. You should use pyenv to set up the right python version for the project. Install brew (if not already installed). Install the xz library (if not already installed). brew install xz Then install pyenv and set the right python version. brew install pyenv pyenv install 3 .8.9 pyenv local 3 .8.9 # You can use global or local. Add this to your ~/.zshrc : export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init - ) \" Run zsh to open a new shell (with the updated ~/.zshrc ). zsh Check that the version of python is the same as you set to pyenv local . python --version If it doesn't match... It might be due to a wrong ordering in your $PATH . Print it and make sure that $PYENV_ROOT/bin is at the beginning. Edit ~/.zshrc accordingly. echo $PATH Install Poetry We use poetry as our dependency manager for the back end. Install poetry . Check that the version of python that poetry uses is the same as you set to pyenv local . poetry run python --version If it prints a warning mentioning another python version... You might need to force poetry to use the right python environment. poetry env use $( which python ) Install the dependencies. poetry install Docker (Optional) Docker is needed for different tasks such as releasing and updating the documentation. However, it won't be needed at first to develop and launch the app. You can skip this step and wait until you are required to use it. Install Docker Desktop . If you are using a Mac, check \"Apple logo\" > \"About This Mac\" to know if you have a Mac with Intel Chip or a Mac with Apple Chip . Set the memory to at least 9 GB in Docker > Preference > Resources. Front End Install Node.js version 16. If you need to use different versions of Node on your machine, you may be interested in NVM . Otherwise, the simplest way to install Node on you Mac is to use Homebrew (and follow directions to add this version to your PATH). brew install node@16 Once you've installed Node , you can install yarn . npm install -g yarn You can then install front-end dependencies, from the webapp folder, using: cd webapp yarn Setting up the development environment Install IDE If developing in back end, we recommend installing PyCharm: Install PyCharm . In PyCharm preferences, you can add your existing python virtual environment as the \"Python Interpreter\", pointing where what poetry run which python prints. If developing in front end, we recommend installing Visual Studio Code: Install Visual Studio Code A pre-defined configuration is available in the repo, to help development. Two things to do: View > Extensions > search for esbenp.prettier-vscode and install it. That's the official Code formatter using prettier by publisher Prettier . File > Open Workspace from File > select azimuth.code-workspace , which will set up two folders: webapp and . (for the rest). In the webapp folder, webapp/.vscode/settings.json will configure Prettier to format your files on save. Pre-commit Hooks We installed pre-commit hooks which will format automatically your code with each commit. The first time, you need to run the following command. poetry run pre-commit install","title":"Initial Setup"},{"location":"development/setup/#initial-setup","text":"","title":"Initial Setup"},{"location":"development/setup/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"development/setup/#install-pyenv-macos","text":"We use python 3.8.9 for the back end in our repo, but python >= 3.8 should work in general. You should use pyenv to set up the right python version for the project. Install brew (if not already installed). Install the xz library (if not already installed). brew install xz Then install pyenv and set the right python version. brew install pyenv pyenv install 3 .8.9 pyenv local 3 .8.9 # You can use global or local. Add this to your ~/.zshrc : export PYENV_ROOT = \" $HOME /.pyenv\" export PATH = \" $PYENV_ROOT /bin: $PATH \" eval \" $( pyenv init - ) \" Run zsh to open a new shell (with the updated ~/.zshrc ). zsh Check that the version of python is the same as you set to pyenv local . python --version If it doesn't match... It might be due to a wrong ordering in your $PATH . Print it and make sure that $PYENV_ROOT/bin is at the beginning. Edit ~/.zshrc accordingly. echo $PATH","title":"Install pyenv (MacOS)"},{"location":"development/setup/#install-poetry","text":"We use poetry as our dependency manager for the back end. Install poetry . Check that the version of python that poetry uses is the same as you set to pyenv local . poetry run python --version If it prints a warning mentioning another python version... You might need to force poetry to use the right python environment. poetry env use $( which python ) Install the dependencies. poetry install","title":"Install Poetry"},{"location":"development/setup/#docker-optional","text":"Docker is needed for different tasks such as releasing and updating the documentation. However, it won't be needed at first to develop and launch the app. You can skip this step and wait until you are required to use it. Install Docker Desktop . If you are using a Mac, check \"Apple logo\" > \"About This Mac\" to know if you have a Mac with Intel Chip or a Mac with Apple Chip . Set the memory to at least 9 GB in Docker > Preference > Resources.","title":"Docker (Optional)"},{"location":"development/setup/#front-end","text":"Install Node.js version 16. If you need to use different versions of Node on your machine, you may be interested in NVM . Otherwise, the simplest way to install Node on you Mac is to use Homebrew (and follow directions to add this version to your PATH). brew install node@16 Once you've installed Node , you can install yarn . npm install -g yarn You can then install front-end dependencies, from the webapp folder, using: cd webapp yarn","title":"Front End"},{"location":"development/setup/#setting-up-the-development-environment","text":"","title":"Setting up the development environment"},{"location":"development/setup/#install-ide","text":"If developing in back end, we recommend installing PyCharm: Install PyCharm . In PyCharm preferences, you can add your existing python virtual environment as the \"Python Interpreter\", pointing where what poetry run which python prints. If developing in front end, we recommend installing Visual Studio Code: Install Visual Studio Code A pre-defined configuration is available in the repo, to help development. Two things to do: View > Extensions > search for esbenp.prettier-vscode and install it. That's the official Code formatter using prettier by publisher Prettier . File > Open Workspace from File > select azimuth.code-workspace , which will set up two folders: webapp and . (for the rest). In the webapp folder, webapp/.vscode/settings.json will configure Prettier to format your files on save.","title":"Install IDE"},{"location":"development/setup/#pre-commit-hooks","text":"We installed pre-commit hooks which will format automatically your code with each commit. The first time, you need to run the following command. poetry run pre-commit install","title":"Pre-commit Hooks"},{"location":"getting-started/","text":"Getting Started This section goes through all the steps to get from installing the repo to launching Azimuth on a dataset and models. Installation details how to install the requirements. Learn Basics describes the application repo and the different steps needed based on your dataset and model format. Run on Your Use Case goes through the steps on how to prepare your config file and run Azimuth on a simple use case.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This section goes through all the steps to get from installing the repo to launching Azimuth on a dataset and models. Installation details how to install the requirements. Learn Basics describes the application repo and the different steps needed based on your dataset and model format. Run on Your Use Case goes through the steps on how to prepare your config file and run Azimuth on a simple use case.","title":"Getting Started"},{"location":"getting-started/a-install/","text":"A. Installation Follow the steps to install the necessary items, and download Azimuth . Requirements Install Docker . Docker allows to use our application without installing all the dependencies. You can also use Azimuth without Docker by installing our dependencies with poetry and yarn . It is slightly more complex, but it can be faster to launch Azimuth. The instructions are in Development . [Mac Only] Setup Docker so it has enough memory to run Azimuth. Right-click on the Docker icon on the toolbar of your computer. Go to Resources and set the \u201cMemory\u201d slider to 9Gb or more. Click on \u201cApply and Restart\u201d. Download Azimuth 's latest release. On GitHub , git clone our repo. Installation completed Congratulations ! You have installed the necessary items and installed Azimuth. Proceed to Learn Basics .","title":"A. Installation"},{"location":"getting-started/a-install/#a-installation","text":"Follow the steps to install the necessary items, and download Azimuth .","title":"A. Installation"},{"location":"getting-started/a-install/#requirements","text":"Install Docker . Docker allows to use our application without installing all the dependencies. You can also use Azimuth without Docker by installing our dependencies with poetry and yarn . It is slightly more complex, but it can be faster to launch Azimuth. The instructions are in Development . [Mac Only] Setup Docker so it has enough memory to run Azimuth. Right-click on the Docker icon on the toolbar of your computer. Go to Resources and set the \u201cMemory\u201d slider to 9Gb or more. Click on \u201cApply and Restart\u201d. Download Azimuth 's latest release. On GitHub , git clone our repo. Installation completed Congratulations ! You have installed the necessary items and installed Azimuth. Proceed to Learn Basics .","title":"Requirements"},{"location":"getting-started/b-basics/","text":"B. Learn Basics Understanding the Application Folder In the downloaded Azimuth folder from step A , the following structure exists: azimuth # Root directory \u251c\u2500\u2500 azimuth \u2502 \u2514\u2500\u2500 # Back End # (7) \u251c\u2500\u2500 azimuth_shr \u2502 \u2514\u2500\u2500 # User dataset, models, and code # (1) \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 # User configs # (2) \u251c\u2500\u2500 webapp \u2502 \u2514\u2500\u2500 # Front End # (8) \u251c\u2500\u2500 .app_env # (3) \u251c\u2500\u2500 docker-compose.yml # (4) \u251c\u2500\u2500 Makefile # (5) \u2514\u2500\u2500 README.md # (6) Where to put your data, model and code, if relevant. azimuth_shr stands for azimuth shared, because it contains user artifacts that are shared with the application. Azimuth provides default artifacts already to load common dataset and models. Where to put all configs. Example configs are provided. Default values for env vars. Where the config and Docker images are specified. Available commands to use Azimuth. Instructions to launch the application. Only available when cloning the repo. Only available when cloning the repo. Where to put your data, code and configs? config and azimuth_shr are two folders where you will put different artifacts before you can run Azimuth on your dataset and models. They get mounted automatically on the Docker image. The Config File The Azimuth config file contains all the information to launch and customize Azimuth. It specifies which dataset and pipelines to load in the app, as well as other variables that allow for customization of the app. Most fields have default values and don't need to be explicitly defined in each config. The Configuration reference details all available fields. Different configuration examples are provided in the repo under config/examples , leveraging pretrained models from HuggingFace . The next step, C. Run on Your Use Case , will detail how to adapt an existing config to create your own. Clearing the Cache Azimuth keeps all artifacts in caching folders so that if you close the app and re-launch, it will load quickly. Once you are done with your analysis, you can delete the cache by running: make clean Run Our Demo to Verify Your Setup Out-of-the-box, Azimuth can run on different demo data and models from HuggingFace ( HF ) . Verify that your setup is working correctly by running a demo. In the terminal, from the azimuth folder (the root directory), execute the following commands. The first one installs the Google Drive downloading library. The second command downloads from Google Drive the demo data and model. Our demo is using a subset of the clinc_oos dataset from HuggingFace, with only 16 classes. pip install gdown make download_demo You cannot install gdown ? Look at the following Discussion to download the data manually. Run our dummy or full demo (option a. or b.), based on how much time you have. If it is the first time that you are running the command, it will take additional time to download the Docker image (~15 min). If you don't have a lot of time and just want to verify your setup, you can run our dummy CLINC demo (~2min): make CFG_PATH=/config/development/clinc_dummy/conf.json launch If you have a bit more time, run our full CLINC demo (~10min): make CFG_PATH=/config/development/clinc/conf.json launch The app will be accessible at http://0.0.0.0:8080 after a few minutes of waiting. The screen will indicate that the start-up tasks have started. When it is completed, the application will be loaded. Skim the Key Concepts section to get a high-level understanding of some concepts used throughout the application. If you are unsure what each screen allows you to do, the User Guide section walks you through all the available interactions on each screen. Successful demo Now that the demo is working, you can adapt the config to make it work on your dataset and model. Proceed to C. Run on Your Use Case .","title":"B. Learn Basics"},{"location":"getting-started/b-basics/#b-learn-basics","text":"","title":"B. Learn Basics"},{"location":"getting-started/b-basics/#understanding-the-application-folder","text":"In the downloaded Azimuth folder from step A , the following structure exists: azimuth # Root directory \u251c\u2500\u2500 azimuth \u2502 \u2514\u2500\u2500 # Back End # (7) \u251c\u2500\u2500 azimuth_shr \u2502 \u2514\u2500\u2500 # User dataset, models, and code # (1) \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 # User configs # (2) \u251c\u2500\u2500 webapp \u2502 \u2514\u2500\u2500 # Front End # (8) \u251c\u2500\u2500 .app_env # (3) \u251c\u2500\u2500 docker-compose.yml # (4) \u251c\u2500\u2500 Makefile # (5) \u2514\u2500\u2500 README.md # (6) Where to put your data, model and code, if relevant. azimuth_shr stands for azimuth shared, because it contains user artifacts that are shared with the application. Azimuth provides default artifacts already to load common dataset and models. Where to put all configs. Example configs are provided. Default values for env vars. Where the config and Docker images are specified. Available commands to use Azimuth. Instructions to launch the application. Only available when cloning the repo. Only available when cloning the repo. Where to put your data, code and configs? config and azimuth_shr are two folders where you will put different artifacts before you can run Azimuth on your dataset and models. They get mounted automatically on the Docker image.","title":"Understanding the Application Folder"},{"location":"getting-started/b-basics/#the-config-file","text":"The Azimuth config file contains all the information to launch and customize Azimuth. It specifies which dataset and pipelines to load in the app, as well as other variables that allow for customization of the app. Most fields have default values and don't need to be explicitly defined in each config. The Configuration reference details all available fields. Different configuration examples are provided in the repo under config/examples , leveraging pretrained models from HuggingFace . The next step, C. Run on Your Use Case , will detail how to adapt an existing config to create your own.","title":"The Config File"},{"location":"getting-started/b-basics/#clearing-the-cache","text":"Azimuth keeps all artifacts in caching folders so that if you close the app and re-launch, it will load quickly. Once you are done with your analysis, you can delete the cache by running: make clean","title":"Clearing the Cache"},{"location":"getting-started/b-basics/#run-our-demo-to-verify-your-setup","text":"Out-of-the-box, Azimuth can run on different demo data and models from HuggingFace ( HF ) . Verify that your setup is working correctly by running a demo. In the terminal, from the azimuth folder (the root directory), execute the following commands. The first one installs the Google Drive downloading library. The second command downloads from Google Drive the demo data and model. Our demo is using a subset of the clinc_oos dataset from HuggingFace, with only 16 classes. pip install gdown make download_demo You cannot install gdown ? Look at the following Discussion to download the data manually. Run our dummy or full demo (option a. or b.), based on how much time you have. If it is the first time that you are running the command, it will take additional time to download the Docker image (~15 min). If you don't have a lot of time and just want to verify your setup, you can run our dummy CLINC demo (~2min): make CFG_PATH=/config/development/clinc_dummy/conf.json launch If you have a bit more time, run our full CLINC demo (~10min): make CFG_PATH=/config/development/clinc/conf.json launch The app will be accessible at http://0.0.0.0:8080 after a few minutes of waiting. The screen will indicate that the start-up tasks have started. When it is completed, the application will be loaded. Skim the Key Concepts section to get a high-level understanding of some concepts used throughout the application. If you are unsure what each screen allows you to do, the User Guide section walks you through all the available interactions on each screen. Successful demo Now that the demo is working, you can adapt the config to make it work on your dataset and model. Proceed to C. Run on Your Use Case .","title":"Run Our Demo to Verify Your Setup"},{"location":"getting-started/c-run/","text":"C. Run on Your Use Case This page guides you through the process of running the app on your data and pipelines, using Docker. Different dataset and text classification models can be supported in Azimuth. Launch Azimuth with no pipeline, or with multiple pipelines Azimuth supports specifying no pipelines , to only perform dataset analysis. It also supports supplying mulitple pipelines , to allow for quick comparison. However, only one dataset per config is allowed. The simplest scenario is if you have a HuggingFace ( HF ) dataset and model . For the sake of simplicity, we explain the instructions to run the app with this scenario. However, you will quickly need to learn about the Configuration details and Custom Objects to launch more complex use cases. 1. Prepare the Config File Run our demo first You haven't run our demo yet? You might want to verify your setup before feeding your own model and dataset. Go back to B. Learn Basics . Start from an existing config and edit the relevant fields to adapt it to your dataset and models. Examples with an HuggingFace ( HF ) dataset and model are available in config/examples ( CLINC is also shown below). Put your model checkpoint (results of .save_pretained() ) under the folder azimuth_shr . In config , copy config/examples/clinc_oos/conf.json to a new folder with your project name. Ex: config/my_project/conf.json . Edit the config: name : put your project name. dataset.args : specify the args required to load your dataset with datasets.load_dataset . Edit columns and rejection_class based on the dataset. pipelines.models.kwargs.checkpoint_path : put your own checkpoint path to your model. The path should start with /azimuth_shr , since this folder will be mounted on Docker. Edit the saliency_layer so it is the name of the input layer of the model. It should be set to null if your model is not from PyTorch or without a word-embedding layer. Links to full reference If you need more details on some of these fields: The Project Config explains in more details name , dataset , columns and rejection_class . The Model Contract Config details how to define pipelines , model_contract and saliency_layer . { \"name\" : \"CLINC-151\" , # (1) \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , # (2) \"args\" : [ # (3) \"clinc_oos\" , \"imbalanced\" ] }, \"columns\" : { # (4) \"text_input\" : \"text\" , \"label\" : \"intent\" }, \"rejection_class\" : \"oos\" , # (5) \"model_contract\" : \"hf_text_classification\" , # (6) \"pipelines\" : [ # (7) { \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , # (8) \"remote\" : \"/azimuth_shr\" , # (9) \"kwargs\" : { # (10) \"checkpoint_path\" : \"transformersbook/ distilbert-base-uncased-distilled-clinc\" } } } ], \"saliency_layer\" : \"distilbert.embeddings.word_embeddings\" , # (11) } Name for your project. Shown in the application to identify your config. If the dataset is a HF dataset, use this class_name . kwargs to send to the class_name . Specify the name of the dataset columns, such as the column with the utterance and the label. Specify the value if a rejection option is present in the classes. If the pipeline is a HF pipeline, use this model_contract . Multiples ML pipelines can be listed to be available in the webapp. If this a HF pipeline, use this class_name . Change only if class_name is not found in /azimuth_shr . kwargs to send to the class. Only checkpoint_path if you use the class above. Name of the layer on which to compute saliency maps. 2. Running the App In the terminal, go to the azimuth root directory . Set CFG_PATH=/config/my_project/conf.json with the location of the config . The initial / is required as your local config folder will be mounted on the Docker container at the root. Execute the following command : make launch The app will be accessible at http://0.0.0.0:8080 after a few minutes of waiting. The start-up tasks will start. Advanced Settings Additional Config Fields The Configuration reference details all additional fields that can be set, such as changing how behavioral tests are executed, the similarity analysis encoder, the batch size and so on. Environment variables No matter where you launch the app from, you can always configure some options through environment variables. They are all redundant with the config attributes, so you can set them in either place. They are the following: Specify the threshold of your model by passing TH (ex: TH=0.6 or NaN if there is no threshold) in the command. If multiple pipelines are defined, the threshold will apply to all. Similarly, pass TEMP=Y (ex: TEMP=3 ) to set the temperature of the model. Disable behavioral tests and similarity by passing respectively BEHAVIORAL_TESTING=null and SIMILARITY=null . Specify the name of the project, passing NAME . You can specify the device on which to run Azimuth, with DEVICE being one of auto , gpu or cpu . If none is provided, auto will be used. Ex: DEVICE=gpu . Specify READ_ONLY_CONFIG=1 to lock the config once Azimuth is launched. Config file prevails over environment variables Remember that the values above are defined in the config too. If conflicting values are defined, values from the config file will prevail.","title":"C. Run on Your Use Case"},{"location":"getting-started/c-run/#c-run-on-your-use-case","text":"This page guides you through the process of running the app on your data and pipelines, using Docker. Different dataset and text classification models can be supported in Azimuth. Launch Azimuth with no pipeline, or with multiple pipelines Azimuth supports specifying no pipelines , to only perform dataset analysis. It also supports supplying mulitple pipelines , to allow for quick comparison. However, only one dataset per config is allowed. The simplest scenario is if you have a HuggingFace ( HF ) dataset and model . For the sake of simplicity, we explain the instructions to run the app with this scenario. However, you will quickly need to learn about the Configuration details and Custom Objects to launch more complex use cases.","title":"C. Run on Your Use Case"},{"location":"getting-started/c-run/#1-prepare-the-config-file","text":"Run our demo first You haven't run our demo yet? You might want to verify your setup before feeding your own model and dataset. Go back to B. Learn Basics . Start from an existing config and edit the relevant fields to adapt it to your dataset and models. Examples with an HuggingFace ( HF ) dataset and model are available in config/examples ( CLINC is also shown below). Put your model checkpoint (results of .save_pretained() ) under the folder azimuth_shr . In config , copy config/examples/clinc_oos/conf.json to a new folder with your project name. Ex: config/my_project/conf.json . Edit the config: name : put your project name. dataset.args : specify the args required to load your dataset with datasets.load_dataset . Edit columns and rejection_class based on the dataset. pipelines.models.kwargs.checkpoint_path : put your own checkpoint path to your model. The path should start with /azimuth_shr , since this folder will be mounted on Docker. Edit the saliency_layer so it is the name of the input layer of the model. It should be set to null if your model is not from PyTorch or without a word-embedding layer. Links to full reference If you need more details on some of these fields: The Project Config explains in more details name , dataset , columns and rejection_class . The Model Contract Config details how to define pipelines , model_contract and saliency_layer . { \"name\" : \"CLINC-151\" , # (1) \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , # (2) \"args\" : [ # (3) \"clinc_oos\" , \"imbalanced\" ] }, \"columns\" : { # (4) \"text_input\" : \"text\" , \"label\" : \"intent\" }, \"rejection_class\" : \"oos\" , # (5) \"model_contract\" : \"hf_text_classification\" , # (6) \"pipelines\" : [ # (7) { \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , # (8) \"remote\" : \"/azimuth_shr\" , # (9) \"kwargs\" : { # (10) \"checkpoint_path\" : \"transformersbook/ distilbert-base-uncased-distilled-clinc\" } } } ], \"saliency_layer\" : \"distilbert.embeddings.word_embeddings\" , # (11) } Name for your project. Shown in the application to identify your config. If the dataset is a HF dataset, use this class_name . kwargs to send to the class_name . Specify the name of the dataset columns, such as the column with the utterance and the label. Specify the value if a rejection option is present in the classes. If the pipeline is a HF pipeline, use this model_contract . Multiples ML pipelines can be listed to be available in the webapp. If this a HF pipeline, use this class_name . Change only if class_name is not found in /azimuth_shr . kwargs to send to the class. Only checkpoint_path if you use the class above. Name of the layer on which to compute saliency maps.","title":"1. Prepare the Config File"},{"location":"getting-started/c-run/#2-running-the-app","text":"In the terminal, go to the azimuth root directory . Set CFG_PATH=/config/my_project/conf.json with the location of the config . The initial / is required as your local config folder will be mounted on the Docker container at the root. Execute the following command : make launch The app will be accessible at http://0.0.0.0:8080 after a few minutes of waiting. The start-up tasks will start.","title":"2. Running the App"},{"location":"getting-started/c-run/#advanced-settings","text":"","title":"Advanced Settings"},{"location":"getting-started/c-run/#additional-config-fields","text":"The Configuration reference details all additional fields that can be set, such as changing how behavioral tests are executed, the similarity analysis encoder, the batch size and so on.","title":"Additional Config Fields"},{"location":"getting-started/c-run/#environment-variables","text":"No matter where you launch the app from, you can always configure some options through environment variables. They are all redundant with the config attributes, so you can set them in either place. They are the following: Specify the threshold of your model by passing TH (ex: TH=0.6 or NaN if there is no threshold) in the command. If multiple pipelines are defined, the threshold will apply to all. Similarly, pass TEMP=Y (ex: TEMP=3 ) to set the temperature of the model. Disable behavioral tests and similarity by passing respectively BEHAVIORAL_TESTING=null and SIMILARITY=null . Specify the name of the project, passing NAME . You can specify the device on which to run Azimuth, with DEVICE being one of auto , gpu or cpu . If none is provided, auto will be used. Ex: DEVICE=gpu . Specify READ_ONLY_CONFIG=1 to lock the config once Azimuth is launched. Config file prevails over environment variables Remember that the values above are defined in the config too. If conflicting values are defined, values from the config file will prevail.","title":"Environment variables"},{"location":"getting-started/changelog/","text":"Releases [2.4.0] - 2022-10-20 Added New dataset warning : Added new class imbalance warnings. Pipeline Comparison : Added a new pipeline comparison mode in the pipeline metrics table to compare the metrics on different pipelines. New Smart Tag Analysis : Added a new plot where smart tag patterns over classes can be easily examined in one view. Changed Renaming : Some sections were renamed in the UI, such as: Dataset Class Distribution Analysis -> Dataset Warnings Performance Analysis -> Pipeline Metrics by Data Subpopulation Performance Overview -> Prediction Overview Proposed actions : We added a new action, merge_classes , and renamed consider_new_class to define_new_class . Improved Confusion Matrix : The order of the classes in the confusion matrix is now smarter: classes where the model gets similarly confused will be closer to one another. The rejection class is always the last row/column in the confusion matrix. A toggle allows the user to keep the original order from the dataset if preferred. Refactoring : We improved the MetricsPerFilter module (which generates the pipeline metrics by data subpopulation table). It now takes ~5 times less time to compute. New config fields : The memory of the dask cluster can now be set to large (12GB) for bigger models. The config can also be in read-only mode, to prevent users from changing its values. Offline Mode : Azimuth can now be launched without internet. Fixed Fixed an issue related to HuggingFace where filtering on an empty dataset would result in an error. [2.3.0] - 2022-08-17 Added Rows of Performance Analysis table now link to exploration page with filters applied, when the user clicks. Added visual bars to the similarity column in the semantically similar utterances. New attribute in the config allows users to change the thresholds that determine a short or long sentence. Fixed Fixed utterances table poorly showing ids greater than 9999. Fixed filtering of aggregation modules without post-processing. Fixed high_epistemic_uncertainty smart tag which wasn't showing in the UI. Fixed crash when hitting See more when it would show over 100 rows in Performance Analysis table. [2.2.3] - 2022-07-25 Fixed Fixed losing hidden columns from Performance Analysis table when changing view (Label, Prediction, etc.). Fixed utterances table poorly showing ids greater than 99, now supporting up to 9999. [2.2.2] - 2022-07-19 Fixed Fixed losing hidden columns when sorting Performance Analysis table, clicking See more / less , or switching dataset split or pipeline. [2.2.1] - 2022-07-15 Fixed Fixed pipeline comparison smart tag family not working as a filter on the Exploration space. [2.2.0] - 2022-07-08 Added Contextual information has been added to most functionalities, linking directly to the relevant documentation. The F1 metric was added as a default metric. Changed Smart tags are now grouped in families, so it is easier to digest them. This also impacts how filters are displayed. The confusion matrix can be shown with raw values (not just normalized). We now show the top 20, instead of 10, most similar examples in the utterances details. Renamed our docker images to use servicenowdocker registry on Docker Hub. Performance Overview Table Columns can be temporarily hidden. Use outcomes' icons as headers instead of the outcomes' full names. Added visual bars, so it is easier to analyze the model's performance. [2.1.1] - 2022-06-06 Changed Our Docker images are now available through Docker Hub Renamed our docker images to use servicenowdocker registry on Docker Hub. Users can now use make launch instead of make compose . Add option DEVICE=auto to automatically run Azimuth on GPU if available. As such, DEVICE=cpu or DEVICE=gpu does not need to be specified. We added analytics in our documentation, which will add a pop-up to accept cookies when users first access the page. Fixed Fixed issue where the performance analysis table showed empty results for some smart tags. Fixed BEHAVIORAL_TESTING environment variable. Fixed unexpected borders in Performance Analysis table. Fixed proposed actions which couldn't be applied on an utterance in the latest release. [2.1.0] - 2022-05-27 Ability to get predictions without postprocessing in the exploration space. See section \"Excluding Post-Processing\" here . New Smart Tags pipeline_disagreement and incorrect_for_all_pipelines as a first step for pipeline comparison. See section \"Pipeline Comparison\" here . Links on top words to filter utterances that contain it. See the section \"Word Clouds\" here . [2.0.0] - 2022-04-12 First public release.","title":"Releases"},{"location":"getting-started/changelog/#releases","text":"","title":"Releases"},{"location":"getting-started/changelog/#240-2022-10-20","text":"","title":"[2.4.0] - 2022-10-20"},{"location":"getting-started/changelog/#added","text":"New dataset warning : Added new class imbalance warnings. Pipeline Comparison : Added a new pipeline comparison mode in the pipeline metrics table to compare the metrics on different pipelines. New Smart Tag Analysis : Added a new plot where smart tag patterns over classes can be easily examined in one view.","title":"Added"},{"location":"getting-started/changelog/#changed","text":"Renaming : Some sections were renamed in the UI, such as: Dataset Class Distribution Analysis -> Dataset Warnings Performance Analysis -> Pipeline Metrics by Data Subpopulation Performance Overview -> Prediction Overview Proposed actions : We added a new action, merge_classes , and renamed consider_new_class to define_new_class . Improved Confusion Matrix : The order of the classes in the confusion matrix is now smarter: classes where the model gets similarly confused will be closer to one another. The rejection class is always the last row/column in the confusion matrix. A toggle allows the user to keep the original order from the dataset if preferred. Refactoring : We improved the MetricsPerFilter module (which generates the pipeline metrics by data subpopulation table). It now takes ~5 times less time to compute. New config fields : The memory of the dask cluster can now be set to large (12GB) for bigger models. The config can also be in read-only mode, to prevent users from changing its values. Offline Mode : Azimuth can now be launched without internet.","title":"Changed"},{"location":"getting-started/changelog/#fixed","text":"Fixed an issue related to HuggingFace where filtering on an empty dataset would result in an error.","title":"Fixed"},{"location":"getting-started/changelog/#230-2022-08-17","text":"","title":"[2.3.0] - 2022-08-17"},{"location":"getting-started/changelog/#added_1","text":"Rows of Performance Analysis table now link to exploration page with filters applied, when the user clicks. Added visual bars to the similarity column in the semantically similar utterances. New attribute in the config allows users to change the thresholds that determine a short or long sentence.","title":"Added"},{"location":"getting-started/changelog/#fixed_1","text":"Fixed utterances table poorly showing ids greater than 9999. Fixed filtering of aggregation modules without post-processing. Fixed high_epistemic_uncertainty smart tag which wasn't showing in the UI. Fixed crash when hitting See more when it would show over 100 rows in Performance Analysis table.","title":"Fixed"},{"location":"getting-started/changelog/#223-2022-07-25","text":"","title":"[2.2.3] - 2022-07-25"},{"location":"getting-started/changelog/#fixed_2","text":"Fixed losing hidden columns from Performance Analysis table when changing view (Label, Prediction, etc.). Fixed utterances table poorly showing ids greater than 99, now supporting up to 9999.","title":"Fixed"},{"location":"getting-started/changelog/#222-2022-07-19","text":"","title":"[2.2.2] - 2022-07-19"},{"location":"getting-started/changelog/#fixed_3","text":"Fixed losing hidden columns when sorting Performance Analysis table, clicking See more / less , or switching dataset split or pipeline.","title":"Fixed"},{"location":"getting-started/changelog/#221-2022-07-15","text":"","title":"[2.2.1] - 2022-07-15"},{"location":"getting-started/changelog/#fixed_4","text":"Fixed pipeline comparison smart tag family not working as a filter on the Exploration space.","title":"Fixed"},{"location":"getting-started/changelog/#220-2022-07-08","text":"","title":"[2.2.0] - 2022-07-08"},{"location":"getting-started/changelog/#added_2","text":"Contextual information has been added to most functionalities, linking directly to the relevant documentation. The F1 metric was added as a default metric.","title":"Added"},{"location":"getting-started/changelog/#changed_1","text":"Smart tags are now grouped in families, so it is easier to digest them. This also impacts how filters are displayed. The confusion matrix can be shown with raw values (not just normalized). We now show the top 20, instead of 10, most similar examples in the utterances details. Renamed our docker images to use servicenowdocker registry on Docker Hub.","title":"Changed"},{"location":"getting-started/changelog/#performance-overview-table","text":"Columns can be temporarily hidden. Use outcomes' icons as headers instead of the outcomes' full names. Added visual bars, so it is easier to analyze the model's performance.","title":"Performance Overview Table"},{"location":"getting-started/changelog/#211-2022-06-06","text":"","title":"[2.1.1] - 2022-06-06"},{"location":"getting-started/changelog/#changed_2","text":"Our Docker images are now available through Docker Hub Renamed our docker images to use servicenowdocker registry on Docker Hub. Users can now use make launch instead of make compose . Add option DEVICE=auto to automatically run Azimuth on GPU if available. As such, DEVICE=cpu or DEVICE=gpu does not need to be specified. We added analytics in our documentation, which will add a pop-up to accept cookies when users first access the page.","title":"Changed"},{"location":"getting-started/changelog/#fixed_5","text":"Fixed issue where the performance analysis table showed empty results for some smart tags. Fixed BEHAVIORAL_TESTING environment variable. Fixed unexpected borders in Performance Analysis table. Fixed proposed actions which couldn't be applied on an utterance in the latest release.","title":"Fixed"},{"location":"getting-started/changelog/#210-2022-05-27","text":"Ability to get predictions without postprocessing in the exploration space. See section \"Excluding Post-Processing\" here . New Smart Tags pipeline_disagreement and incorrect_for_all_pipelines as a first step for pipeline comparison. See section \"Pipeline Comparison\" here . Links on top words to filter utterances that contain it. See the section \"Word Clouds\" here .","title":"[2.1.0] - 2022-05-27"},{"location":"getting-started/changelog/#200-2022-04-12","text":"First public release.","title":"[2.0.0] - 2022-04-12"},{"location":"key-concepts/","text":"Key Concepts Azimuth leverages different analyses and concepts to enhance the process of dataset analysis and error analysis . The notion of smart tags is the most important concept, as it unifies most of the other analyses. Smart Tags Smart tags are assigned to utterances by Azimuth when the app is launched. They can be seen as meta-data on the utterance and/or its prediction . The goal is to guide the error analysis process , identifying interesting data samples which may require further action and investigation . Different families of smart tags exist, based on the different types of analyses that Azimuth provides. Smart tag examples Examples of smart tag families : partial syntax : identifies utterances with a partial syntax, e.g. missing a verb. behavioral testing : identifies utterances which failed at least one behavioral test. Examples of individual smart tags: long_sentences identifies utterances with more than X tokens. failed_punctuation identifies utterances that failed at least one punctuation test. The full list of smart tags is available in Smart Tags . Proposed Actions While smart tags are computed automatically and cannot be changed, proposed actions are annotations that can be added by the user to identify a proposed action that should be done on a specific data sample. Proposed action examples relabel to identify data samples whose labels should be changed. remove to identify data samples that should be removed from the dataset. A dedicated page on Proposed Actions gives the full list of available actions. Prediction Outcomes Another key concept used through the application is the notion of prediction outcomes. It acts as a metric of success for a given prediction. More details are available in Prediction outcomes . Correct & Predicted Correct & Rejected Incorrect & Rejected Incorrect & Predicted Analyses In Azimuth, different types of analysis are provided. Each analysis has a dedicated section in the documentation. Almost all of them (except saliency maps) are linked to smart tags. Saliency Maps Syntax Analysis Similarity Analysis Behavioral Testing Uncertainty Estimation","title":"Key Concepts"},{"location":"key-concepts/#key-concepts","text":"Azimuth leverages different analyses and concepts to enhance the process of dataset analysis and error analysis . The notion of smart tags is the most important concept, as it unifies most of the other analyses.","title":"Key Concepts"},{"location":"key-concepts/#smart-tags","text":"Smart tags are assigned to utterances by Azimuth when the app is launched. They can be seen as meta-data on the utterance and/or its prediction . The goal is to guide the error analysis process , identifying interesting data samples which may require further action and investigation . Different families of smart tags exist, based on the different types of analyses that Azimuth provides. Smart tag examples Examples of smart tag families : partial syntax : identifies utterances with a partial syntax, e.g. missing a verb. behavioral testing : identifies utterances which failed at least one behavioral test. Examples of individual smart tags: long_sentences identifies utterances with more than X tokens. failed_punctuation identifies utterances that failed at least one punctuation test. The full list of smart tags is available in Smart Tags .","title":"Smart Tags"},{"location":"key-concepts/#proposed-actions","text":"While smart tags are computed automatically and cannot be changed, proposed actions are annotations that can be added by the user to identify a proposed action that should be done on a specific data sample. Proposed action examples relabel to identify data samples whose labels should be changed. remove to identify data samples that should be removed from the dataset. A dedicated page on Proposed Actions gives the full list of available actions.","title":"Proposed Actions"},{"location":"key-concepts/#prediction-outcomes","text":"Another key concept used through the application is the notion of prediction outcomes. It acts as a metric of success for a given prediction. More details are available in Prediction outcomes . Correct & Predicted Correct & Rejected Incorrect & Rejected Incorrect & Predicted","title":"Prediction Outcomes"},{"location":"key-concepts/#analyses","text":"In Azimuth, different types of analysis are provided. Each analysis has a dedicated section in the documentation. Almost all of them (except saliency maps) are linked to smart tags. Saliency Maps Syntax Analysis Similarity Analysis Behavioral Testing Uncertainty Estimation","title":"Analyses"},{"location":"key-concepts/behavioral-testing/","text":"Behavioral Testing What is it? Performing behavioral testing on ML models was first introduced in the checklist paper ( Ribeiro, Marco Tulio, et al., 2020 1 ). Behavioral tests provide an assessment of the model robustness to small modifications to the input . Proper behavioral testing can help in detecting bias or other potential harmful aspects of the model that may not be otherwise obvious. Where is this used in Azimuth? In Azimuth, behavioral tests are automatically executed when launching the tool, using the provided dataset and model. The details of all the tests that were computed for a given utterance are shown in the Utterance Details . A summary of each test for all utterances in both dataset splits (training and evaluation) is available in the Behavioral Testing Summary . Finally, a Smart Tag is generated for each utterance for which at least one test of each family has failed. How is it computed? The tests are deterministic for reproducibility purposes. Test Failing Criteria The tests can fail for two reasons. The test will fail if the predicted class for the modified utterance is different from the predicted class of the original utterance. Failing examples Original Utterance Predicted Class Azimuth is the best tool positive Modified Utterance Predicted Class Test fails? Hello Azimuth is the best tool positive NO Azimuth is the best tool!!! negative YES The test will fail if the confidence associated with the predicted class of the modified utterance is too different (based on a threshold) from the confidence of the original utterance. By default, the threshold is set to 1, meaning the tests will never fail due to a change in confidence for the same predicted class. Failing examples Original Utterance Predicted Class Confidence Azimuth is the best tool positive 95% Threshold set to 0.1: Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool positive 82% YES Threshold set to 1: Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool positive 82% NO Available Tests All tests are invariant (the modification should not change the predicted class) and assess the robustness of the model. The tool currently has 2 families of tests: Fuzzy Matching and Punctuation . For each test, different modification types can be applied ( Insertion , Deletion , etc.) For certain tests, all modification types are applied to each utterance (e.g., Typos and Neutral Token ). For others, only one modification type is applied based on the presence of a certain pattern in the utterance (e.g., Punctuation and Contractions tests). Fuzzy Matching Typos : For this test, we simulate common typos that might happen when typing an utterance. By default, the test creates one typo per utterance. The different types of simulated typos ( modification types ) are: Swap : Random swap of two adjacent characters in a word. Deletion : Deletion of random characters in a word. Replacement : Keyboard proximity-based typos inserted in a word. Neutral Token : Default neutral tokens are added to the utterance. PreInsertion : One of [\"pls\", \"please\", \"hello\", \"greetings\"] is added at the beginning of the utterance. PostInsertion : One of [\"pls\", \"please\", \"thank you\", \"appreciated\"] is added at the end of an utterance. Contractions : This test is applied only when the utterance contains a relevant expression that can be contracted or expanded. The list is taken from NL-Augmenter . Contraction : Contract relevant expressions, if present. Expansion : Expand relevant expressions, if present. Punctuation Question Mark : Adds/Deletes/Replaces question marks. Deletion : Removes the ending question mark, if present. Replacement : Replaces the ending punctuation sign ('.', '!', ','), if present, by a question mark. PostInsertion : Adds an ending question mark when the utterance does not end with a punctuation sign. Ending period : Same logic as the Question Mark test, with a period. Deletion : Removes the ending period, if present. Replacement : Replaces the ending punctuation sign ('?', '!', ','), if present, by a period. PostInsertion : Adds an ending period when the utterance does not end with a punctuation sign. Inner Comma : Adds/Deletes comma inside the utterance (not at the end). Deletion : Removes all commas inside the utterance, if present. Insertion : Adds a comma near the middle of the utterance. Inner Period : Same logic as the Inner Comma test, with a period. Deletion : Removes all periods inside the utterance, if present. Insertion : Adds a period near the middle of the utterance. Configuration Behavioral Testing Configuration details how to change some parameters, such as the lists of neutral tokens, the number of typos and the threshold confidence delta above which the tests should fail. Ribeiro, Marco Tulio, et al. \"Beyond accuracy: Behavioral testing of NLP models with CheckList.\" Association for Computational Linguistics (ACL), 2020. \u21a9","title":"Behavioral Testing"},{"location":"key-concepts/behavioral-testing/#behavioral-testing","text":"","title":"Behavioral Testing"},{"location":"key-concepts/behavioral-testing/#what-is-it","text":"Performing behavioral testing on ML models was first introduced in the checklist paper ( Ribeiro, Marco Tulio, et al., 2020 1 ). Behavioral tests provide an assessment of the model robustness to small modifications to the input . Proper behavioral testing can help in detecting bias or other potential harmful aspects of the model that may not be otherwise obvious.","title":"What is it?"},{"location":"key-concepts/behavioral-testing/#where-is-this-used-in-azimuth","text":"In Azimuth, behavioral tests are automatically executed when launching the tool, using the provided dataset and model. The details of all the tests that were computed for a given utterance are shown in the Utterance Details . A summary of each test for all utterances in both dataset splits (training and evaluation) is available in the Behavioral Testing Summary . Finally, a Smart Tag is generated for each utterance for which at least one test of each family has failed.","title":"Where is this used in Azimuth?"},{"location":"key-concepts/behavioral-testing/#how-is-it-computed","text":"The tests are deterministic for reproducibility purposes.","title":"How is it computed?"},{"location":"key-concepts/behavioral-testing/#test-failing-criteria","text":"The tests can fail for two reasons. The test will fail if the predicted class for the modified utterance is different from the predicted class of the original utterance. Failing examples Original Utterance Predicted Class Azimuth is the best tool positive Modified Utterance Predicted Class Test fails? Hello Azimuth is the best tool positive NO Azimuth is the best tool!!! negative YES The test will fail if the confidence associated with the predicted class of the modified utterance is too different (based on a threshold) from the confidence of the original utterance. By default, the threshold is set to 1, meaning the tests will never fail due to a change in confidence for the same predicted class. Failing examples Original Utterance Predicted Class Confidence Azimuth is the best tool positive 95% Threshold set to 0.1: Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool positive 82% YES Threshold set to 1: Modified Utterance Predicted Class Confidence Test fails? Hello Azimuth is the best tool positive 82% NO","title":"Test Failing Criteria"},{"location":"key-concepts/behavioral-testing/#available-tests","text":"All tests are invariant (the modification should not change the predicted class) and assess the robustness of the model. The tool currently has 2 families of tests: Fuzzy Matching and Punctuation . For each test, different modification types can be applied ( Insertion , Deletion , etc.) For certain tests, all modification types are applied to each utterance (e.g., Typos and Neutral Token ). For others, only one modification type is applied based on the presence of a certain pattern in the utterance (e.g., Punctuation and Contractions tests).","title":"Available Tests"},{"location":"key-concepts/behavioral-testing/#fuzzy-matching","text":"Typos : For this test, we simulate common typos that might happen when typing an utterance. By default, the test creates one typo per utterance. The different types of simulated typos ( modification types ) are: Swap : Random swap of two adjacent characters in a word. Deletion : Deletion of random characters in a word. Replacement : Keyboard proximity-based typos inserted in a word. Neutral Token : Default neutral tokens are added to the utterance. PreInsertion : One of [\"pls\", \"please\", \"hello\", \"greetings\"] is added at the beginning of the utterance. PostInsertion : One of [\"pls\", \"please\", \"thank you\", \"appreciated\"] is added at the end of an utterance. Contractions : This test is applied only when the utterance contains a relevant expression that can be contracted or expanded. The list is taken from NL-Augmenter . Contraction : Contract relevant expressions, if present. Expansion : Expand relevant expressions, if present.","title":"Fuzzy Matching"},{"location":"key-concepts/behavioral-testing/#punctuation","text":"Question Mark : Adds/Deletes/Replaces question marks. Deletion : Removes the ending question mark, if present. Replacement : Replaces the ending punctuation sign ('.', '!', ','), if present, by a question mark. PostInsertion : Adds an ending question mark when the utterance does not end with a punctuation sign. Ending period : Same logic as the Question Mark test, with a period. Deletion : Removes the ending period, if present. Replacement : Replaces the ending punctuation sign ('?', '!', ','), if present, by a period. PostInsertion : Adds an ending period when the utterance does not end with a punctuation sign. Inner Comma : Adds/Deletes comma inside the utterance (not at the end). Deletion : Removes all commas inside the utterance, if present. Insertion : Adds a comma near the middle of the utterance. Inner Period : Same logic as the Inner Comma test, with a period. Deletion : Removes all periods inside the utterance, if present. Insertion : Adds a period near the middle of the utterance.","title":"Punctuation"},{"location":"key-concepts/behavioral-testing/#configuration","text":"Behavioral Testing Configuration details how to change some parameters, such as the lists of neutral tokens, the number of typos and the threshold confidence delta above which the tests should fail. Ribeiro, Marco Tulio, et al. \"Beyond accuracy: Behavioral testing of NLP models with CheckList.\" Association for Computational Linguistics (ACL), 2020. \u21a9","title":"Configuration"},{"location":"key-concepts/outcomes/","text":"Prediction Outcomes Based on the predicted class and the label, Azimuth provides Outcomes , an assessment of the correctness of a given prediction. Outcomes are determined for individual utterances as a metric of success, and are also used across the application as an overall metric. These outcomes align with some business outcomes: not all errors (or correct predictions) are equal. Rejection Class Many ML problems have the notion of a rejection class, sometimes called the abstention class. This is the class that indicates the absence of a prediction . In some use cases, the model can predict it directly as one of its classes, and in other scenarios, it is only predicted when the confidence of a model is below a given threshold . In Azimuth, the rejection class needs to be defined in the config, as explained in the Project Configuration . From a business outcome, it is usually less costly to not predict anything than to predict the wrong class. This is why the notion of rejection is present in the outcomes, as explained below. Outcomes Azimuth defines 4 different outcomes: Correct & Predicted When the predicted class matches the label, and is not the rejection class. These are the predictions that add the most value . Correct & Rejected When the predicted class matches the label, but is the rejection class. These predictions are correct, but do not necessarily bring a lot of value , given that the data sample does not come from one of the more meaningful classes. Incorrect & Rejected When the predicted class does not match the label, but the rejection class has been predicted. These predicted are incorrect, but not as costly as when the wrong class is predicted. Incorrect & Predicted When the predicted class does not match the label, and is not the rejection class. These predicted are incorrect and are the most costly . The colors of the outcomes are used throughout the application to show how many utterances of each of the outcomes are present in different aggregations.","title":"Prediction Outcomes"},{"location":"key-concepts/outcomes/#prediction-outcomes","text":"Based on the predicted class and the label, Azimuth provides Outcomes , an assessment of the correctness of a given prediction. Outcomes are determined for individual utterances as a metric of success, and are also used across the application as an overall metric. These outcomes align with some business outcomes: not all errors (or correct predictions) are equal.","title":"Prediction Outcomes"},{"location":"key-concepts/outcomes/#rejection-class","text":"Many ML problems have the notion of a rejection class, sometimes called the abstention class. This is the class that indicates the absence of a prediction . In some use cases, the model can predict it directly as one of its classes, and in other scenarios, it is only predicted when the confidence of a model is below a given threshold . In Azimuth, the rejection class needs to be defined in the config, as explained in the Project Configuration . From a business outcome, it is usually less costly to not predict anything than to predict the wrong class. This is why the notion of rejection is present in the outcomes, as explained below.","title":"Rejection Class"},{"location":"key-concepts/outcomes/#outcomes","text":"Azimuth defines 4 different outcomes: Correct & Predicted When the predicted class matches the label, and is not the rejection class. These are the predictions that add the most value . Correct & Rejected When the predicted class matches the label, but is the rejection class. These predictions are correct, but do not necessarily bring a lot of value , given that the data sample does not come from one of the more meaningful classes. Incorrect & Rejected When the predicted class does not match the label, but the rejection class has been predicted. These predicted are incorrect, but not as costly as when the wrong class is predicted. Incorrect & Predicted When the predicted class does not match the label, and is not the rejection class. These predicted are incorrect and are the most costly . The colors of the outcomes are used throughout the application to show how many utterances of each of the outcomes are present in different aggregations.","title":"Outcomes"},{"location":"key-concepts/proposed-actions/","text":"Proposed Actions In the utterance table or the utterance details, annotations can be added to indicate whether an action should be taken for each data sample. The annotations can be exported with the dataset from the Utterances Table . Proposed Actions in the Utterances Table of the Exploration Space Five proposed actions are currently supported: Relabel : The label for this utterance should be changed. Augment with Similar : This utterance lacks nearby utterances in the same class, and new, similarly labeled utterances should be added. Define New Class : This data point may belong to a new class that doesn't exist in the currently defined classes. Based on the number of data points identified with this action, a user may choose to add a new class. Merge Classes : The label and the predicted class for this data point may be too similar. Based on the number of data points identified with this action, a user may choose to merge the two classes. Remove : This data point is irrelevant or otherwise problematic, and should be removed from the dataset. Investigate : The action is not yet clear, but the data point should be further investigated/discussed, within Azimuth or outside the tool.","title":"Proposed Actions"},{"location":"key-concepts/proposed-actions/#proposed-actions","text":"In the utterance table or the utterance details, annotations can be added to indicate whether an action should be taken for each data sample. The annotations can be exported with the dataset from the Utterances Table . Proposed Actions in the Utterances Table of the Exploration Space Five proposed actions are currently supported: Relabel : The label for this utterance should be changed. Augment with Similar : This utterance lacks nearby utterances in the same class, and new, similarly labeled utterances should be added. Define New Class : This data point may belong to a new class that doesn't exist in the currently defined classes. Based on the number of data points identified with this action, a user may choose to add a new class. Merge Classes : The label and the predicted class for this data point may be too similar. Based on the number of data points identified with this action, a user may choose to merge the two classes. Remove : This data point is irrelevant or otherwise problematic, and should be removed from the dataset. Investigate : The action is not yet clear, but the data point should be further investigated/discussed, within Azimuth or outside the tool.","title":"Proposed Actions"},{"location":"key-concepts/saliency/","text":"Saliency Maps What is it? A saliency map is a feature-based explainability ( XAI ) method that is available for gradient-based ML models. Its role is to estimate how much each variable contributes to the model prediction for a given data sample. In the case of NLP , variables are usually tokens or words. Literature In NLP , the current state-of-the-art models (Transformers) are black boxes . It\u2019s not trivial to understand why they make mistakes or why they are right. Saliency maps first became popular for computer vision problems, where the results would be a heatmap of the most contributing pixels. There has been recent growing interest in saliency maps as an XAI technique for NLP : Wallace et al. 1 in their AllenNLP toolkit implement vanilla gradient, integrated gradients, and smoothGrad for several NLP tasks and mostly pre- BERT models. Han et al. 2 use gradient-based saliency maps for sentiment analysis and NLI on BERT . Atanasova et al. 3 evaluate different saliency techniques on a variety of models including BERT -based models. Bastings and Filippova 4 argue for using saliency maps over attention-based explanations when determining the input tokens most relevant to a prediction. Their end-user is a model developer, rather than a user of the system. Other XAI techniques Apart from saliency maps, other feature-based XAI techniques exist, such as SHAP or LIME . Where is this used in Azimuth? In Azimuth, we display a saliency map over a specific utterance to show the importance of each token to the model prediction. When available, it is both displayed in the Utterance Details and in the Utterances Table . Saliency map of a specific utterance. Saliency example In this example, bill is the word that contributes the most to the prediction bill_balance . How is it computed? We use the technique Vanilla Gradient , shown to satisfy input invariance in Kindermans et al. 5 We simply backpropagate the gradient to the input layer of the network: in our case, the word-embedding layer. We then take the L2 norm to aggregate the gradients across all dimensions of the layer to determine the saliency value for each token. Saliency maps are only supported for certain models Saliency maps are only available for models that have gradients. Additionally, their input layer needs to be a token-embedding layer, so that the gradients can be computed per token. For example, a sentence embedder cannot back-propagate the gradients with sufficient granularity in the utterance. Configuration Assuming the model architecture allows for saliency maps, the name of the input layer needs to be defined in the config file, as detailed in Model Contract Configuration . Wallace, Eric, et al. \"Allennlp interpret: A framework for explaining predictions of nlp models.\" arXiv preprint arXiv:1909.09251 (2019). \u21a9 Han, Xiaochuang, Byron C. Wallace, and Yulia Tsvetkov. \"Explaining black box predictions and unveiling data artifacts through influence functions.\" arXiv preprint arXiv:2005.06676 (2020). \u21a9 Atanasova, Pepa, et al. \"A diagnostic study of explainability techniques for text classification.\" arXiv preprint arXiv:2009.13295 (2020). \u21a9 Bastings, Jasmijn, and Katja Filippova. \"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?.\" arXiv preprint arXiv:2010.05607 (2020). \u21a9 Kindermans, Pieter-Jan, et al. \"The (un) reliability of saliency methods.\" Explainable AI : Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280. \u21a9","title":"Saliency Maps"},{"location":"key-concepts/saliency/#saliency-maps","text":"","title":"Saliency Maps"},{"location":"key-concepts/saliency/#what-is-it","text":"A saliency map is a feature-based explainability ( XAI ) method that is available for gradient-based ML models. Its role is to estimate how much each variable contributes to the model prediction for a given data sample. In the case of NLP , variables are usually tokens or words.","title":"What is it?"},{"location":"key-concepts/saliency/#literature","text":"In NLP , the current state-of-the-art models (Transformers) are black boxes . It\u2019s not trivial to understand why they make mistakes or why they are right. Saliency maps first became popular for computer vision problems, where the results would be a heatmap of the most contributing pixels. There has been recent growing interest in saliency maps as an XAI technique for NLP : Wallace et al. 1 in their AllenNLP toolkit implement vanilla gradient, integrated gradients, and smoothGrad for several NLP tasks and mostly pre- BERT models. Han et al. 2 use gradient-based saliency maps for sentiment analysis and NLI on BERT . Atanasova et al. 3 evaluate different saliency techniques on a variety of models including BERT -based models. Bastings and Filippova 4 argue for using saliency maps over attention-based explanations when determining the input tokens most relevant to a prediction. Their end-user is a model developer, rather than a user of the system. Other XAI techniques Apart from saliency maps, other feature-based XAI techniques exist, such as SHAP or LIME .","title":"Literature"},{"location":"key-concepts/saliency/#where-is-this-used-in-azimuth","text":"In Azimuth, we display a saliency map over a specific utterance to show the importance of each token to the model prediction. When available, it is both displayed in the Utterance Details and in the Utterances Table . Saliency map of a specific utterance. Saliency example In this example, bill is the word that contributes the most to the prediction bill_balance .","title":"Where is this used in Azimuth?"},{"location":"key-concepts/saliency/#how-is-it-computed","text":"We use the technique Vanilla Gradient , shown to satisfy input invariance in Kindermans et al. 5 We simply backpropagate the gradient to the input layer of the network: in our case, the word-embedding layer. We then take the L2 norm to aggregate the gradients across all dimensions of the layer to determine the saliency value for each token. Saliency maps are only supported for certain models Saliency maps are only available for models that have gradients. Additionally, their input layer needs to be a token-embedding layer, so that the gradients can be computed per token. For example, a sentence embedder cannot back-propagate the gradients with sufficient granularity in the utterance.","title":"How is it computed?"},{"location":"key-concepts/saliency/#configuration","text":"Assuming the model architecture allows for saliency maps, the name of the input layer needs to be defined in the config file, as detailed in Model Contract Configuration . Wallace, Eric, et al. \"Allennlp interpret: A framework for explaining predictions of nlp models.\" arXiv preprint arXiv:1909.09251 (2019). \u21a9 Han, Xiaochuang, Byron C. Wallace, and Yulia Tsvetkov. \"Explaining black box predictions and unveiling data artifacts through influence functions.\" arXiv preprint arXiv:2005.06676 (2020). \u21a9 Atanasova, Pepa, et al. \"A diagnostic study of explainability techniques for text classification.\" arXiv preprint arXiv:2009.13295 (2020). \u21a9 Bastings, Jasmijn, and Katja Filippova. \"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?.\" arXiv preprint arXiv:2010.05607 (2020). \u21a9 Kindermans, Pieter-Jan, et al. \"The (un) reliability of saliency methods.\" Explainable AI : Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280. \u21a9","title":"Configuration"},{"location":"key-concepts/similarity/","text":"Similarity Analysis What is it? Similarity analysis is based on the relative locations of utterances in embedding space. This analysis can be quite powerful given that no trained ML model is needed ; only a dataset needs to be supplied. Within Azimuth, different similarity analyses are provided to determine how similar utterances are within a class, across classes, and so on. This can help indicate whether classes are well-defined, or whether changes should be made to improve the dataset, such as by redefining classes, relabeling or omitting data, or augmenting the dataset. Where is this used in Azimuth? In Azimuth, the similarity analysis is used to derive Smart Tags , and also to show the most similar utterances in both dataset splits on the Utterances Details . Similar utterances in the Utterance Details. How is it Computed? Similarity Computation To get utterance embeddings, Azimuth uses a sentence encoder ( all-MiniLM-L12-v2 from sentence-transformers ) based on a BERT architecture ( Reimers and Gurevych, 2019 1 ). It then computes the cosine similarity (via a dot product on normalized embeddings ) between each utterance in the dataset and all other utterances in both dataset splits (training and evaluation). On the Utterances Details , the most similar examples are presented in descending order (i.e., most similar first), along with the cosine similarity to the selected utterance. A cosine similarity of 1 indicates that the utterance is identical, while 0 indicates that it is unrelated. Smart Tag Family: Dissimilar No Close Tags Some utterances may have no close neighbors in a dataset split - that is, their most similar utterances have low cosine similarities. When the cosine similarity of an utterance's closest neighbor is below a threshold (default = 0.5), the utterance gets tagged with no_close_train and/or no_close_eval , according to the dataset split being assessed (training or evaluation). Note that this tag is class label-agnostic. Conflicting Neighbors Tags It can be useful to assess whether the most similar data samples to an utterance (its neighbors) come from the same or different classes. When most of its neighboring utterances are from a different class , it might indicate a mislabeling issue, overlapping classes, data drift, or simply a difficult utterance to predict. Two Smart Tags highlight these sorts of utterances, based on the label heterogeneity of the neighborhood in each dataset split (training or evaluation). If 90% or more of an utterance's most similar data samples (neighbors) in a dataset split belong to a different class, it will be tagged as conflicting_neighbors_train and/or conflicting_neighbors_eval , based on which dataset split is being examined. (E.g., an utterance in the test set will be compared to its neighbors in both the training and evaluation dataset splits.) Configuration Similarity Analysis Configuration details how to change the encoder for the embeddings on which similarity is computed, as well as the two thresholds used to determine the smart tags. Reimers, Nils, and Iryna Gurevych. \"Sentence-bert: Sentence embeddings using siamese bert-networks.\" arXiv preprint arXiv:1908.10084 (2019). \u21a9","title":"Similarity Analysis"},{"location":"key-concepts/similarity/#similarity-analysis","text":"","title":"Similarity Analysis"},{"location":"key-concepts/similarity/#what-is-it","text":"Similarity analysis is based on the relative locations of utterances in embedding space. This analysis can be quite powerful given that no trained ML model is needed ; only a dataset needs to be supplied. Within Azimuth, different similarity analyses are provided to determine how similar utterances are within a class, across classes, and so on. This can help indicate whether classes are well-defined, or whether changes should be made to improve the dataset, such as by redefining classes, relabeling or omitting data, or augmenting the dataset.","title":"What is it?"},{"location":"key-concepts/similarity/#where-is-this-used-in-azimuth","text":"In Azimuth, the similarity analysis is used to derive Smart Tags , and also to show the most similar utterances in both dataset splits on the Utterances Details . Similar utterances in the Utterance Details.","title":"Where is this used in Azimuth?"},{"location":"key-concepts/similarity/#how-is-it-computed","text":"","title":"How is it Computed?"},{"location":"key-concepts/similarity/#similarity-computation","text":"To get utterance embeddings, Azimuth uses a sentence encoder ( all-MiniLM-L12-v2 from sentence-transformers ) based on a BERT architecture ( Reimers and Gurevych, 2019 1 ). It then computes the cosine similarity (via a dot product on normalized embeddings ) between each utterance in the dataset and all other utterances in both dataset splits (training and evaluation). On the Utterances Details , the most similar examples are presented in descending order (i.e., most similar first), along with the cosine similarity to the selected utterance. A cosine similarity of 1 indicates that the utterance is identical, while 0 indicates that it is unrelated.","title":"Similarity Computation"},{"location":"key-concepts/similarity/#smart-tag-family-dissimilar","text":"","title":"Smart Tag Family: Dissimilar"},{"location":"key-concepts/similarity/#no-close-tags","text":"Some utterances may have no close neighbors in a dataset split - that is, their most similar utterances have low cosine similarities. When the cosine similarity of an utterance's closest neighbor is below a threshold (default = 0.5), the utterance gets tagged with no_close_train and/or no_close_eval , according to the dataset split being assessed (training or evaluation). Note that this tag is class label-agnostic.","title":"No Close Tags"},{"location":"key-concepts/similarity/#conflicting-neighbors-tags","text":"It can be useful to assess whether the most similar data samples to an utterance (its neighbors) come from the same or different classes. When most of its neighboring utterances are from a different class , it might indicate a mislabeling issue, overlapping classes, data drift, or simply a difficult utterance to predict. Two Smart Tags highlight these sorts of utterances, based on the label heterogeneity of the neighborhood in each dataset split (training or evaluation). If 90% or more of an utterance's most similar data samples (neighbors) in a dataset split belong to a different class, it will be tagged as conflicting_neighbors_train and/or conflicting_neighbors_eval , based on which dataset split is being examined. (E.g., an utterance in the test set will be compared to its neighbors in both the training and evaluation dataset splits.)","title":"Conflicting Neighbors Tags"},{"location":"key-concepts/similarity/#configuration","text":"Similarity Analysis Configuration details how to change the encoder for the embeddings on which similarity is computed, as well as the two thresholds used to determine the smart tags. Reimers, Nils, and Iryna Gurevych. \"Sentence-bert: Sentence embeddings using siamese bert-networks.\" arXiv preprint arXiv:1908.10084 (2019). \u21a9","title":"Configuration"},{"location":"key-concepts/smart-tags/","text":"Smart Tags When Azimuth is launched, smart tags are computed automatically (or loaded from the cache) on all utterances, on both training and evaluation sets. Conceptually, smart tags are meta-data on the utterance and/or its prediction . They help to narrow down data samples to identify those that may require further action and investigation . Different families of smart tags exist, based on the different types of analyses that Azimuth provides. The current list of supported smart tag, and their families, is detailed below. Extreme Length Syntax Analysis gives more details on how the syntactic information is computed. multiple_sentences : The number of sentences is above 1. All other syntactic smart tags will be disabled when this is the case. long_sentence : The number of tokens is greater than or equal to the defined threshold (default is 16). short_sentence : The number of tokens is less than or equal to the defined threshold (default is 3). Partial Syntax Syntax Analysis gives more details on how the syntactic information is computed. missing_subj : The sentence is missing a subject. missing_verb : The sentence is missing a verb. missing_obj : The sentence is missing an object. Dissimilar Similarity Analysis provides more information on how similarity is computed. conflicting_neighbors_train : The utterance has very few (or no) neighbors from the same class in the training set. conflicting_neighbors_eval : The utterance has very few (or no) neighbors from the same class in the evaluation set. no_close_train : The closest utterance in the training set has a cosine similarity below a threshold (default = 0.5). no_close_eval : The closest utterance in the evaluation set has a cosine similarity below a threshold (default = 0.5). Almost Correct These smart tags do not come from a particular analysis. They are computed based on the predictions and the labels. correct_top_3 : The top 1 prediction is not the right one, but the right one is in the top 3. correct_low_conf : The top 1 prediction was the right one, but its confidence is below the threshold, and thus the rejection class was predicted. Behavioral Testing Behavioral Testing lists all the tests that are executed. failed_fuzzy_matching : At least one fuzzy matching test failed. failed_punctuation : At least one punctuation test failed. Pipeline Comparison Smart tags that are computed based on the difference between pipelines predictions. incorrect_for_all_pipelines : When all pipelines give the wrong prediction. pipeline_disagreement : When at least one of the pipelines disagrees with the others. Uncertain Uncertainty Quantification provides more details on how the uncertainty is estimated. high_epistemic_uncertainty : If an uncertainty config was defined, this tag will highlight predictions with high epistemic uncertainty.","title":"Smart Tags"},{"location":"key-concepts/smart-tags/#smart-tags","text":"When Azimuth is launched, smart tags are computed automatically (or loaded from the cache) on all utterances, on both training and evaluation sets. Conceptually, smart tags are meta-data on the utterance and/or its prediction . They help to narrow down data samples to identify those that may require further action and investigation . Different families of smart tags exist, based on the different types of analyses that Azimuth provides. The current list of supported smart tag, and their families, is detailed below.","title":"Smart Tags"},{"location":"key-concepts/smart-tags/#extreme-length","text":"Syntax Analysis gives more details on how the syntactic information is computed. multiple_sentences : The number of sentences is above 1. All other syntactic smart tags will be disabled when this is the case. long_sentence : The number of tokens is greater than or equal to the defined threshold (default is 16). short_sentence : The number of tokens is less than or equal to the defined threshold (default is 3).","title":"Extreme Length "},{"location":"key-concepts/smart-tags/#partial-syntax","text":"Syntax Analysis gives more details on how the syntactic information is computed. missing_subj : The sentence is missing a subject. missing_verb : The sentence is missing a verb. missing_obj : The sentence is missing an object.","title":"Partial Syntax "},{"location":"key-concepts/smart-tags/#dissimilar","text":"Similarity Analysis provides more information on how similarity is computed. conflicting_neighbors_train : The utterance has very few (or no) neighbors from the same class in the training set. conflicting_neighbors_eval : The utterance has very few (or no) neighbors from the same class in the evaluation set. no_close_train : The closest utterance in the training set has a cosine similarity below a threshold (default = 0.5). no_close_eval : The closest utterance in the evaluation set has a cosine similarity below a threshold (default = 0.5).","title":"Dissimilar "},{"location":"key-concepts/smart-tags/#almost-correct","text":"These smart tags do not come from a particular analysis. They are computed based on the predictions and the labels. correct_top_3 : The top 1 prediction is not the right one, but the right one is in the top 3. correct_low_conf : The top 1 prediction was the right one, but its confidence is below the threshold, and thus the rejection class was predicted.","title":"Almost Correct "},{"location":"key-concepts/smart-tags/#behavioral-testing","text":"Behavioral Testing lists all the tests that are executed. failed_fuzzy_matching : At least one fuzzy matching test failed. failed_punctuation : At least one punctuation test failed.","title":"Behavioral Testing "},{"location":"key-concepts/smart-tags/#pipeline-comparison","text":"Smart tags that are computed based on the difference between pipelines predictions. incorrect_for_all_pipelines : When all pipelines give the wrong prediction. pipeline_disagreement : When at least one of the pipelines disagrees with the others.","title":"Pipeline Comparison "},{"location":"key-concepts/smart-tags/#uncertain","text":"Uncertainty Quantification provides more details on how the uncertainty is estimated. high_epistemic_uncertainty : If an uncertainty config was defined, this tag will highlight predictions with high epistemic uncertainty.","title":"Uncertain "},{"location":"key-concepts/syntax-analysis/","text":"Syntax Analysis What is it? The syntax of an utterance usually refers to its structure, such as how ideas and words are ordered in a sentence. Azimuth provides smart tags based on the length and syntactic structure of utterances. Where is this used in Azimuth? Based on the syntax of each utterance, Azimuth computes syntactic Smart Tags (Extreme Length and Partial Syntax families). Additionally, the length mismatch plot in the Dataset Class Distribution Analysis compares the length of the utterances in the training set and the evaluation set. How is it computed? POS Tags Part-of-speech ( POS ) tagging is a common technique to tag each word in a given text as belonging to a category according to its grammatical properties. Examples could be 'verb', or 'direct object'. Azimuth uses spaCy , an open-source library, to perform POS tagging on each token of an utterance. It is currently set up for English only. import spacy from spacy.lang.en import English # Sentencizer spacy_pipeline = English () spacy_pipeline . add_pipe ( \"sentencizer\" ) # (5) # Part of Speech subj_tags = [ \"nsubj\" , \"nsubjpass\" ] # (1) obj_tags = [ \"dobj\" , \"pobj\" , \"dobj\" ] # (2) verb_tags = [ \"VERB\" , \"AUX\" ] # (3) spacy_pos = spacy . load ( \"en_core_web_sm\" ) # (4) Tags to detect a subject in a sentence. Tags to detect an object in a sentence. Tags to detect a verb in a sentence. Parser to determine the POS tags in an utterance. Used to compute the number of sentences in an utterance. Based on this, the following smart tags are computed: multiple_sentences , missing_subj , missing_verb and missing_obj . Token Count To compute the number of tokens per utterance, the following tokenizer is used: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) Based on the token count, the long_sentence and short_sentence smart tags are computed. Configuration Syntax Analysis Config explains how to edit the thresholds to determine what is considered a short or long sentence.","title":"Syntax Analysis"},{"location":"key-concepts/syntax-analysis/#syntax-analysis","text":"","title":"Syntax Analysis"},{"location":"key-concepts/syntax-analysis/#what-is-it","text":"The syntax of an utterance usually refers to its structure, such as how ideas and words are ordered in a sentence. Azimuth provides smart tags based on the length and syntactic structure of utterances.","title":"What is it?"},{"location":"key-concepts/syntax-analysis/#where-is-this-used-in-azimuth","text":"Based on the syntax of each utterance, Azimuth computes syntactic Smart Tags (Extreme Length and Partial Syntax families). Additionally, the length mismatch plot in the Dataset Class Distribution Analysis compares the length of the utterances in the training set and the evaluation set.","title":"Where is this used in Azimuth?"},{"location":"key-concepts/syntax-analysis/#how-is-it-computed","text":"","title":"How is it computed?"},{"location":"key-concepts/syntax-analysis/#pos-tags","text":"Part-of-speech ( POS ) tagging is a common technique to tag each word in a given text as belonging to a category according to its grammatical properties. Examples could be 'verb', or 'direct object'. Azimuth uses spaCy , an open-source library, to perform POS tagging on each token of an utterance. It is currently set up for English only. import spacy from spacy.lang.en import English # Sentencizer spacy_pipeline = English () spacy_pipeline . add_pipe ( \"sentencizer\" ) # (5) # Part of Speech subj_tags = [ \"nsubj\" , \"nsubjpass\" ] # (1) obj_tags = [ \"dobj\" , \"pobj\" , \"dobj\" ] # (2) verb_tags = [ \"VERB\" , \"AUX\" ] # (3) spacy_pos = spacy . load ( \"en_core_web_sm\" ) # (4) Tags to detect a subject in a sentence. Tags to detect an object in a sentence. Tags to detect a verb in a sentence. Parser to determine the POS tags in an utterance. Used to compute the number of sentences in an utterance. Based on this, the following smart tags are computed: multiple_sentences , missing_subj , missing_verb and missing_obj .","title":"POS Tags"},{"location":"key-concepts/syntax-analysis/#token-count","text":"To compute the number of tokens per utterance, the following tokenizer is used: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) Based on the token count, the long_sentence and short_sentence smart tags are computed.","title":"Token Count"},{"location":"key-concepts/syntax-analysis/#configuration","text":"Syntax Analysis Config explains how to edit the thresholds to determine what is considered a short or long sentence.","title":"Configuration"},{"location":"key-concepts/uncertainty/","text":"Uncertainty Estimation What is it? Uncertainty estimation is a field of machine learning that aims to determine when a model is uncertain of its prediction. By nature, deep learning models are overconfident and their predictions are not trustworthy 1 . It is possible for a model to output a prediction with high confidence and still be uncertain. Uncertainty is split into two distinct sources: data uncertainty and model uncertainty. Data uncertainty, also called aleatoric uncertainty, comes from mislabelled examples, noisy data, overlapping classes, etc. Model uncertainty, also called epistemic uncertainty, relates to uncertainty around the model's parameters. As such, epistemic uncertainty is going to be high when a data sample is close to the model's decision boundary. It is important to know that while epistemic uncertainty can be reduced with more data, the intrinsic noise that produces aleatoric uncertainty can't be reduced with more data. More information on uncertainty estimation can be found in the BAAL documentation . Where is it used in Azimuth? Azimuth has some simple uncertainty estimation capabilities. If an uncertainty configuration is provided in the config file, Azimuth assigns a Smart Tag for utterances with high epistemic uncertainty. These utterances tend to be outliers or mislabeled examples. How is it computed? Epistemic Uncertainty Smart Tag On Pytorch models, Azimuth leverages MC -Dropout ( Gal et al. 2015 ) 2 . MC -Dropout draws multiple sets of weights from the model's posterior distribution, effectively creating a Bayesian ensemble. This type of ensemble is weaker than a regular ensemble, as there is a high correlation between each member of the ensemble 4 . It uses this weak ensemble to estimate the epistemic uncertainty using BALD ( Houlsby et al. 2013 ). The maximum BALD value is log(C) where C is the number of classes. Predictions with high epistemic uncertainty are data points for which slight changes in the model parameters can cause significant changes in the predictions. On models that do not have any Dropout layers, this has no effect. Configuration Model Contract Configuration offers some attributes to enable uncertainty quantification, by defining the number of iterations for MC -Dropout, and the threshold value for the smart tag. Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction. Scalia et al. J. Chem. Inf. Model, 2020 \u21a9 Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Gal and Ghahramani, ICML, 2020 \u21a9 Bayesian active learning for classification and preference learning. Houlsby et al. arXiv preprint arXiv:1112.5745, 2011 \u21a9 The power of ensembles for active learning in image classification. Beluch et al. CVPR, 2018 \u21a9","title":"Uncertainty Estimation"},{"location":"key-concepts/uncertainty/#uncertainty-estimation","text":"","title":"Uncertainty Estimation"},{"location":"key-concepts/uncertainty/#what-is-it","text":"Uncertainty estimation is a field of machine learning that aims to determine when a model is uncertain of its prediction. By nature, deep learning models are overconfident and their predictions are not trustworthy 1 . It is possible for a model to output a prediction with high confidence and still be uncertain. Uncertainty is split into two distinct sources: data uncertainty and model uncertainty. Data uncertainty, also called aleatoric uncertainty, comes from mislabelled examples, noisy data, overlapping classes, etc. Model uncertainty, also called epistemic uncertainty, relates to uncertainty around the model's parameters. As such, epistemic uncertainty is going to be high when a data sample is close to the model's decision boundary. It is important to know that while epistemic uncertainty can be reduced with more data, the intrinsic noise that produces aleatoric uncertainty can't be reduced with more data. More information on uncertainty estimation can be found in the BAAL documentation .","title":"What is it?"},{"location":"key-concepts/uncertainty/#where-is-it-used-in-azimuth","text":"Azimuth has some simple uncertainty estimation capabilities. If an uncertainty configuration is provided in the config file, Azimuth assigns a Smart Tag for utterances with high epistemic uncertainty. These utterances tend to be outliers or mislabeled examples.","title":"Where is it used in Azimuth?"},{"location":"key-concepts/uncertainty/#how-is-it-computed","text":"","title":"How is it computed?"},{"location":"key-concepts/uncertainty/#epistemic-uncertainty-smart-tag","text":"On Pytorch models, Azimuth leverages MC -Dropout ( Gal et al. 2015 ) 2 . MC -Dropout draws multiple sets of weights from the model's posterior distribution, effectively creating a Bayesian ensemble. This type of ensemble is weaker than a regular ensemble, as there is a high correlation between each member of the ensemble 4 . It uses this weak ensemble to estimate the epistemic uncertainty using BALD ( Houlsby et al. 2013 ). The maximum BALD value is log(C) where C is the number of classes. Predictions with high epistemic uncertainty are data points for which slight changes in the model parameters can cause significant changes in the predictions. On models that do not have any Dropout layers, this has no effect.","title":"Epistemic Uncertainty Smart Tag"},{"location":"key-concepts/uncertainty/#configuration","text":"Model Contract Configuration offers some attributes to enable uncertainty quantification, by defining the number of iterations for MC -Dropout, and the threshold value for the smart tag. Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction. Scalia et al. J. Chem. Inf. Model, 2020 \u21a9 Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Gal and Ghahramani, ICML, 2020 \u21a9 Bayesian active learning for classification and preference learning. Houlsby et al. arXiv preprint arXiv:1112.5745, 2011 \u21a9 The power of ensembles for active learning in image classification. Beluch et al. CVPR, 2018 \u21a9","title":"Configuration"},{"location":"reference/","text":"Reference Our reference is split in two sections: Configuration provides detailed information about the different fields in the Azimuth configuration that allow to define the dataset, the pipelines and different customization. Custom Objects describes how to create custom objects that allow to add your own datasets, pipelines and metrics.","title":"Reference"},{"location":"reference/#reference","text":"Our reference is split in two sections: Configuration provides detailed information about the different fields in the Azimuth configuration that allow to define the dataset, the pipelines and different customization. Custom Objects describes how to create custom objects that allow to add your own datasets, pipelines and metrics.","title":"Reference"},{"location":"reference/configuration/","text":"Configuration The Azimuth configuration allows defining different fields, some mandatory, that will customize Azimuth. We grouped the fields in config scopes based on what they control. All classes inherit from one another in a chain, the last one being AzimuthConfig , which contains all fields. To help with detecting the mandatory fields in the config, the following legend is shown throughout the reference. : Mandatory fields in the config. : Only mandatory if Azimuth is used to analyze models , and not just a dataset. : Usually mandatory fields, but default values exist that may work for some use cases. : The default values should work for most use cases. Config Scopes Project Config Mandatory fields to define the name of the project and the dataset information. Model Contract Config Defines how Azimuth interacts with the models/pipelines. Common Fields Config Fields that are common to many applications (batch size for example). Customize Azimuth Analyses : In these sections, different analyses in Azimuth can be configured. Behavioral Testing Config Defines how the behavioral tests are generated. Similarity Analysis Config Modifies the similarity analysis. Dataset Class Distribution Analysis Config Configure Dataset Class Distribution Analysis.","title":"Configuration"},{"location":"reference/configuration/#configuration","text":"The Azimuth configuration allows defining different fields, some mandatory, that will customize Azimuth. We grouped the fields in config scopes based on what they control. All classes inherit from one another in a chain, the last one being AzimuthConfig , which contains all fields. To help with detecting the mandatory fields in the config, the following legend is shown throughout the reference. : Mandatory fields in the config. : Only mandatory if Azimuth is used to analyze models , and not just a dataset. : Usually mandatory fields, but default values exist that may work for some use cases. : The default values should work for most use cases.","title":"Configuration"},{"location":"reference/configuration/#config-scopes","text":"Project Config Mandatory fields to define the name of the project and the dataset information. Model Contract Config Defines how Azimuth interacts with the models/pipelines. Common Fields Config Fields that are common to many applications (batch size for example). Customize Azimuth Analyses : In these sections, different analyses in Azimuth can be configured. Behavioral Testing Config Defines how the behavioral tests are generated. Similarity Analysis Config Modifies the similarity analysis. Dataset Class Distribution Analysis Config Configure Dataset Class Distribution Analysis.","title":"Config Scopes"},{"location":"reference/configuration/common/","text":"Common Fields Config These fields are generic and can be adapted based on the user's machine. Class Definition Config Example class CommonFieldsConfig ( ProjectConfig , extra = Extra . ignore ): \"\"\"\"\"\" artifact_path : str = \"/cache\" batch_size : int = 32 use_cuda : Union [ Literal [ \"auto\" ], bool ] = \"auto\" large_dask_cluster : bool = False read_only_config : bool = False Example to append to the config to override the default batch_size . { \"batch_size\" : 64 , } Artifact Path Default value : /cache Where to store the caching artifacts ( HDF5 files and HF datasets). The value needs to be available inside Docker (see docker-compose.yml ). /cache is available by default on the docker image. Not using Docker? If Azimuth is run without Docker, the cache needs to be a path with write access ( /cache will not work). Batch Size Default value : 32 Batch size to use during inference. A higher batch size will make computation faster, depending on the memory available on your machine. Use Cuda Default value : auto If cuda is available on your machine, set to true , otherwise false . Can also be set to \"auto\" and let the user-code take care of it. Large Dask Cluster Default value : False The memory of the dask cluster is usually 6GB. If your models are big or if you encounter garbage collection errors, you can set the memory to 12GB by setting large_dask_cluster to True . Read-Only Config Default value : False This field allows to block the changes to the config when set to True . This can be useful in certain context, such as when hosting a demo.","title":"Common Fields Config"},{"location":"reference/configuration/common/#common-fields-config","text":"These fields are generic and can be adapted based on the user's machine. Class Definition Config Example class CommonFieldsConfig ( ProjectConfig , extra = Extra . ignore ): \"\"\"\"\"\" artifact_path : str = \"/cache\" batch_size : int = 32 use_cuda : Union [ Literal [ \"auto\" ], bool ] = \"auto\" large_dask_cluster : bool = False read_only_config : bool = False Example to append to the config to override the default batch_size . { \"batch_size\" : 64 , }","title":"Common Fields Config"},{"location":"reference/configuration/common/#artifact-path","text":"Default value : /cache Where to store the caching artifacts ( HDF5 files and HF datasets). The value needs to be available inside Docker (see docker-compose.yml ). /cache is available by default on the docker image. Not using Docker? If Azimuth is run without Docker, the cache needs to be a path with write access ( /cache will not work).","title":"Artifact Path"},{"location":"reference/configuration/common/#batch-size","text":"Default value : 32 Batch size to use during inference. A higher batch size will make computation faster, depending on the memory available on your machine.","title":"Batch Size"},{"location":"reference/configuration/common/#use-cuda","text":"Default value : auto If cuda is available on your machine, set to true , otherwise false . Can also be set to \"auto\" and let the user-code take care of it.","title":"Use Cuda"},{"location":"reference/configuration/common/#large-dask-cluster","text":"Default value : False The memory of the dask cluster is usually 6GB. If your models are big or if you encounter garbage collection errors, you can set the memory to 12GB by setting large_dask_cluster to True .","title":"Large Dask Cluster"},{"location":"reference/configuration/common/#read-only-config","text":"Default value : False This field allows to block the changes to the config when set to True . This can be useful in certain context, such as when hosting a demo.","title":"Read-Only Config"},{"location":"reference/configuration/model_contract/","text":"Model Contract Config Fields from this scope defines how Azimuth interacts with the ML pipelines and the metrics. Class Definition Config Example from typing import Dict , List , Optional from azimuth.config import MetricDefinition , PipelineDefinition , UncertaintyOptions from azimuth.types import SupportedModelContract class ModelContractConfig : model_contract : SupportedModelContract = SupportedModelContract . hf_text_classification # (1) pipelines : Optional [ List [ PipelineDefinition ]] = None # (2) uncertainty : UncertaintyOptions = UncertaintyOptions () # (3) saliency_layer : Optional [ str ] = None # (4) metrics : Dict [ str , MetricDefinition ] = { # (5) \"Precision\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"precision\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), \"Recall\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"recall\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), \"F1\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"f1\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), } model_contract needs to be chosen based on the model type. List of pipelines. Can also be set to null to launch Azimuth with a dataset only. Enable uncertainty quantification. Layer name where to calculate the gradients, normally the word embeddings layer. Only available for Pytorch models. HuggingFace Metrics. { \"model_contract\" : \"hf_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } } } ] } Model Contract Mandatory field with an ML pipeline. Default value : hf_text_classification The model contract will be determined based on the model type. More details on what model contract to select based on the model is available in Define a Model . hf_text_classification supports transformers.Pipeline from HF . Examples are provided in the repo under config/examples . custom_text_classification supports any Callable and is more generic. Some features from Azimuth will be unavailable, such as saliency maps. file_based_text_classification supports reading the predictions from a file. A lot of features from Azimuth will be unavailable, such as behavioral testing and saliency maps. Pipelines Mandatory field with an ML pipeline. Default value : None In Azimuth, we define an ML pipeline as the combination of a model and postprocessors. This field accepts a list, since multiple pipelines can be loaded in Azimuth. If set to null , Azimuth will be launched without any pipeline. Pipeline Definition Config Example No Pipelines from typing import List , Optional , Union from pydantic import BaseSettings , Field from azimuth.config import CustomObject , TemperatureScaling , ThresholdConfig class PipelineDefinition ( BaseSettings ): name : str # (1) model : CustomObject # (2) postprocessors : Optional [ List [ # (3) Union [ TemperatureScaling , ThresholdConfig , CustomObject ]]] = Field ( [ TemperatureScaling ( temperature = 1.0 ), ThresholdConfig ( threshold = 0.5 ), ] ) Add a name to the pipeline to easily recognize it from the webapp. Ex: distilbert-base-uncased-th-0.9 Azimuth offers a helper function for HF pipelines. See the config example. The default postprocessors in Azimuth is a temperature of 1 and a threshold of 0.5. They can be changed (Ex: postprocessors: [{\"temperature\": 3}] ), disabled ( postprocessors: null ), or replaced with new ones defined with custom objects. If using a transformers.Pipeline from HF , the configuration below should work. { \"pipelines\" : [ { \"name\" : \"distilbert-base-uncased-th-0.9\" \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } \"postprocessors\" : [ { \"threshold\" : 0.9 } ] } ] } This will launch Azimuth without any pipelines. Dataset Analysis is still available. { \"pipelines\" : null } Both the model and the postprocessors are defined with Custom Objects . model allows to define a model. The model can be a HuggingFace pipeline, or any callable. The model can even include its own post-processing. Defining a Model details how to do that with custom objects. postprocessors defines the postprocessors. Azimuth offers some default values for temperature scaling and thresholding. Users can also provide their own postprocessor functions, or disabled them ( postprocessors: null ). Defining Postprocessors details the different use cases. Beginner users should start with simple use cases Beginner users should aim to use Azimuth's default supported postprocessors : temperature scaling and thresholding. We provide shortcuts to override the default values. Ex: {\"postprocessors\": [{\"temperature\": 3, \"threshold\": 0.9}]} . You can also use environment variable TEMP and TH . Ex: TH=0.6 . Uncertainty Default value : UncertaintyOptions() Azimuth has some simple uncertainty estimation capabilities. By default, they are disabled given that it can be computationally expensive. On any model, we can provide the entropy of the predictions which is an approximation of the predictive uncertainty. In addition, we can tag high epistemic items above a threshold. More information on how we compute uncertainty is available in Uncertainty Estimation . Class Definition Config Example from pydantic import BaseModel class UncertaintyOptions ( BaseModel ): iterations : int = 1 # (1) high_epistemic_threshold : float = 0.1 # (2) Number of MC sampling to do. The default is 1, which disables BMA . Threshold to determine high epistemic items. { \"uncertainty\" : { \"iterations\" : 20 } } Saliency Layer Default value : None If using a Pytorch model, Saliency Maps can be available. Specify the name of the embedding layer on which to compute them. Example: distilbert.embeddings.word_embeddings . Metrics Default value : Precision and Recall. See in the config example below. By default, Azimuth will compute Precision, Recall and F1 on the dataset. metrics leverages custom objects, with an additional field which allow defining kwargs to be sent to the metric compute() function. You can add metrics available from the HuggingFace Metric Hub , or create your own, as detailed in Defining Metrics . Metric Definition Config Example from typing import Dict from pydantic import Field from azimuth.config import CustomObject class MetricDefinition ( CustomObject ): additional_kwargs : Dict = Field ( default_factory = dict , title = \"Additional kwargs\" , description = \"Keyword arguments supplied to `compute`\" , ) These are the default values. This example shows how this could be adapted to other metrics. { \"metrics\" : { \"Precision\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"precision\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"Recall\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"recall\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"F1\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"f1\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } } } }","title":"Model Contract Config"},{"location":"reference/configuration/model_contract/#model-contract-config","text":"Fields from this scope defines how Azimuth interacts with the ML pipelines and the metrics. Class Definition Config Example from typing import Dict , List , Optional from azimuth.config import MetricDefinition , PipelineDefinition , UncertaintyOptions from azimuth.types import SupportedModelContract class ModelContractConfig : model_contract : SupportedModelContract = SupportedModelContract . hf_text_classification # (1) pipelines : Optional [ List [ PipelineDefinition ]] = None # (2) uncertainty : UncertaintyOptions = UncertaintyOptions () # (3) saliency_layer : Optional [ str ] = None # (4) metrics : Dict [ str , MetricDefinition ] = { # (5) \"Precision\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"precision\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), \"Recall\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"recall\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), \"F1\" : MetricDefinition ( class_name = \"datasets.load_metric\" , kwargs = { \"path\" : \"f1\" }, additional_kwargs = { \"average\" : \"weighted\" }, ), } model_contract needs to be chosen based on the model type. List of pipelines. Can also be set to null to launch Azimuth with a dataset only. Enable uncertainty quantification. Layer name where to calculate the gradients, normally the word embeddings layer. Only available for Pytorch models. HuggingFace Metrics. { \"model_contract\" : \"hf_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } } } ] }","title":"Model Contract Config"},{"location":"reference/configuration/model_contract/#model-contract","text":"Mandatory field with an ML pipeline. Default value : hf_text_classification The model contract will be determined based on the model type. More details on what model contract to select based on the model is available in Define a Model . hf_text_classification supports transformers.Pipeline from HF . Examples are provided in the repo under config/examples . custom_text_classification supports any Callable and is more generic. Some features from Azimuth will be unavailable, such as saliency maps. file_based_text_classification supports reading the predictions from a file. A lot of features from Azimuth will be unavailable, such as behavioral testing and saliency maps.","title":"Model Contract"},{"location":"reference/configuration/model_contract/#pipelines","text":"Mandatory field with an ML pipeline. Default value : None In Azimuth, we define an ML pipeline as the combination of a model and postprocessors. This field accepts a list, since multiple pipelines can be loaded in Azimuth. If set to null , Azimuth will be launched without any pipeline. Pipeline Definition Config Example No Pipelines from typing import List , Optional , Union from pydantic import BaseSettings , Field from azimuth.config import CustomObject , TemperatureScaling , ThresholdConfig class PipelineDefinition ( BaseSettings ): name : str # (1) model : CustomObject # (2) postprocessors : Optional [ List [ # (3) Union [ TemperatureScaling , ThresholdConfig , CustomObject ]]] = Field ( [ TemperatureScaling ( temperature = 1.0 ), ThresholdConfig ( threshold = 0.5 ), ] ) Add a name to the pipeline to easily recognize it from the webapp. Ex: distilbert-base-uncased-th-0.9 Azimuth offers a helper function for HF pipelines. See the config example. The default postprocessors in Azimuth is a temperature of 1 and a threshold of 0.5. They can be changed (Ex: postprocessors: [{\"temperature\": 3}] ), disabled ( postprocessors: null ), or replaced with new ones defined with custom objects. If using a transformers.Pipeline from HF , the configuration below should work. { \"pipelines\" : [ { \"name\" : \"distilbert-base-uncased-th-0.9\" \"model\" : { \"class_name\" : \"loading_resources.load_hf_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } \"postprocessors\" : [ { \"threshold\" : 0.9 } ] } ] } This will launch Azimuth without any pipelines. Dataset Analysis is still available. { \"pipelines\" : null } Both the model and the postprocessors are defined with Custom Objects . model allows to define a model. The model can be a HuggingFace pipeline, or any callable. The model can even include its own post-processing. Defining a Model details how to do that with custom objects. postprocessors defines the postprocessors. Azimuth offers some default values for temperature scaling and thresholding. Users can also provide their own postprocessor functions, or disabled them ( postprocessors: null ). Defining Postprocessors details the different use cases. Beginner users should start with simple use cases Beginner users should aim to use Azimuth's default supported postprocessors : temperature scaling and thresholding. We provide shortcuts to override the default values. Ex: {\"postprocessors\": [{\"temperature\": 3, \"threshold\": 0.9}]} . You can also use environment variable TEMP and TH . Ex: TH=0.6 .","title":"Pipelines"},{"location":"reference/configuration/model_contract/#uncertainty","text":"Default value : UncertaintyOptions() Azimuth has some simple uncertainty estimation capabilities. By default, they are disabled given that it can be computationally expensive. On any model, we can provide the entropy of the predictions which is an approximation of the predictive uncertainty. In addition, we can tag high epistemic items above a threshold. More information on how we compute uncertainty is available in Uncertainty Estimation . Class Definition Config Example from pydantic import BaseModel class UncertaintyOptions ( BaseModel ): iterations : int = 1 # (1) high_epistemic_threshold : float = 0.1 # (2) Number of MC sampling to do. The default is 1, which disables BMA . Threshold to determine high epistemic items. { \"uncertainty\" : { \"iterations\" : 20 } }","title":"Uncertainty"},{"location":"reference/configuration/model_contract/#saliency-layer","text":"Default value : None If using a Pytorch model, Saliency Maps can be available. Specify the name of the embedding layer on which to compute them. Example: distilbert.embeddings.word_embeddings .","title":"Saliency Layer"},{"location":"reference/configuration/model_contract/#metrics","text":"Default value : Precision and Recall. See in the config example below. By default, Azimuth will compute Precision, Recall and F1 on the dataset. metrics leverages custom objects, with an additional field which allow defining kwargs to be sent to the metric compute() function. You can add metrics available from the HuggingFace Metric Hub , or create your own, as detailed in Defining Metrics . Metric Definition Config Example from typing import Dict from pydantic import Field from azimuth.config import CustomObject class MetricDefinition ( CustomObject ): additional_kwargs : Dict = Field ( default_factory = dict , title = \"Additional kwargs\" , description = \"Keyword arguments supplied to `compute`\" , ) These are the default values. This example shows how this could be adapted to other metrics. { \"metrics\" : { \"Precision\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"precision\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"Recall\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"recall\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"F1\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"f1\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } } } }","title":"Metrics"},{"location":"reference/configuration/project/","text":"Project Config The project configuration contains mandatory fields that specify the dataset to load in Azimuth. Class Definition Config Example from typing import Optional from pydantic import BaseSettings , Field from azimuth.config import ColumnConfiguration , CustomObject class ProjectConfig ( BaseSettings ): name : str = Field ( \"New project\" , env = \"NAME\" ) dataset : CustomObject columns : ColumnConfiguration = ColumnConfiguration () rejection_class : Optional [ str ] = \"REJECTION_CLASS\" { \"name\" : \"Banking77 Model v4\" , \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , \"args\" : [ \"banking77\" ] }, \"columns\" : { \"text_input\" : \"text\" , \"label\" : \"target\" }, \"rejection_class\" : \"NA\" , } Name Default value : New project Environment Variable : NAME Any name can be set for the config. For example, it can represent the name of the dataset and/or the model. Ex: Banking77 Model v4 . Dataset Mandatory field To define which dataset to load in the application, Azimuth uses Custom Objects . If the dataset is already on HuggingFace, you can use the datasets.load_dataset from HF , as shown in the example below. If you have your own dataset, you will need to create your own custom object, as explained in Defining Dataset . Custom Object Definition Config Example with HF from typing import Any , Dict , List , Optional , Union from pydantic import BaseModel , Field class CustomObject ( BaseModel ): class_name : str = Field ( ... , title = \"Class name to load\" ) args : List [ Union [ \"CustomObject\" , Any ]] = [] kwargs : Dict [ str , Union [ \"CustomObject\" , Any ]] = {} remote : Optional [ str ] = None # (1) Absolute path to class_name . Example to load banking77 from HF . { \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , \"args\" : [ \"banking77\" ] } } Columns Default value : ColumnConfiguration() All dataset column names are configurable. The mandatory columns and their descriptions are as follows: Field name Default Description text_input utterance The preprocessed utterance. label label The class label for the utterance, as type datasets.ClassLabel . Class Definition Config Example from pydantic import BaseModel class ColumnConfiguration ( BaseModel ): text_input : str = \"utterance\" # (1) raw_text_input : str = \"utterance_raw\" # (2) label : str = \"label\" # (3) failed_parsing_reason : str = \"failed_parsing_reason\" # (4) Column for the text input that will be send to the pipeline. Optional column for the raw text input (before any pre-processing). Unused at the moment. Features column for the label Optional column to specify whether an example has failed preprocessing. Unused at the moment. Example to override the default column values. { \"columns\" : { \"text_input\" : \"text\" , \"label\" : \"target\" } } Rejection class Default value : REJECTION_CLASS The field rejection_class requires the class to be present in the dataset. If your dataset doesn't have a rejection class, set the value to null . More details on the rejection class is available in Prediction Outcomes .","title":"Project Config"},{"location":"reference/configuration/project/#project-config","text":"The project configuration contains mandatory fields that specify the dataset to load in Azimuth. Class Definition Config Example from typing import Optional from pydantic import BaseSettings , Field from azimuth.config import ColumnConfiguration , CustomObject class ProjectConfig ( BaseSettings ): name : str = Field ( \"New project\" , env = \"NAME\" ) dataset : CustomObject columns : ColumnConfiguration = ColumnConfiguration () rejection_class : Optional [ str ] = \"REJECTION_CLASS\" { \"name\" : \"Banking77 Model v4\" , \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , \"args\" : [ \"banking77\" ] }, \"columns\" : { \"text_input\" : \"text\" , \"label\" : \"target\" }, \"rejection_class\" : \"NA\" , }","title":"Project Config"},{"location":"reference/configuration/project/#name","text":"Default value : New project Environment Variable : NAME Any name can be set for the config. For example, it can represent the name of the dataset and/or the model. Ex: Banking77 Model v4 .","title":"Name"},{"location":"reference/configuration/project/#dataset","text":"Mandatory field To define which dataset to load in the application, Azimuth uses Custom Objects . If the dataset is already on HuggingFace, you can use the datasets.load_dataset from HF , as shown in the example below. If you have your own dataset, you will need to create your own custom object, as explained in Defining Dataset . Custom Object Definition Config Example with HF from typing import Any , Dict , List , Optional , Union from pydantic import BaseModel , Field class CustomObject ( BaseModel ): class_name : str = Field ( ... , title = \"Class name to load\" ) args : List [ Union [ \"CustomObject\" , Any ]] = [] kwargs : Dict [ str , Union [ \"CustomObject\" , Any ]] = {} remote : Optional [ str ] = None # (1) Absolute path to class_name . Example to load banking77 from HF . { \"dataset\" : { \"class_name\" : \"datasets.load_dataset\" , \"args\" : [ \"banking77\" ] } }","title":"Dataset"},{"location":"reference/configuration/project/#columns","text":"Default value : ColumnConfiguration() All dataset column names are configurable. The mandatory columns and their descriptions are as follows: Field name Default Description text_input utterance The preprocessed utterance. label label The class label for the utterance, as type datasets.ClassLabel . Class Definition Config Example from pydantic import BaseModel class ColumnConfiguration ( BaseModel ): text_input : str = \"utterance\" # (1) raw_text_input : str = \"utterance_raw\" # (2) label : str = \"label\" # (3) failed_parsing_reason : str = \"failed_parsing_reason\" # (4) Column for the text input that will be send to the pipeline. Optional column for the raw text input (before any pre-processing). Unused at the moment. Features column for the label Optional column to specify whether an example has failed preprocessing. Unused at the moment. Example to override the default column values. { \"columns\" : { \"text_input\" : \"text\" , \"label\" : \"target\" } }","title":"Columns"},{"location":"reference/configuration/project/#rejection-class","text":"Default value : REJECTION_CLASS The field rejection_class requires the class to be present in the dataset. If your dataset doesn't have a rejection class, set the value to null . More details on the rejection class is available in Prediction Outcomes .","title":"Rejection class"},{"location":"reference/configuration/analyses/","text":"Analyses Customization Four analyses can be configured in Azimuth. Go to each relevant section to learn more about the different attributes that can be defined. Class Definition Config Example class PerturbationTestingConfig ( ModelContractConfig ): behavioral_testing : Optional [ BehavioralTestingOptions ] = Field ( BehavioralTestingOptions (), env = \"BEHAVIORAL_TESTING\" ) class SimilarityConfig ( CommonFieldsConfig ): similarity : Optional [ SimilarityOptions ] = Field ( SimilarityOptions (), env = \"SIMILARITY\" ) class DatasetWarningConfig ( CommonFieldsConfig ): dataset_warnings : DatasetWarningsOptions = DatasetWarningsOptions () class SyntaxConfig ( CommonFieldsConfig ): syntax : SyntaxOptions = SyntaxOptions () This will disable both behavioral testing and similarity analysis in Azimuth. { \"behavioral_testing\" : null , \"similarity\" : null }","title":"Analyses Customization"},{"location":"reference/configuration/analyses/#analyses-customization","text":"Four analyses can be configured in Azimuth. Go to each relevant section to learn more about the different attributes that can be defined. Class Definition Config Example class PerturbationTestingConfig ( ModelContractConfig ): behavioral_testing : Optional [ BehavioralTestingOptions ] = Field ( BehavioralTestingOptions (), env = \"BEHAVIORAL_TESTING\" ) class SimilarityConfig ( CommonFieldsConfig ): similarity : Optional [ SimilarityOptions ] = Field ( SimilarityOptions (), env = \"SIMILARITY\" ) class DatasetWarningConfig ( CommonFieldsConfig ): dataset_warnings : DatasetWarningsOptions = DatasetWarningsOptions () class SyntaxConfig ( CommonFieldsConfig ): syntax : SyntaxOptions = SyntaxOptions () This will disable both behavioral testing and similarity analysis in Azimuth. { \"behavioral_testing\" : null , \"similarity\" : null }","title":"Analyses Customization"},{"location":"reference/configuration/analyses/behavioral_testing/","text":"Behavioral Testing Config Default value: BehavioralTestingOptions() Environment Variable : BEHAVIORAL_TESTING Behavioral Testing in the Key Concepts section explains how the different configuration attributes will affect the tests results. If your machine does not have a lot of computing power, behavioral_testing can be set to null . It can be enabled later on in the application. Class Definition Config Example Disabling Behavioral Testing from typing import List from pydantic import BaseModel class NeutralTokenOptions ( BaseModel ): threshold : float = 1 # (5) suffix_list : List [ str ] = [ \"pls\" , \"please\" , \"thank you\" , \"appreciated\" ] prefix_list : List [ str ] = [ \"pls\" , \"please\" , \"hello\" , \"greetings\" ] class PunctuationTestOptions ( BaseModel ): threshold : float = 1 # (4) class FuzzyMatchingTestOptions ( BaseModel ): threshold : float = 1 # (3) class TypoTestOptions ( BaseModel ): threshold : float = 1 # (2) nb_typos_per_utterance : int = 1 # (1) class BehavioralTestingOptions ( BaseModel ): neutral_token : NeutralTokenOptions = NeutralTokenOptions () punctuation : PunctuationTestOptions = PunctuationTestOptions () fuzzy_matching : FuzzyMatchingTestOptions = FuzzyMatchingTestOptions () typo : TypoTestOptions = TypoTestOptions () seed : int = 300 Ex: if nb_typos_per_utterance = 2, this will create 2 tests per utterance, one with 1 typo and another with 2 typos. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. For example, if the user wants to change the threshold for the punctuation test: { \"behavioral_testing\" : { \"punctuation\" : { \"threshold\" : 0.1 } } } { \"behavioral_testing\" : null }","title":"Behavioral Testing Config"},{"location":"reference/configuration/analyses/behavioral_testing/#behavioral-testing-config","text":"Default value: BehavioralTestingOptions() Environment Variable : BEHAVIORAL_TESTING Behavioral Testing in the Key Concepts section explains how the different configuration attributes will affect the tests results. If your machine does not have a lot of computing power, behavioral_testing can be set to null . It can be enabled later on in the application. Class Definition Config Example Disabling Behavioral Testing from typing import List from pydantic import BaseModel class NeutralTokenOptions ( BaseModel ): threshold : float = 1 # (5) suffix_list : List [ str ] = [ \"pls\" , \"please\" , \"thank you\" , \"appreciated\" ] prefix_list : List [ str ] = [ \"pls\" , \"please\" , \"hello\" , \"greetings\" ] class PunctuationTestOptions ( BaseModel ): threshold : float = 1 # (4) class FuzzyMatchingTestOptions ( BaseModel ): threshold : float = 1 # (3) class TypoTestOptions ( BaseModel ): threshold : float = 1 # (2) nb_typos_per_utterance : int = 1 # (1) class BehavioralTestingOptions ( BaseModel ): neutral_token : NeutralTokenOptions = NeutralTokenOptions () punctuation : PunctuationTestOptions = PunctuationTestOptions () fuzzy_matching : FuzzyMatchingTestOptions = FuzzyMatchingTestOptions () typo : TypoTestOptions = TypoTestOptions () seed : int = 300 Ex: if nb_typos_per_utterance = 2, this will create 2 tests per utterance, one with 1 typo and another with 2 typos. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. Threshold that defines the confidence gap above which the test will fail. For example, if the user wants to change the threshold for the punctuation test: { \"behavioral_testing\" : { \"punctuation\" : { \"threshold\" : 0.1 } } } { \"behavioral_testing\" : null }","title":"Behavioral Testing Config"},{"location":"reference/configuration/analyses/dataset_warnings/","text":"Dataset Class Distribution Analysis Config Default value: DatasetWarningsOptions() Some thresholds can be set to modify the number of warnings in the Dataset Class Distribution Analysis . Class Definition Config Example from pydantic import BaseModel class DatasetWarningsOptions ( BaseModel ): min_num_per_class : int = 20 # (1) max_delta_class_imbalance : float = 0.5 # (2) max_delta_representation : float = 0.05 # (3) max_delta_mean_tokens : float = 3.0 # (4) max_delta_std_tokens : float = 3.0 # (5) Threshold for the first set of warnings (missing samples). Threshold for the second set of warnings (class imbalance). Threshold for the third set of warnings (representation mismatch). Threshold for the fourth set of warnings (length mismatch). Threshold for the fourth set of warnings (length mismatch). { \"dataset_warnings\" : { \"min_num_per_class\" : 40 } }","title":"Dataset Class Distribution Analysis Config"},{"location":"reference/configuration/analyses/dataset_warnings/#dataset-class-distribution-analysis-config","text":"Default value: DatasetWarningsOptions() Some thresholds can be set to modify the number of warnings in the Dataset Class Distribution Analysis . Class Definition Config Example from pydantic import BaseModel class DatasetWarningsOptions ( BaseModel ): min_num_per_class : int = 20 # (1) max_delta_class_imbalance : float = 0.5 # (2) max_delta_representation : float = 0.05 # (3) max_delta_mean_tokens : float = 3.0 # (4) max_delta_std_tokens : float = 3.0 # (5) Threshold for the first set of warnings (missing samples). Threshold for the second set of warnings (class imbalance). Threshold for the third set of warnings (representation mismatch). Threshold for the fourth set of warnings (length mismatch). Threshold for the fourth set of warnings (length mismatch). { \"dataset_warnings\" : { \"min_num_per_class\" : 40 } }","title":"Dataset Class Distribution Analysis Config"},{"location":"reference/configuration/analyses/similarity/","text":"Similarity Analysis Config Default value: SimilarityOptions() Environment Variable : SIMILARITY In Key Concepts, Similarity Analysis explains how the different configuration attributes will affect the analysis results. If your machine does not have a lot of computing power, similarity can be set to null . It can be enabled later on in the application. Class Definition Config Example Disabling Similarity Analysis from pydantic import BaseModel class SimilarityOptions ( BaseModel ): faiss_encoder : str = \"all-MiniLM-L12-v2\" # (1) few_similar_threshold : float = 0.9 # (2) no_close_threshold : float = 0.5 # (3) The name of your encoder must be supported by sentence-transformers . Threshold to determine the ratio of utterances that should belong to another class for the smart tags conflicting_neighbors_train / conflicting_neighbors_eval . Threshold for cosine similarity for the smart tags no_close_train / no_close_eval . { \"similarity\" : { \"faiss_encoder\" : \"your_encoder\" } } { \"similarity\" : null }","title":"Similarity Analysis Config"},{"location":"reference/configuration/analyses/similarity/#similarity-analysis-config","text":"Default value: SimilarityOptions() Environment Variable : SIMILARITY In Key Concepts, Similarity Analysis explains how the different configuration attributes will affect the analysis results. If your machine does not have a lot of computing power, similarity can be set to null . It can be enabled later on in the application. Class Definition Config Example Disabling Similarity Analysis from pydantic import BaseModel class SimilarityOptions ( BaseModel ): faiss_encoder : str = \"all-MiniLM-L12-v2\" # (1) few_similar_threshold : float = 0.9 # (2) no_close_threshold : float = 0.5 # (3) The name of your encoder must be supported by sentence-transformers . Threshold to determine the ratio of utterances that should belong to another class for the smart tags conflicting_neighbors_train / conflicting_neighbors_eval . Threshold for cosine similarity for the smart tags no_close_train / no_close_eval . { \"similarity\" : { \"faiss_encoder\" : \"your_encoder\" } } { \"similarity\" : null }","title":"Similarity Analysis Config"},{"location":"reference/configuration/analyses/syntax/","text":"Syntax Analysis Config Default value: SyntaxOptions() Some thresholds can be modified to determine what is considered a short or a long sentence, as explained in Syntax Analysis . Class Definition Config Example from pydantic import BaseModel class SyntaxOptions ( BaseModel ): short_sentence_max_token : int = 3 # (1) long_sentence_min_token : int = 16 # (2) Maximum number of tokens for a sentence to be tagged as short (e.g <=3 for the default) Minimum number of tokens for a sentence to be tagged as long (e.g >=16 for the default) { \"syntax\" : { \"short_sentence_max_token\" : 5 } }","title":"Syntax Analysis Config"},{"location":"reference/configuration/analyses/syntax/#syntax-analysis-config","text":"Default value: SyntaxOptions() Some thresholds can be modified to determine what is considered a short or a long sentence, as explained in Syntax Analysis . Class Definition Config Example from pydantic import BaseModel class SyntaxOptions ( BaseModel ): short_sentence_max_token : int = 3 # (1) long_sentence_min_token : int = 16 # (2) Maximum number of tokens for a sentence to be tagged as short (e.g <=3 for the default) Minimum number of tokens for a sentence to be tagged as long (e.g >=16 for the default) { \"syntax\" : { \"short_sentence_max_token\" : 5 } }","title":"Syntax Analysis Config"},{"location":"reference/custom-objects/","text":"Custom Objects Azimuth uses Custom Objects to define how to integrate with models, datasets and metrics. Custom Objects are instantiated at runtime to load the object; the ML pipeline function will return a pipeline, and the Dataset function will return the dataset. Our primary integration is through HuggingFace , but Azimuth supports any other type of dataset or model. Defining Datasets Defining a Model Defining Postprocessors Defining Metrics What is a Custom Object? A Custom Object is simply a path to a function and its arguments. When users supply their functions and classes, they should be added to azimuth_shr , as it is already mounted at startup on the Docker image. from typing import Any , Dict , List , Optional , Union from pydantic import BaseModel , Field class CustomObject ( BaseModel ): class_name : str = Field ( ... , title = \"Class name to load\" ) args : List [ Union [ \"CustomObject\" , Any ]] = [] kwargs : Dict [ str , Union [ \"CustomObject\" , Any ]] = {} remote : Optional [ str ] = None # (1) Absolute path to class_name . class_name is the name of the function or class that is located in remote . args and kwargs will be sent to the function/class. Example Here is in example of two custom objects. In azimuth_shr/loading_resources.py , we will add two functions which will load a model and a dataset. The configuration file will then be defined as shown in config example. azimuth_shr/loading_resources.py Config Example import transformers import datasets def my_model ( ckpt_path ) -> transformers . Pipeline : pipeline = ... # Load the pipeline from ckpt_path return pipeline def my_dataset ( ckpt_path ) -> datasets . DatasetDict : dataset = ... # Load the Dataset from ckpt_path return dataset { \"dataset\" : { \"class_name\" : \"loading_resources.my_dataset\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"/azimuth_shr/data/my_dataset\" # (1) } }, \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.my_model\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } } } ] } Path to the dataset. Should be put under /azimuth_shr so it is mounted on the Docker image automatically. Using the Config If the function has an argument named azimuth_config , Azimuth will supply the config file to the function automatically. This can be useful if some attributes from the config are needed. from azimuth.config import AzimuthConfig # This is fine. def my_model ( num_classes , azimuth_config : AzimuthConfig ): pass # Also fine def my_model ( num_classes ): pass Install dependencies at runtime To avoid needing to modify the Docker image to include your dependencies, Azimuth supports installing dependencies at runtime. To do this, the field remote can be one of the following: A package on Pypi Example: remote: \"torchvision\" A folder with requirements.txt A folder with setup.py If none of the above matches, we append remote to PYTHONPATH , but no dependency will be installed.","title":"Custom Objects"},{"location":"reference/custom-objects/#custom-objects","text":"Azimuth uses Custom Objects to define how to integrate with models, datasets and metrics. Custom Objects are instantiated at runtime to load the object; the ML pipeline function will return a pipeline, and the Dataset function will return the dataset. Our primary integration is through HuggingFace , but Azimuth supports any other type of dataset or model. Defining Datasets Defining a Model Defining Postprocessors Defining Metrics","title":"Custom Objects"},{"location":"reference/custom-objects/#what-is-a-custom-object","text":"A Custom Object is simply a path to a function and its arguments. When users supply their functions and classes, they should be added to azimuth_shr , as it is already mounted at startup on the Docker image. from typing import Any , Dict , List , Optional , Union from pydantic import BaseModel , Field class CustomObject ( BaseModel ): class_name : str = Field ( ... , title = \"Class name to load\" ) args : List [ Union [ \"CustomObject\" , Any ]] = [] kwargs : Dict [ str , Union [ \"CustomObject\" , Any ]] = {} remote : Optional [ str ] = None # (1) Absolute path to class_name . class_name is the name of the function or class that is located in remote . args and kwargs will be sent to the function/class.","title":"What is a Custom Object?"},{"location":"reference/custom-objects/#example","text":"Here is in example of two custom objects. In azimuth_shr/loading_resources.py , we will add two functions which will load a model and a dataset. The configuration file will then be defined as shown in config example. azimuth_shr/loading_resources.py Config Example import transformers import datasets def my_model ( ckpt_path ) -> transformers . Pipeline : pipeline = ... # Load the pipeline from ckpt_path return pipeline def my_dataset ( ckpt_path ) -> datasets . DatasetDict : dataset = ... # Load the Dataset from ckpt_path return dataset { \"dataset\" : { \"class_name\" : \"loading_resources.my_dataset\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"/azimuth_shr/data/my_dataset\" # (1) } }, \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.my_model\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"ckpt_path\" : \"distilbert-base-uncased-finetuned-sst-2-english\" } } } ] } Path to the dataset. Should be put under /azimuth_shr so it is mounted on the Docker image automatically.","title":"Example"},{"location":"reference/custom-objects/#using-the-config","text":"If the function has an argument named azimuth_config , Azimuth will supply the config file to the function automatically. This can be useful if some attributes from the config are needed. from azimuth.config import AzimuthConfig # This is fine. def my_model ( num_classes , azimuth_config : AzimuthConfig ): pass # Also fine def my_model ( num_classes ): pass","title":"Using the Config"},{"location":"reference/custom-objects/#install-dependencies-at-runtime","text":"To avoid needing to modify the Docker image to include your dependencies, Azimuth supports installing dependencies at runtime. To do this, the field remote can be one of the following: A package on Pypi Example: remote: \"torchvision\" A folder with requirements.txt A folder with setup.py If none of the above matches, we append remote to PYTHONPATH , but no dependency will be installed.","title":"Install dependencies at runtime"},{"location":"reference/custom-objects/dataset/","text":"Defining Dataset In Project Config is described how a dataset needs to be defined with a Custom Object in the config. This section details how to define the class_name , args and kwargs defined in the custom object. Dataset Definition Azimuth supports the HuggingFace Dataset API . The loading function for the dataset must respect the following contract: from datasets import DatasetDict from azimuth.config import AzimuthConfig def load_your_dataset ( azimuth_config : AzimuthConfig , ** kwargs ) -> DatasetDict : ... Your don't have a HuggingFace Dataset ? If your dataset is not a HuggingFace Dataset , you can convert it easily using the following resources from HuggingFace: from local files from in-memory data We suggest following this HuggingFace tutorial to know more about dataset loading using Huggingface. Dataset splits Azimuth expects the train and one of validation or test splits to be available. If both validation and test are available, we will pick the former. The train is not mandatory for Azimuth to run. Column names and rejection class Go to the Project Config to see other attributes that should be set along with the dataset. Example Using this API , we can load SST2, a sentiment analysis dataset. Note: in this case, we can omit azimuth_config from the definition because we don't need it. azimuth_shr/loading_resources.py Configuration file from datasets import DatasetDict , load_dataset def load_sst2_dataset ( dataset_name : str ) -> DatasetDict : datasets = load_dataset ( \"glue\" , dataset_name ) return DatasetDict ( { \"train\" : datasets [ \"train\" ], \"validation\" : datasets [ \"validation\" ]} ) { \"dataset\" : { \"class_name\" : \"loading_resources.load_sst2_dataset\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"dataset_name\" : \"sst2\" } }, \"columns\" : { \"text_input\" : \"sentence\" }, \"rejection_class\" : null }","title":"Defining Dataset"},{"location":"reference/custom-objects/dataset/#defining-dataset","text":"In Project Config is described how a dataset needs to be defined with a Custom Object in the config. This section details how to define the class_name , args and kwargs defined in the custom object.","title":"Defining Dataset"},{"location":"reference/custom-objects/dataset/#dataset-definition","text":"Azimuth supports the HuggingFace Dataset API . The loading function for the dataset must respect the following contract: from datasets import DatasetDict from azimuth.config import AzimuthConfig def load_your_dataset ( azimuth_config : AzimuthConfig , ** kwargs ) -> DatasetDict : ... Your don't have a HuggingFace Dataset ? If your dataset is not a HuggingFace Dataset , you can convert it easily using the following resources from HuggingFace: from local files from in-memory data We suggest following this HuggingFace tutorial to know more about dataset loading using Huggingface.","title":"Dataset Definition"},{"location":"reference/custom-objects/dataset/#dataset-splits","text":"Azimuth expects the train and one of validation or test splits to be available. If both validation and test are available, we will pick the former. The train is not mandatory for Azimuth to run.","title":"Dataset splits"},{"location":"reference/custom-objects/dataset/#column-names-and-rejection-class","text":"Go to the Project Config to see other attributes that should be set along with the dataset.","title":"Column names and rejection class"},{"location":"reference/custom-objects/dataset/#example","text":"Using this API , we can load SST2, a sentiment analysis dataset. Note: in this case, we can omit azimuth_config from the definition because we don't need it. azimuth_shr/loading_resources.py Configuration file from datasets import DatasetDict , load_dataset def load_sst2_dataset ( dataset_name : str ) -> DatasetDict : datasets = load_dataset ( \"glue\" , dataset_name ) return DatasetDict ( { \"train\" : datasets [ \"train\" ], \"validation\" : datasets [ \"validation\" ]} ) { \"dataset\" : { \"class_name\" : \"loading_resources.load_sst2_dataset\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"dataset_name\" : \"sst2\" } }, \"columns\" : { \"text_input\" : \"sentence\" }, \"rejection_class\" : null }","title":"Example"},{"location":"reference/custom-objects/metric/","text":"Defining Metrics Azimuth builds upon the HuggingFace Metric API . Please refer to HuggingFace Metric Hub that details all available metrics that can be added to Azimuth. Metric definition A metric definition is a simple Custom Object that loads a HuggingFace metric. The metric custom object has an extra-field additional_kwargs that will be pass to the compute() function of the HuggingFace metric. This will be required for some metrics, as shown in the example with precision, recall and F1 below. Metric Definition Config Example from typing import Dict from pydantic import Field from azimuth.config import CustomObject class MetricDefinition ( CustomObject ): additional_kwargs : Dict = Field ( default_factory = dict , title = \"Additional kwargs\" , description = \"Keyword arguments supplied to `compute`.\" , ) By default, Azimuth loads Precision, Recall and F1 with average=weighted : { \"metrics\" : { \"Precision\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"precision\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"Recall\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"recall\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"F1\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"f1\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } } } } Custom Metrics For more experienced users, HuggingFace presents how to create custom metrics in this tutorial . For custom metrics, Azimuth supplies probabilities when possible. The compute method must follow this signature: from typing import Any , Dict , List import numpy as np def _compute ( self , predictions : List [ int ], # (1) references : List [ int ], # (2) probabilities : np . ndarray , # (3) ) -> Dict [ str , Any ]: # (4) ... Predicted labels (1 class per sample). Ground truth labels. Probabilities computed by the pipeline. A dictionary with the computed values.","title":"Defining Metrics"},{"location":"reference/custom-objects/metric/#defining-metrics","text":"Azimuth builds upon the HuggingFace Metric API . Please refer to HuggingFace Metric Hub that details all available metrics that can be added to Azimuth.","title":"Defining Metrics"},{"location":"reference/custom-objects/metric/#metric-definition","text":"A metric definition is a simple Custom Object that loads a HuggingFace metric. The metric custom object has an extra-field additional_kwargs that will be pass to the compute() function of the HuggingFace metric. This will be required for some metrics, as shown in the example with precision, recall and F1 below. Metric Definition Config Example from typing import Dict from pydantic import Field from azimuth.config import CustomObject class MetricDefinition ( CustomObject ): additional_kwargs : Dict = Field ( default_factory = dict , title = \"Additional kwargs\" , description = \"Keyword arguments supplied to `compute`.\" , ) By default, Azimuth loads Precision, Recall and F1 with average=weighted : { \"metrics\" : { \"Precision\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"precision\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"Recall\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"recall\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } }, \"F1\" : { \"class_name\" : \"datasets.load_metric\" , \"kwargs\" : { \"path\" : \"f1\" }, \"additional_kwargs\" : { \"average\" : \"weighted\" } } } }","title":"Metric definition"},{"location":"reference/custom-objects/metric/#custom-metrics","text":"For more experienced users, HuggingFace presents how to create custom metrics in this tutorial . For custom metrics, Azimuth supplies probabilities when possible. The compute method must follow this signature: from typing import Any , Dict , List import numpy as np def _compute ( self , predictions : List [ int ], # (1) references : List [ int ], # (2) probabilities : np . ndarray , # (3) ) -> Dict [ str , Any ]: # (4) ... Predicted labels (1 class per sample). Ground truth labels. Probabilities computed by the pipeline. A dictionary with the computed values.","title":"Custom Metrics"},{"location":"reference/custom-objects/model/","text":"Defining a Model A model is defined as a function that takes an utterance as input and outputs a SupportedOutput (defined below). Some models will output probabilities, whereas other models are \"pipelines\" which include post-processing steps. Azimuth supports both these use cases. The model_contract field from the config will determine how Azimuth will interface with the model, as detailed in this section. Model Definition In the config, the custom object for the model defines the class that will load the model. from typing import Union , Callable from transformers import Pipeline from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Union [ Pipeline , Callable ]: ... The output of the function ( transformers.Pipeline or Callable ) will determine the model_contract that should be used. When a transformers.Pipeline from HF is returned, we expect it to contain tokenizer: Tokenizer and model: nn.Module . Model Prediction Signature The inputs of the model prediction signature will change based on the model_contract . However, the supported outputs are the same for all model contracts. Supported Output from typing import Union import numpy as np import transformers from torch import Tensor # (1) from azimuth.modules.model_contracts.text_classification import ( PipelineOutputProtocol ) SupportedOutput = Union [ np . ndarray , Tensor , transformers . file_utils . ModelOutput , PipelineOutputProtocol ] Tensor from TensorFlow is also supported. If the model does not have its own postprocessing, the supported output should be one of the 3 first outputs listed above, as shown in the example below. Example Assuming the following function, the table below shows how to transform probs in the 3 first supported outputs. import numpy as np from scipy.special import softmax NUM_SAMPLES = 10 NUM_CLASSES = 2 probs = softmax ( np . random . rand ( NUM_SAMPLES , NUM_CLASSES ), - 1 ) Supported Output Example Shape np.ndarray probs [N, num_classes] Tensor torch.from_numpy(probs) or tf.convert_to_tensor(probs) [N, num_classes] ModelOutput SequenceClassifierOutput(logits=torch.log(probs)) NA If your model already includes post-processing, or if you decide to create your own post-processing in Azimuth (we already support thresholding and temperature scaling), it will need to output a PipelineOutputProtocol . More details can be found in Define Postprocessors . Model contracts To differentiate between the different types of models, Azimuth uses a field named model_contract . model_contract Model type Framework hf_text_classification HuggingFace Pipeline Supported by HF custom_text_classification Callable Any file_based_text_classification Callable Any HuggingFace Pipeline model_contract Model type Framework hf_text_classification HuggingFace Pipeline Supported by HF * *Saliency maps are only available for Pytorch models. This is our canonical API . It currently supports all of our features and includes some optimization for HuggingFace inference. Model Definition In the config, the custom object for the model should return a transformers.Pipeline . from transformers import Pipeline from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Pipeline : ... For most use cases, loading_resources.load_hf_text_classif_pipeline will work. Model Prediction Signature The prediction signature is the one from transformers . We will supply the following arguments: from typing import List from azimuth.modules.model_contracts.text_classification import SupportedOutput def __call__ ( utterances : List [ str ], num_workers : int , batch_size : int ) -> SupportedOutput : ... Example This is how we would load a pretrained BERT model using this API : azimuth_shr/loading_resources.py Configuration file from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , Pipeline , TextClassificationPipeline , ) from azimuth.config import AzimuthConfig from azimuth_shr.loading_resources import _should_use_cuda def load_text_classif_pipeline ( checkpoint_path : str , azimuth_config : AzimuthConfig ) -> Pipeline : model = AutoModelForSequenceClassification . from_pretrained ( checkpoint_path ) tokenizer = AutoTokenizer . from_pretrained ( checkpoint_path , use_fast = False ) device = 0 if _should_use_cuda ( azimuth_config ) else - 1 return TextClassificationPipeline ( model = model , tokenizer = tokenizer , device = device , return_all_scores = True ) # (1) We set return_all_scores=True to get all softmax outputs. \"model_contract\" : \"hf_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"checkpoint_path\" : \"/azimuth_shr/files/clinc/CLINC150_trained_model\" } } } ] Examples are also provided in the repo under config/examples . Other frameworks model_contract Model type Framework custom_text_classification Callable Any This general purpose API can be used with any framework. Model Definition The user-defined model is a Callable that returns predictions from a list of strings. from typing import Callable from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Callable : ... Model Prediction Signature When the model is called with a list of utterances, it should return a prediction for each utterance. from typing import List from azimuth.modules.model_contracts.text_classification import SupportedOutput def __call__ ( utterances : List [ str ]) -> SupportedOutput : ... Disabled features Saliency maps Epistemic uncertainty estimation Example For models coming from other frameworks, the loading function returns a Callable . azimuth_shr/loading_resources.py Configuration file def load_keras_model ( checkpoint_path : str , azimuth_config : AzimuthConfig ) -> Callable : model = tf . keras . models . load_model ( checkpoint_path ) return model \"model_contract\" : \"custom_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_keras_model\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"checkpoint_path\" : \"/azimuth_shr/files/clinc/keras_clinc_model\" } } } ] File-based model_contract Model type Framework file_based_text_classification Callable Any Azimuth can also work without a model, but with predictions supplied in a file. To do so, when calling the prediction function, Azimuth will provide the row index of each utterance along with the sentence. The predictions should be after postprocessing. Model Definition In azimut_shr/models/file_based.FileBasedModel is given the class that can be used to load file-based models. Model Prediction Signature from typing import Any , Dict from azimuth.modules.model_contracts.text_classification import SupportedOutput from azimuth.types import DatasetSplitName def __call__ ( utterances : Dict [ str , Any ], dataset_split_name : DatasetSplitName ) -> SupportedOutput : ... where the utterances have the following keys: row_idx : the indices for each row. utterance : utterances as defined in AzimuthConfig.columns.text_input . Disabled features Saliency maps Epistemic uncertainty estimation Threshold Comparison Example { \"model_contract\" : \"file_based_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"models.file_based.FileBasedModel\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"test_path\" : \"/azimuth_shr/path_to_your_prediction_file\" } } } ] }","title":"Defining a Model"},{"location":"reference/custom-objects/model/#defining-a-model","text":"A model is defined as a function that takes an utterance as input and outputs a SupportedOutput (defined below). Some models will output probabilities, whereas other models are \"pipelines\" which include post-processing steps. Azimuth supports both these use cases. The model_contract field from the config will determine how Azimuth will interface with the model, as detailed in this section.","title":"Defining a Model"},{"location":"reference/custom-objects/model/#model-definition","text":"In the config, the custom object for the model defines the class that will load the model. from typing import Union , Callable from transformers import Pipeline from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Union [ Pipeline , Callable ]: ... The output of the function ( transformers.Pipeline or Callable ) will determine the model_contract that should be used. When a transformers.Pipeline from HF is returned, we expect it to contain tokenizer: Tokenizer and model: nn.Module .","title":"Model Definition"},{"location":"reference/custom-objects/model/#model-prediction-signature","text":"The inputs of the model prediction signature will change based on the model_contract . However, the supported outputs are the same for all model contracts.","title":"Model Prediction Signature"},{"location":"reference/custom-objects/model/#supported-output","text":"from typing import Union import numpy as np import transformers from torch import Tensor # (1) from azimuth.modules.model_contracts.text_classification import ( PipelineOutputProtocol ) SupportedOutput = Union [ np . ndarray , Tensor , transformers . file_utils . ModelOutput , PipelineOutputProtocol ] Tensor from TensorFlow is also supported. If the model does not have its own postprocessing, the supported output should be one of the 3 first outputs listed above, as shown in the example below.","title":"Supported Output"},{"location":"reference/custom-objects/model/#example","text":"Assuming the following function, the table below shows how to transform probs in the 3 first supported outputs. import numpy as np from scipy.special import softmax NUM_SAMPLES = 10 NUM_CLASSES = 2 probs = softmax ( np . random . rand ( NUM_SAMPLES , NUM_CLASSES ), - 1 ) Supported Output Example Shape np.ndarray probs [N, num_classes] Tensor torch.from_numpy(probs) or tf.convert_to_tensor(probs) [N, num_classes] ModelOutput SequenceClassifierOutput(logits=torch.log(probs)) NA If your model already includes post-processing, or if you decide to create your own post-processing in Azimuth (we already support thresholding and temperature scaling), it will need to output a PipelineOutputProtocol . More details can be found in Define Postprocessors .","title":"Example"},{"location":"reference/custom-objects/model/#model-contracts","text":"To differentiate between the different types of models, Azimuth uses a field named model_contract . model_contract Model type Framework hf_text_classification HuggingFace Pipeline Supported by HF custom_text_classification Callable Any file_based_text_classification Callable Any","title":"Model contracts"},{"location":"reference/custom-objects/model/#huggingface-pipeline","text":"model_contract Model type Framework hf_text_classification HuggingFace Pipeline Supported by HF * *Saliency maps are only available for Pytorch models. This is our canonical API . It currently supports all of our features and includes some optimization for HuggingFace inference.","title":"HuggingFace Pipeline"},{"location":"reference/custom-objects/model/#model-definition_1","text":"In the config, the custom object for the model should return a transformers.Pipeline . from transformers import Pipeline from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Pipeline : ... For most use cases, loading_resources.load_hf_text_classif_pipeline will work.","title":"Model Definition"},{"location":"reference/custom-objects/model/#model-prediction-signature_1","text":"The prediction signature is the one from transformers . We will supply the following arguments: from typing import List from azimuth.modules.model_contracts.text_classification import SupportedOutput def __call__ ( utterances : List [ str ], num_workers : int , batch_size : int ) -> SupportedOutput : ...","title":"Model Prediction Signature"},{"location":"reference/custom-objects/model/#example_1","text":"This is how we would load a pretrained BERT model using this API : azimuth_shr/loading_resources.py Configuration file from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , Pipeline , TextClassificationPipeline , ) from azimuth.config import AzimuthConfig from azimuth_shr.loading_resources import _should_use_cuda def load_text_classif_pipeline ( checkpoint_path : str , azimuth_config : AzimuthConfig ) -> Pipeline : model = AutoModelForSequenceClassification . from_pretrained ( checkpoint_path ) tokenizer = AutoTokenizer . from_pretrained ( checkpoint_path , use_fast = False ) device = 0 if _should_use_cuda ( azimuth_config ) else - 1 return TextClassificationPipeline ( model = model , tokenizer = tokenizer , device = device , return_all_scores = True ) # (1) We set return_all_scores=True to get all softmax outputs. \"model_contract\" : \"hf_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_text_classif_pipeline\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"checkpoint_path\" : \"/azimuth_shr/files/clinc/CLINC150_trained_model\" } } } ] Examples are also provided in the repo under config/examples .","title":"Example"},{"location":"reference/custom-objects/model/#other-frameworks","text":"model_contract Model type Framework custom_text_classification Callable Any This general purpose API can be used with any framework.","title":"Other frameworks"},{"location":"reference/custom-objects/model/#model-definition_2","text":"The user-defined model is a Callable that returns predictions from a list of strings. from typing import Callable from azimuth.config import AzimuthConfig def load_model ( azimuth_config : AzimuthConfig , ** kwargs ) -> Callable : ...","title":"Model Definition"},{"location":"reference/custom-objects/model/#model-prediction-signature_2","text":"When the model is called with a list of utterances, it should return a prediction for each utterance. from typing import List from azimuth.modules.model_contracts.text_classification import SupportedOutput def __call__ ( utterances : List [ str ]) -> SupportedOutput : ...","title":"Model Prediction Signature"},{"location":"reference/custom-objects/model/#disabled-features","text":"Saliency maps Epistemic uncertainty estimation","title":"Disabled features"},{"location":"reference/custom-objects/model/#example_2","text":"For models coming from other frameworks, the loading function returns a Callable . azimuth_shr/loading_resources.py Configuration file def load_keras_model ( checkpoint_path : str , azimuth_config : AzimuthConfig ) -> Callable : model = tf . keras . models . load_model ( checkpoint_path ) return model \"model_contract\" : \"custom_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"loading_resources.load_keras_model\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"checkpoint_path\" : \"/azimuth_shr/files/clinc/keras_clinc_model\" } } } ]","title":"Example"},{"location":"reference/custom-objects/model/#file-based","text":"model_contract Model type Framework file_based_text_classification Callable Any Azimuth can also work without a model, but with predictions supplied in a file. To do so, when calling the prediction function, Azimuth will provide the row index of each utterance along with the sentence. The predictions should be after postprocessing.","title":"File-based"},{"location":"reference/custom-objects/model/#model-definition_3","text":"In azimut_shr/models/file_based.FileBasedModel is given the class that can be used to load file-based models.","title":"Model Definition"},{"location":"reference/custom-objects/model/#model-prediction-signature_3","text":"from typing import Any , Dict from azimuth.modules.model_contracts.text_classification import SupportedOutput from azimuth.types import DatasetSplitName def __call__ ( utterances : Dict [ str , Any ], dataset_split_name : DatasetSplitName ) -> SupportedOutput : ... where the utterances have the following keys: row_idx : the indices for each row. utterance : utterances as defined in AzimuthConfig.columns.text_input .","title":"Model Prediction Signature"},{"location":"reference/custom-objects/model/#disabled-features_1","text":"Saliency maps Epistemic uncertainty estimation Threshold Comparison","title":"Disabled features"},{"location":"reference/custom-objects/model/#example_3","text":"{ \"model_contract\" : \"file_based_text_classification\" , \"pipelines\" : [ { \"model\" : { \"class_name\" : \"models.file_based.FileBasedModel\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"test_path\" : \"/azimuth_shr/path_to_your_prediction_file\" } } } ] }","title":"Example"},{"location":"reference/custom-objects/postprocessors/","text":"Defining Postprocessors Postprocessors can be applied after any model in Azimuth. Default Postprocessors in Azimuth By default, Azimuth applies thresholding at 0.5. It also supports Temperature Scaling. Azimuth provides shortcuts to override the threshold and the temperature. Add this as postprocessors in the PipelineDefinition . For Temperature Scaling : {\"postprocessors\": [{\"temperature\": 1}]} For Thresholding : {\"postprocessors\": [{\"threshold\": 0.5}]} Model with its own postprocessing If your model already includes postprocessing, as explained in Define a Model , the model prediction signature needs to have a specific output, PipelineOutputProtocol , that will contain both the predictions from the model, and the post-processed predictions. from typing import Protocol from azimuth.utils.ml.postprocessing import PostProcessingIO class PipelineOutputProtocol ( Protocol ): \"\"\"Class containing result of a batch\"\"\" model_output : PostProcessingIO # (1) postprocessor_output : PostProcessingIO # (2) model output before passing through post-processing stage: texts, logits, probs, preds output after passing through post-processing: pre-processed texts, logits, probs, preds PostProcessingIO is defined as the following. from typing import List from azimuth.types import AliasModel , Array class PostProcessingIO ( AliasModel ): texts : List [ str ] # (1) logits : Array [ float ] # (2) preds : Array [ int ] # (3) probs : Array [ float ] # (4) The utterance text. Length of N Logits of the model. Shape of (N, C) Predicted class, argmax of probabilities. Shape of (N, 1) Probabilities of the model # Shape of (N, C) In your code, you don't have to extend PipelineOutputProtocol or PostProcessingIO ; you can use your own library, and as long as the fields match, Azimuth will accept it. This is done so that our users don't have to add Azimuth as a dependency. @dataclass class PostprocessingData : texts : List [ str ] # len [num_samples] logits : np . ndarray # shape [num_samples, classes] preds : np . ndarray # shape [num_samples] probs : np . ndarray # shape [num_samples, classes] ### Valid class MyPipelineOutput ( BaseModel ): # Could be dataclass as well. model_output : PostprocessingData postprocessor_output : PostprocessingData ### Invalid because the field names do not match class MyPipelineOutput ( BaseModel ): predictions : MyPipelineOutput postprocessed_predictions : MyPipelineOutput In the Config {\"postprocessors\": null} should then be added to the config, to avoid re-postprocessing in Azimuth. User-Defined Postprocessors Similarly to a model and a dataset, users can add their own postprocessors in Azimuth with custom objects. However, some typing needs to be respected for Azimuth to handle it. First, the post-processing class needs PostProcessingIO , as defined above, as both input and output. To get consistent results, all values need to be updated by the post-processors. For example, if a postprocessor modifies the logits , it must recompute probs as well. The API for a postprocessor is the following: from azimuth.utils.ml.postprocessing import PostProcessingIO def __call__ ( self , post_processing_io : PostProcessingIO ) -> PostProcessingIO : ... You can also extend azimuth.utils.ml.postprocessing.Postprocessing to write your own postprocessor. Example Let's define a postprocessor that will do Temperature scaling: azimuth_shr/loading_resources.py Configuration file from azimuth.functional.postprocessing import PostProcessingIO from scipy.special import expit , softmax class TemperatureScaling : def __init__ ( self , temperature ): self . temperature = temperature def __call__ ( self , post_processing_io : PostProcessingIO ) -> PostProcessingIO : new_logits = post_processing_io . logits / self . temperature confidences = ( softmax ( new_logits , axis = 1 ) if post_processing_io . is_multiclass else expit ( new_logits ) ) return PostProcessingIO ( texts = post_processing_io . texts , logits = new_logits , preds = post_processing_io . preds , probs = confidences , ) \"pipelines\" : [ { \"model\" : ... , \"postprocessors\" : [ { \"class_name\" : \"loading_resources.TemperatureScaling\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"temperature\" : 3 } } ] } ]","title":"Defining Postprocessors"},{"location":"reference/custom-objects/postprocessors/#defining-postprocessors","text":"Postprocessors can be applied after any model in Azimuth.","title":"Defining Postprocessors"},{"location":"reference/custom-objects/postprocessors/#default-postprocessors-in-azimuth","text":"By default, Azimuth applies thresholding at 0.5. It also supports Temperature Scaling. Azimuth provides shortcuts to override the threshold and the temperature. Add this as postprocessors in the PipelineDefinition . For Temperature Scaling : {\"postprocessors\": [{\"temperature\": 1}]} For Thresholding : {\"postprocessors\": [{\"threshold\": 0.5}]}","title":"Default Postprocessors in Azimuth"},{"location":"reference/custom-objects/postprocessors/#model-with-its-own-postprocessing","text":"If your model already includes postprocessing, as explained in Define a Model , the model prediction signature needs to have a specific output, PipelineOutputProtocol , that will contain both the predictions from the model, and the post-processed predictions. from typing import Protocol from azimuth.utils.ml.postprocessing import PostProcessingIO class PipelineOutputProtocol ( Protocol ): \"\"\"Class containing result of a batch\"\"\" model_output : PostProcessingIO # (1) postprocessor_output : PostProcessingIO # (2) model output before passing through post-processing stage: texts, logits, probs, preds output after passing through post-processing: pre-processed texts, logits, probs, preds PostProcessingIO is defined as the following. from typing import List from azimuth.types import AliasModel , Array class PostProcessingIO ( AliasModel ): texts : List [ str ] # (1) logits : Array [ float ] # (2) preds : Array [ int ] # (3) probs : Array [ float ] # (4) The utterance text. Length of N Logits of the model. Shape of (N, C) Predicted class, argmax of probabilities. Shape of (N, 1) Probabilities of the model # Shape of (N, C) In your code, you don't have to extend PipelineOutputProtocol or PostProcessingIO ; you can use your own library, and as long as the fields match, Azimuth will accept it. This is done so that our users don't have to add Azimuth as a dependency. @dataclass class PostprocessingData : texts : List [ str ] # len [num_samples] logits : np . ndarray # shape [num_samples, classes] preds : np . ndarray # shape [num_samples] probs : np . ndarray # shape [num_samples, classes] ### Valid class MyPipelineOutput ( BaseModel ): # Could be dataclass as well. model_output : PostprocessingData postprocessor_output : PostprocessingData ### Invalid because the field names do not match class MyPipelineOutput ( BaseModel ): predictions : MyPipelineOutput postprocessed_predictions : MyPipelineOutput","title":"Model with its own postprocessing"},{"location":"reference/custom-objects/postprocessors/#in-the-config","text":"{\"postprocessors\": null} should then be added to the config, to avoid re-postprocessing in Azimuth.","title":"In the Config"},{"location":"reference/custom-objects/postprocessors/#user-defined-postprocessors","text":"Similarly to a model and a dataset, users can add their own postprocessors in Azimuth with custom objects. However, some typing needs to be respected for Azimuth to handle it. First, the post-processing class needs PostProcessingIO , as defined above, as both input and output. To get consistent results, all values need to be updated by the post-processors. For example, if a postprocessor modifies the logits , it must recompute probs as well. The API for a postprocessor is the following: from azimuth.utils.ml.postprocessing import PostProcessingIO def __call__ ( self , post_processing_io : PostProcessingIO ) -> PostProcessingIO : ... You can also extend azimuth.utils.ml.postprocessing.Postprocessing to write your own postprocessor.","title":"User-Defined Postprocessors"},{"location":"reference/custom-objects/postprocessors/#example","text":"Let's define a postprocessor that will do Temperature scaling: azimuth_shr/loading_resources.py Configuration file from azimuth.functional.postprocessing import PostProcessingIO from scipy.special import expit , softmax class TemperatureScaling : def __init__ ( self , temperature ): self . temperature = temperature def __call__ ( self , post_processing_io : PostProcessingIO ) -> PostProcessingIO : new_logits = post_processing_io . logits / self . temperature confidences = ( softmax ( new_logits , axis = 1 ) if post_processing_io . is_multiclass else expit ( new_logits ) ) return PostProcessingIO ( texts = post_processing_io . texts , logits = new_logits , preds = post_processing_io . preds , probs = confidences , ) \"pipelines\" : [ { \"model\" : ... , \"postprocessors\" : [ { \"class_name\" : \"loading_resources.TemperatureScaling\" , \"remote\" : \"/azimuth_shr\" , \"kwargs\" : { \"temperature\" : 3 } } ] } ]","title":"Example"},{"location":"user-guide/","text":"Dashboard Welcome to Azimuth! Explore the different analyses and tools of Azimuth using the dashboard. Navigate through the different sections to get a deeper understanding of the dataset and the pipeline. Use Azimuth with no pipeline, or with multiple pipelines Azimuth can be launched without any pipelines. All the information related to the pipelines (prediction, behavioral testing and so on) will then be unavailable on all screens. It can also be launched with multiple pipelines. Use the dropdown in the top banner to switch between pipelines. Top Banner The top banner contains useful information and links. The project name from the config file is shown. A dropdown allows you to select the different pipelines defined in the config. It also allows you to select no pipelines. The settings allow you to enable/disable different analyses. A link to the support Slack channel and to the documentation is available in the help option. Don't miss out on the exploration space! At the top, access the Exploration Space to explore and interact with the utterances and the predictions. Dataset Warnings The dataset warnings section highlights issues related to class size , class imbalance and dataset shift , i.e. differences between the data distributions of the training and the evaluation sets. Missing samples : Verify if each intent has sufficient samples in both sets. Class imbalance : Flag when some classes suffer from imbalance in either split. Representation mismatch : Assess that the representation of each intent is similar in both sets. Length mismatch : Verify that the utterances' length are similar for each intent in both sets. Select View Details to get to Dataset Warnings . Pipeline Metrics by Data Subpopulation This section summarizes the quality of the predictions in terms of the prediction outcomes and metrics available in Azimuth, for different data subpopulations. Change the value in the dropdown to see the metrics broken down per label, predicted class, or smart tag families. Use the toggle to alternate between the training set or on the evaluation set. The user can click on any row in the table to get directly to the exploration space with the corresponding filters applied. This allows for further investigation of errors. As an example, clicking on the row with the label freeze_account will bring the user to the exploration space with that same filter applied. This works with prediction classes and smart tags too. Click on Compare pipelines to display the table fullscreen and compare all metrics across pipelines, as explained in Pipeline Metrics Comparison . Sort the table and hide columns Click a column header to sort the values in ascending or descending order. The default order is descending by the number of utterances, except for NO_PREDICTION / NO_SMART_TAGS which will be first. overall always stay at the top. Beside each column header, click the vertical dots to hide the corresponding column, or multiple ones by selecting 'Show columns'. However, the table columns will reappear if the page is refreshed. Go to the exploration space to interact with metrics The same metrics are available on the Exploration Space , where you can filter by any combination of values, and see more information on what each metric represents. Smart Tag Analysis The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes , along with sample counts and prediction accuracies. Use the dropdown to switch between values for labels or for predictions. Use the toggle to alternate between the training and evaluation sets. The Transpose toggle transposes the table and thus the axes for each bar plot. The default view aides analysis of each smart tag across all classes, whereas the transposed view makes it easier to investigate the smart tag pattern for a specific class. Select View details to get to Smart Tag Analysis . Sort the table by bar plot columns Click a column header (or row label if transposed) to sort the values in ascending or descending order. This works for bar plot columns as well as numerical columns. The default order is descending by the number of utterances, except for the rejection class, which will be first. Go to the exploration space to see samples Clicking on a bar takes you to the exploration space with corresponding filters applied, where you can further explore the tagged samples, including the specific smart tags applied. Behavioral Testing The Behavioral Testing section summarizes the behavioral testing performance. The failure rates on both the evaluation set and the training set highlight the ratio of failed tests to the total amount of tests. Click the failure rates to alternate between the performance on the training set or on the evaluation set. Select View details to get to Behavioral Testing Summary , which provides more information on tests and the option to export the results. Scrollable table The data is ordered in descending order by failure rate. The table is scrollable. File-based configurations With file-based configurations, the behavioral tests are generated and can be exported. However, since the tool does not have access to the model, predictions cannot be made for the modified utterances. As such, by default the tests have a failure rate of 0% (the new prediction is hard-coded to the original value). Post-processing Analysis The Post-processing Analysis provides an assessment of the performance of one post-processing step: the thresholding. The visualization shows the prediction outcome count on the evaluation set for different thresholds. Click View Details to see the plot full screen in Post-processing Analysis . Only available for some configs This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at null .","title":"Dashboard"},{"location":"user-guide/#dashboard","text":"Welcome to Azimuth! Explore the different analyses and tools of Azimuth using the dashboard. Navigate through the different sections to get a deeper understanding of the dataset and the pipeline. Use Azimuth with no pipeline, or with multiple pipelines Azimuth can be launched without any pipelines. All the information related to the pipelines (prediction, behavioral testing and so on) will then be unavailable on all screens. It can also be launched with multiple pipelines. Use the dropdown in the top banner to switch between pipelines.","title":"Dashboard"},{"location":"user-guide/#top-banner","text":"The top banner contains useful information and links. The project name from the config file is shown. A dropdown allows you to select the different pipelines defined in the config. It also allows you to select no pipelines. The settings allow you to enable/disable different analyses. A link to the support Slack channel and to the documentation is available in the help option. Don't miss out on the exploration space! At the top, access the Exploration Space to explore and interact with the utterances and the predictions.","title":"Top Banner"},{"location":"user-guide/#dataset-warnings","text":"The dataset warnings section highlights issues related to class size , class imbalance and dataset shift , i.e. differences between the data distributions of the training and the evaluation sets. Missing samples : Verify if each intent has sufficient samples in both sets. Class imbalance : Flag when some classes suffer from imbalance in either split. Representation mismatch : Assess that the representation of each intent is similar in both sets. Length mismatch : Verify that the utterances' length are similar for each intent in both sets. Select View Details to get to Dataset Warnings .","title":"Dataset Warnings"},{"location":"user-guide/#pipeline-metrics-by-data-subpopulation","text":"This section summarizes the quality of the predictions in terms of the prediction outcomes and metrics available in Azimuth, for different data subpopulations. Change the value in the dropdown to see the metrics broken down per label, predicted class, or smart tag families. Use the toggle to alternate between the training set or on the evaluation set. The user can click on any row in the table to get directly to the exploration space with the corresponding filters applied. This allows for further investigation of errors. As an example, clicking on the row with the label freeze_account will bring the user to the exploration space with that same filter applied. This works with prediction classes and smart tags too. Click on Compare pipelines to display the table fullscreen and compare all metrics across pipelines, as explained in Pipeline Metrics Comparison . Sort the table and hide columns Click a column header to sort the values in ascending or descending order. The default order is descending by the number of utterances, except for NO_PREDICTION / NO_SMART_TAGS which will be first. overall always stay at the top. Beside each column header, click the vertical dots to hide the corresponding column, or multiple ones by selecting 'Show columns'. However, the table columns will reappear if the page is refreshed. Go to the exploration space to interact with metrics The same metrics are available on the Exploration Space , where you can filter by any combination of values, and see more information on what each metric represents.","title":"Pipeline Metrics by Data Subpopulation"},{"location":"user-guide/#smart-tag-analysis","text":"The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes , along with sample counts and prediction accuracies. Use the dropdown to switch between values for labels or for predictions. Use the toggle to alternate between the training and evaluation sets. The Transpose toggle transposes the table and thus the axes for each bar plot. The default view aides analysis of each smart tag across all classes, whereas the transposed view makes it easier to investigate the smart tag pattern for a specific class. Select View details to get to Smart Tag Analysis . Sort the table by bar plot columns Click a column header (or row label if transposed) to sort the values in ascending or descending order. This works for bar plot columns as well as numerical columns. The default order is descending by the number of utterances, except for the rejection class, which will be first. Go to the exploration space to see samples Clicking on a bar takes you to the exploration space with corresponding filters applied, where you can further explore the tagged samples, including the specific smart tags applied.","title":"Smart Tag Analysis"},{"location":"user-guide/#behavioral-testing","text":"The Behavioral Testing section summarizes the behavioral testing performance. The failure rates on both the evaluation set and the training set highlight the ratio of failed tests to the total amount of tests. Click the failure rates to alternate between the performance on the training set or on the evaluation set. Select View details to get to Behavioral Testing Summary , which provides more information on tests and the option to export the results. Scrollable table The data is ordered in descending order by failure rate. The table is scrollable. File-based configurations With file-based configurations, the behavioral tests are generated and can be exported. However, since the tool does not have access to the model, predictions cannot be made for the modified utterances. As such, by default the tests have a failure rate of 0% (the new prediction is hard-coded to the original value).","title":"Behavioral Testing"},{"location":"user-guide/#post-processing-analysis","text":"The Post-processing Analysis provides an assessment of the performance of one post-processing step: the thresholding. The visualization shows the prediction outcome count on the evaluation set for different thresholds. Click View Details to see the plot full screen in Post-processing Analysis . Only available for some configs This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at null .","title":"Post-processing Analysis"},{"location":"user-guide/behavioral-testing-summary/","text":"Behavioral Testing Summary The Behavioral Testing Summary displays a report with the failure rates of each test , on both the evaluation set and the training set. Invariant (the modification should not change the predicted class) tests are currently supported. Behavioral Testing in Key Concepts gives detailed information on the tests. Table Content Each test belongs to a family and a name . The test categorization is then further broken down by modification type . The verb of the description is used to name the modification type. The failure rate (FR) indicates the number of total failed utterances, over all modified utterances generated for each test. Hover over the FR to see the average delta (absolute difference) in the prediction confidence, i.e. when the predicted class remains the same as the original utterance. The failure rate on the training set is the performance of the modified training set when tested with the trained model. This might be useful to test the robustness of the model on data points it has already seen, before taking the extra step of understanding robustness in the presence of new data. The last column shows one example from the dataset of the modifications that were made on the original utterance. Sort the table Click the failure rate headers to sort the values in ascending or descending order. Download A summary of the test results can be downloaded , as well as the modified sets generated for both the evaluation set and training set. Download the required file by clicking Export in the upper right corner of the table and then selecting the appropriate item from the menu. Augment custom utterance See our Custom Utterances page to learn how to augment additional data.","title":"Behavioral Testing Summary"},{"location":"user-guide/behavioral-testing-summary/#behavioral-testing-summary","text":"The Behavioral Testing Summary displays a report with the failure rates of each test , on both the evaluation set and the training set. Invariant (the modification should not change the predicted class) tests are currently supported. Behavioral Testing in Key Concepts gives detailed information on the tests.","title":"Behavioral Testing Summary"},{"location":"user-guide/behavioral-testing-summary/#table-content","text":"Each test belongs to a family and a name . The test categorization is then further broken down by modification type . The verb of the description is used to name the modification type. The failure rate (FR) indicates the number of total failed utterances, over all modified utterances generated for each test. Hover over the FR to see the average delta (absolute difference) in the prediction confidence, i.e. when the predicted class remains the same as the original utterance. The failure rate on the training set is the performance of the modified training set when tested with the trained model. This might be useful to test the robustness of the model on data points it has already seen, before taking the extra step of understanding robustness in the presence of new data. The last column shows one example from the dataset of the modifications that were made on the original utterance. Sort the table Click the failure rate headers to sort the values in ascending or descending order.","title":"Table Content"},{"location":"user-guide/behavioral-testing-summary/#download","text":"A summary of the test results can be downloaded , as well as the modified sets generated for both the evaluation set and training set. Download the required file by clicking Export in the upper right corner of the table and then selecting the appropriate item from the menu. Augment custom utterance See our Custom Utterances page to learn how to augment additional data.","title":"Download"},{"location":"user-guide/custom-utterances/","text":"Custom utterances Azimuth has limited support for \"custom utterances\", i.e. utterances not part of the initial dataset. Azimuth offers support through API routes accessible locally at http://0.0.0.0:8091/docs#/Custom%20Utterances%20v1 . Data augmentation To augment a list of utterances, you can follow these intructions: Python Output from pprint import pprint import requests utterances = [ \"welcome to Azimuth\" , \"Don't forget to star our repo!\" ] response = requests . get ( \"http://0.0.0.0:8091/custom_utterances/perturbed_utterances\" , params = { \"utterances\" : utterances }) . json () pprint ([ r [ \"perturbedUtterance\" ] for r in response ]) >>> [ 'pls welcome to Azimuth' , 'please welcome to Azimuth' , ..., 'Do not forget to star our repo!' ] Saliency If a pipeline allows it, you can get saliency of a custom utterance by doing the following: Python Output from pprint import pprint import requests utterances = [ \"welcome to Azimuth\" , \"Don't forget to star our repo!\" ] response = requests . get ( \"http://0.0.0.0:8091/custom_utterances/saliency\" , params = { \"utterances\" : utterances , \"pipeline_index\" : 0 }) . json () pprint ( response ) >>> [{ 'saliency' : [ 0 .08587087690830231, ... ] , 'tokens' : [ '[CLS]' , 'welcome' , 'to' , 'az' , '##im' , '##uth' , '[SEP]' ]} , ... ]","title":"Custom utterances"},{"location":"user-guide/custom-utterances/#custom-utterances","text":"Azimuth has limited support for \"custom utterances\", i.e. utterances not part of the initial dataset. Azimuth offers support through API routes accessible locally at http://0.0.0.0:8091/docs#/Custom%20Utterances%20v1 .","title":"Custom utterances"},{"location":"user-guide/custom-utterances/#data-augmentation","text":"To augment a list of utterances, you can follow these intructions: Python Output from pprint import pprint import requests utterances = [ \"welcome to Azimuth\" , \"Don't forget to star our repo!\" ] response = requests . get ( \"http://0.0.0.0:8091/custom_utterances/perturbed_utterances\" , params = { \"utterances\" : utterances }) . json () pprint ([ r [ \"perturbedUtterance\" ] for r in response ]) >>> [ 'pls welcome to Azimuth' , 'please welcome to Azimuth' , ..., 'Do not forget to star our repo!' ]","title":"Data augmentation"},{"location":"user-guide/custom-utterances/#saliency","text":"If a pipeline allows it, you can get saliency of a custom utterance by doing the following: Python Output from pprint import pprint import requests utterances = [ \"welcome to Azimuth\" , \"Don't forget to star our repo!\" ] response = requests . get ( \"http://0.0.0.0:8091/custom_utterances/saliency\" , params = { \"utterances\" : utterances , \"pipeline_index\" : 0 }) . json () pprint ( response ) >>> [{ 'saliency' : [ 0 .08587087690830231, ... ] , 'tokens' : [ '[CLS]' , 'welcome' , 'to' , 'az' , '##im' , '##uth' , '[SEP]' ]} , ... ]","title":"Saliency"},{"location":"user-guide/dataset-warnings/","text":"Dataset Warnings Datasets can suffer from a variety of issues, such as class imbalance, classes with low sample counts, and dataset shift. These warnings help detect some of these issues. Missing samples In this first analysis, the application flags when a class has fewer than X (default is 20) samples in either the training or the evaluation set. The plot helps to visualize the values for each class. Class Imbalance In this second analysis, Azimuth detects class imbalance issues. It raises a flag for all classes where the relative difference between the number of samples in that class and the mean sample count per class in a dataset split is above a certain threshold Y . The default is 50%. Dataset Shift A discrepancy between the training and evaluation splits can cause problems with a model. For example, the model may not have a representative sample of examples to train on, making it generalize poorly in production. Alternatively, if your evaluation set does not come from the same data distribution as the data in production, measuring model performance on this evaluation set may not be a good indicator of the performance in production . Distribution analysis aims to give warnings when the training and evaluation sets look too different in some aspect of the data. Representation mismatch This analysis flags when a class is over-represented in the evaluation set (relative to other classes) or the training set. If the delta between the percentage of a class in each set is above Z % (default is 5%), the analysis flags it. Length mismatch Length mismatch compares the number of tokens per utterance in both sets. The application flags a warning if the mean and/or standard deviation between the 2 distributions is above A and B ( default is 3 for both) respectively. Configuration All thresholds mentioned ( X / Y / Z / A / B ) can be modified in the config file, as explained in Dataset Warnings Configuration .","title":"Dataset Warnings"},{"location":"user-guide/dataset-warnings/#dataset-warnings","text":"Datasets can suffer from a variety of issues, such as class imbalance, classes with low sample counts, and dataset shift. These warnings help detect some of these issues.","title":"Dataset Warnings"},{"location":"user-guide/dataset-warnings/#missing-samples","text":"In this first analysis, the application flags when a class has fewer than X (default is 20) samples in either the training or the evaluation set. The plot helps to visualize the values for each class.","title":"Missing samples"},{"location":"user-guide/dataset-warnings/#class-imbalance","text":"In this second analysis, Azimuth detects class imbalance issues. It raises a flag for all classes where the relative difference between the number of samples in that class and the mean sample count per class in a dataset split is above a certain threshold Y . The default is 50%.","title":"Class Imbalance"},{"location":"user-guide/dataset-warnings/#dataset-shift","text":"A discrepancy between the training and evaluation splits can cause problems with a model. For example, the model may not have a representative sample of examples to train on, making it generalize poorly in production. Alternatively, if your evaluation set does not come from the same data distribution as the data in production, measuring model performance on this evaluation set may not be a good indicator of the performance in production . Distribution analysis aims to give warnings when the training and evaluation sets look too different in some aspect of the data.","title":"Dataset Shift"},{"location":"user-guide/dataset-warnings/#representation-mismatch","text":"This analysis flags when a class is over-represented in the evaluation set (relative to other classes) or the training set. If the delta between the percentage of a class in each set is above Z % (default is 5%), the analysis flags it.","title":"Representation mismatch"},{"location":"user-guide/dataset-warnings/#length-mismatch","text":"Length mismatch compares the number of tokens per utterance in both sets. The application flags a warning if the mean and/or standard deviation between the 2 distributions is above A and B ( default is 3 for both) respectively.","title":"Length mismatch"},{"location":"user-guide/dataset-warnings/#configuration","text":"All thresholds mentioned ( X / Y / Z / A / B ) can be modified in the config file, as explained in Dataset Warnings Configuration .","title":"Configuration"},{"location":"user-guide/pipeline-metrics-comparison/","text":"Pipeline Metrics Comparison This table summarizes the quality of the predictions for different data subpopulations. When selecting a second model next to Compare Baseline with , the table will display the metrics for both the baseline and the second model, along with delta columns that show the difference for each metric and subpopulation. All interactions available from the Dashboard are also available here, including the ability to sort and hide columns, and to click on a row.","title":"Pipeline Metrics Comparison"},{"location":"user-guide/pipeline-metrics-comparison/#pipeline-metrics-comparison","text":"This table summarizes the quality of the predictions for different data subpopulations. When selecting a second model next to Compare Baseline with , the table will display the metrics for both the baseline and the second model, along with delta columns that show the difference for each metric and subpopulation. All interactions available from the Dashboard are also available here, including the ability to sort and hide columns, and to click on a row.","title":"Pipeline Metrics Comparison"},{"location":"user-guide/post-processing-analysis/","text":"Post-processing Analysis Threshold Comparison With the Threshold Comparison page, you can compare the performance of the model on the evaluation set at different threshold values . The visualization shows the performance for threshold values between 0 and 95%, with increments of 5%. A suggested minimum amount of correct predictions, as well as a maximum amount of incorrect predictions, are displayed on the plot. Only available for some configs This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at null .","title":"Post-processing Analysis"},{"location":"user-guide/post-processing-analysis/#post-processing-analysis","text":"","title":"Post-processing Analysis"},{"location":"user-guide/post-processing-analysis/#threshold-comparison","text":"With the Threshold Comparison page, you can compare the performance of the model on the evaluation set at different threshold values . The visualization shows the performance for threshold values between 0 and 95%, with increments of 5%. A suggested minimum amount of correct predictions, as well as a maximum amount of incorrect predictions, are displayed on the plot. Only available for some configs This section is only available when the threshold is known and can be edited. This means it is unavailable for file-based configs, and for pipelines with their own postprocessors, i.e. when postprocessors are set at null .","title":"Threshold Comparison"},{"location":"user-guide/settings/","text":"Settings Speed-up the Start-Up Disable behavioral testing and similarity analysis in the config file to increase the start-up speed, as explained in Behavioral Testing Configuration and Similarity Configuration . Enable them through settings later on without restarting the app.","title":"Settings"},{"location":"user-guide/settings/#settings","text":"","title":"Settings"},{"location":"user-guide/settings/#speed-up-the-start-up","text":"Disable behavioral testing and similarity analysis in the config file to increase the start-up speed, as explained in Behavioral Testing Configuration and Similarity Configuration . Enable them through settings later on without restarting the app.","title":"Speed-up the Start-Up"},{"location":"user-guide/smart-tag-analysis/","text":"Smart Tag Analysis The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes , along with sample counts and prediction accuracies. The analyses associated with each smart tag family may also be associated with a specific model behavior, failure mode, and/or approach to address any issues. For example, Dissimilar smart tags are associated with class definition issues, such as class overlap, and may require actions such as relabeling training samples or augmenting the training data. Partial Syntax smart tags can help you assess whether any syntactical patterns are associated with greater misclassification, which might warrant data augmentation. To further explore a pattern, including investigating individual samples and smart tags, click on a bar to go to the exploration space with the corresponding filters applied. This can help guide any actions required to address a problem. For example, examining misclassified samples tagged no_close_train can help direct targeted data augmentation, if necessary. Smart Tags in Key Concepts gives information on the smart tags included in each smart tag family, with links to more information on the analyses associated with each. Table Details The table is described in the default view; in the transposed view, references to columns should be interpreted as references to rows. Table controls Use the toggle to alternate between the training and evaluation sets. The Transpose toggle transposes the table and thus the axes for each bar plot. The default view aides analysis of each smart tag across all classes, which may be a good starting point for assessing overall dataset and pipeline patterns. In contrast, the transposed view groups bars for each class on its own axis. This makes it easier to investigate patterns for a specific class that has been identified as needing further analysis, such as one with greater misclassification rates. Columns The first column shows the class variable for which other values are presented. Use the dropdown to switch between labels and predictions. The second and third columns show sample count and pipeline accuracy, which can help with identifying or prioritizing classes to investigate. For example, you may want to sort by accuracy in ascending order, to focus on classes for which the model had more difficulty. The remaining columns show bar plots across the selected class variable for each smart tag family. Colors indicate prediction outcomes , and column sorting acts on the full stacked bar.","title":"Smart Tag Analysis"},{"location":"user-guide/smart-tag-analysis/#smart-tag-analysis","text":"The Smart Tag Analysis shows the proportion of samples that have been tagged by each smart tag family, broken down by prediction outcomes , along with sample counts and prediction accuracies. The analyses associated with each smart tag family may also be associated with a specific model behavior, failure mode, and/or approach to address any issues. For example, Dissimilar smart tags are associated with class definition issues, such as class overlap, and may require actions such as relabeling training samples or augmenting the training data. Partial Syntax smart tags can help you assess whether any syntactical patterns are associated with greater misclassification, which might warrant data augmentation. To further explore a pattern, including investigating individual samples and smart tags, click on a bar to go to the exploration space with the corresponding filters applied. This can help guide any actions required to address a problem. For example, examining misclassified samples tagged no_close_train can help direct targeted data augmentation, if necessary. Smart Tags in Key Concepts gives information on the smart tags included in each smart tag family, with links to more information on the analyses associated with each.","title":"Smart Tag Analysis"},{"location":"user-guide/smart-tag-analysis/#table-details","text":"The table is described in the default view; in the transposed view, references to columns should be interpreted as references to rows.","title":"Table Details"},{"location":"user-guide/smart-tag-analysis/#table-controls","text":"Use the toggle to alternate between the training and evaluation sets. The Transpose toggle transposes the table and thus the axes for each bar plot. The default view aides analysis of each smart tag across all classes, which may be a good starting point for assessing overall dataset and pipeline patterns. In contrast, the transposed view groups bars for each class on its own axis. This makes it easier to investigate patterns for a specific class that has been identified as needing further analysis, such as one with greater misclassification rates.","title":"Table controls"},{"location":"user-guide/smart-tag-analysis/#columns","text":"The first column shows the class variable for which other values are presented. Use the dropdown to switch between labels and predictions. The second and third columns show sample count and pipeline accuracy, which can help with identifying or prioritizing classes to investigate. For example, you may want to sort by accuracy in ascending order, to focus on classes for which the model had more difficulty. The remaining columns show bar plots across the selected class variable for each smart tag family. Colors indicate prediction outcomes , and column sorting acts on the full stacked bar.","title":"Columns"},{"location":"user-guide/exploration-space/","text":"Introduction to Exploration Space The Exploration Space includes the datasets and predictions of your model in an interactive way. Explore the utterances and the predictions, spot patterns in errors, and annotate the data to indicate further work to improve the model's predictions. Access from dashboard Access this space through the header of the dashboard. Views Interact with the Exploration Space in different views. Filter the data on any view with the control panel. Both training and evaluation sets can be explored. Pipeline dropdown Don't forget the pipeline dropdown in the top banner to see how the performance of different pipelines compare. This space also exists without selecting any pipelines, to perform dataset analysis. Prediction Overview Assess the quality of the metrics for any given subset of the data. Visualize the distribution of the confidence scores, according to prediction outcome. See the most important words from the utterances, according to prediction outcome. Confusion Matrix Visualize the model confusion between each pair of intents. Utterance Table Explore the utterances, with their labels, predictions, and smart tags. Access all utterance details, including the detailed prediction results, the behavioral tests, and the most similar utterances. Annotate utterances to identify further work. Control Panel All views are affected by the control panel. Share and save the filters The control panel selection can be saved and shared by using the url, which contains the selected filters and options. It can also be collapsed to leave more space for the rest of the content. Evaluation Set and Training Set Explore the views on both the evaluation and the training set by clicking the toggle. Clicking on a tab will update the view. Excluding Post-Processing By default, the predictions are shown after the ML pipeline, including the model prediction and the post-processing steps (temperature scaling, thresholding, or else). To analyze the model predictions only, you can exclude the post-processing steps in the exploration space. This will affect the displayed predictions, as well as related computations: the metrics, the confidence histogram and the confusion matrix. This will not affect the smart tags, as these would be too long to recompute. Filters Filter the data on any view according to different dimensions of the data, such as the model's confidence, the utterances label, or the smart tags. The available categories and behaviors of filters are listed below. Filter Categories Search a particular string to filter utterances that contain it. Filter predictions based on their confidence value . You can specify a minimum and a maximum value. Filter predictions according to their prediction outcomes . Filter by labeled class (the target). Filter by predicted class . Filter by smart tags . Filter by user-applied proposed actions . Filters Behavior Bars Distribution The bar distributions to the right of the filters show the prediction outcome count for a given filter. Selecting or deselecting filters update the bars based on the current selection. Filters Selection The checkbox beside the name of each filter category can be used to select or deselect all corresponding filters. When selecting filters, the other filters that can no longer be selected will be greyed-out . Example: selecting a smart tag filter greys out the NO_SMART_TAGS option. The number of utterances currently selected by the filters is shown in parentheses at the top of the panel (beside the title Filters ). Click Clear filters to reset all filters to the default values. Negative filtering For smart tag families and proposed actions, you can filter based on the absence of all available filters within a category, respectively NO_SMART_TAGS and NO_ACTION . Search Use the search bar to find specific filters.","title":"Introduction to Exploration Space"},{"location":"user-guide/exploration-space/#introduction-to-exploration-space","text":"The Exploration Space includes the datasets and predictions of your model in an interactive way. Explore the utterances and the predictions, spot patterns in errors, and annotate the data to indicate further work to improve the model's predictions. Access from dashboard Access this space through the header of the dashboard.","title":"Introduction to Exploration Space"},{"location":"user-guide/exploration-space/#views","text":"Interact with the Exploration Space in different views. Filter the data on any view with the control panel. Both training and evaluation sets can be explored. Pipeline dropdown Don't forget the pipeline dropdown in the top banner to see how the performance of different pipelines compare. This space also exists without selecting any pipelines, to perform dataset analysis.","title":"Views"},{"location":"user-guide/exploration-space/#prediction-overview","text":"Assess the quality of the metrics for any given subset of the data. Visualize the distribution of the confidence scores, according to prediction outcome. See the most important words from the utterances, according to prediction outcome.","title":"Prediction Overview"},{"location":"user-guide/exploration-space/#confusion-matrix","text":"Visualize the model confusion between each pair of intents.","title":"Confusion Matrix"},{"location":"user-guide/exploration-space/#utterance-table","text":"Explore the utterances, with their labels, predictions, and smart tags. Access all utterance details, including the detailed prediction results, the behavioral tests, and the most similar utterances. Annotate utterances to identify further work.","title":"Utterance Table"},{"location":"user-guide/exploration-space/#control-panel","text":"All views are affected by the control panel. Share and save the filters The control panel selection can be saved and shared by using the url, which contains the selected filters and options. It can also be collapsed to leave more space for the rest of the content.","title":"Control Panel"},{"location":"user-guide/exploration-space/#evaluation-set-and-training-set","text":"Explore the views on both the evaluation and the training set by clicking the toggle. Clicking on a tab will update the view.","title":"Evaluation Set and Training Set"},{"location":"user-guide/exploration-space/#excluding-post-processing","text":"By default, the predictions are shown after the ML pipeline, including the model prediction and the post-processing steps (temperature scaling, thresholding, or else). To analyze the model predictions only, you can exclude the post-processing steps in the exploration space. This will affect the displayed predictions, as well as related computations: the metrics, the confidence histogram and the confusion matrix. This will not affect the smart tags, as these would be too long to recompute.","title":"Excluding Post-Processing"},{"location":"user-guide/exploration-space/#filters","text":"Filter the data on any view according to different dimensions of the data, such as the model's confidence, the utterances label, or the smart tags. The available categories and behaviors of filters are listed below.","title":"Filters"},{"location":"user-guide/exploration-space/#filter-categories","text":"Search a particular string to filter utterances that contain it. Filter predictions based on their confidence value . You can specify a minimum and a maximum value. Filter predictions according to their prediction outcomes . Filter by labeled class (the target). Filter by predicted class . Filter by smart tags . Filter by user-applied proposed actions .","title":"Filter Categories"},{"location":"user-guide/exploration-space/#filters-behavior","text":"","title":"Filters Behavior"},{"location":"user-guide/exploration-space/#bars-distribution","text":"The bar distributions to the right of the filters show the prediction outcome count for a given filter. Selecting or deselecting filters update the bars based on the current selection.","title":"Bars Distribution"},{"location":"user-guide/exploration-space/#filters-selection","text":"The checkbox beside the name of each filter category can be used to select or deselect all corresponding filters. When selecting filters, the other filters that can no longer be selected will be greyed-out . Example: selecting a smart tag filter greys out the NO_SMART_TAGS option. The number of utterances currently selected by the filters is shown in parentheses at the top of the panel (beside the title Filters ). Click Clear filters to reset all filters to the default values.","title":"Filters Selection"},{"location":"user-guide/exploration-space/#negative-filtering","text":"For smart tag families and proposed actions, you can filter based on the absence of all available filters within a category, respectively NO_SMART_TAGS and NO_ACTION .","title":"Negative filtering"},{"location":"user-guide/exploration-space/#search","text":"Use the search bar to find specific filters.","title":"Search"},{"location":"user-guide/exploration-space/confusion-matrix/","text":"Confusion Matrix The confusion matrix displays the model confusion between each pair of intents . The confusion is defined as the number of utterances with a given label that are predicted as another label. Normalization The toggle \"Normalize\" in the top right corner allows alternating between normalized and raw values. When normalized, the number of utterances is divided by to the total number of utterances with the given label. Class Ordering The default order for the rows and columns is determined based on the reverse Cuthill-Mckee algorithm, which will group as many classes as possible with similar confusion. The algorithm ignores all confusion values under 10%. The rejection class is also ignored and is always the last one in the order. Toggling off \"Reorder classes\" disables the reordering and allows showing the confusion matrix according to the class order provided by the user. Outcome colors The prediction outcome colors are shown on the confusion matrix. Example In this example, 45% of utterances labeled as bill_due were predicted as bill_balance .","title":"Confusion Matrix"},{"location":"user-guide/exploration-space/confusion-matrix/#confusion-matrix","text":"The confusion matrix displays the model confusion between each pair of intents . The confusion is defined as the number of utterances with a given label that are predicted as another label.","title":"Confusion Matrix"},{"location":"user-guide/exploration-space/confusion-matrix/#normalization","text":"The toggle \"Normalize\" in the top right corner allows alternating between normalized and raw values. When normalized, the number of utterances is divided by to the total number of utterances with the given label.","title":"Normalization"},{"location":"user-guide/exploration-space/confusion-matrix/#class-ordering","text":"The default order for the rows and columns is determined based on the reverse Cuthill-Mckee algorithm, which will group as many classes as possible with similar confusion. The algorithm ignores all confusion values under 10%. The rejection class is also ignored and is always the last one in the order. Toggling off \"Reorder classes\" disables the reordering and allows showing the confusion matrix according to the class order provided by the user. Outcome colors The prediction outcome colors are shown on the confusion matrix. Example In this example, 45% of utterances labeled as bill_due were predicted as bill_balance .","title":"Class Ordering"},{"location":"user-guide/exploration-space/prediction-overview/","text":"Prediction Overview The Prediction Overview centralizes the metrics , the confidence histogram and two word clouds to show important words from the utterances. Metrics Assess the quality of the model with different metrics. Hover over the information icon to see more information on each metric. The first tile corresponds to the performance based on prediction outcomes : Correct & Predicted Correct & Rejected Incorrect & Rejected Incorrect & Predicted The second tile contains the precision , recall and F1 . The last tile shows the Expected Calibration Error (ECE), which indicates the quality of the model's calibration. An ECE of 0 means perfect calibration; A lower ECE is better. Hover the ECE to show a plot that displays the breakdown of the ECE computation per bin. Confidence Histogram The Confidence Histogram displays the distribution of model confidences, grouped-by prediction outcomes . The threshold, if set in the Project Configuration , is displayed on the plot. Assess the distribution Assess the confidence distribution by looking at the shape of the curve, the min and max values, and the ratio in each bin. Look at the histogram for any subset of the data by using the control panel. Word Clouds To the right of the histogram is a word cloud showing the most important words for correct and incorrect predictions . Correct predictions include Correct & Predicted and Correct & Rejected , while incorrect predictions include Incorrect & Predicted and Incorrect & Rejected . The word clouds change depending on the filters and indicate the frequency of each important word across all filtered utterances. Clicking on a word will filter utterances that contain it. Clicking it a second time will clear that search filter, so does the \u2717 in the Search utterances filter. What does the word cloud count? If saliency maps are available, the word clouds show the count of salient words . A word is considered salient in an utterance if the sum of its tokens' saliency values is greater than 60% of the largest saliency value in the utterance. If saliency maps are not available, the word clouds only show the most frequent words which are not part of a pre-defined list of stop words (from nltk.corpus ).","title":"Prediction Overview"},{"location":"user-guide/exploration-space/prediction-overview/#prediction-overview","text":"The Prediction Overview centralizes the metrics , the confidence histogram and two word clouds to show important words from the utterances.","title":"Prediction Overview"},{"location":"user-guide/exploration-space/prediction-overview/#metrics","text":"Assess the quality of the model with different metrics. Hover over the information icon to see more information on each metric. The first tile corresponds to the performance based on prediction outcomes : Correct & Predicted Correct & Rejected Incorrect & Rejected Incorrect & Predicted The second tile contains the precision , recall and F1 . The last tile shows the Expected Calibration Error (ECE), which indicates the quality of the model's calibration. An ECE of 0 means perfect calibration; A lower ECE is better. Hover the ECE to show a plot that displays the breakdown of the ECE computation per bin.","title":"Metrics"},{"location":"user-guide/exploration-space/prediction-overview/#confidence-histogram","text":"The Confidence Histogram displays the distribution of model confidences, grouped-by prediction outcomes . The threshold, if set in the Project Configuration , is displayed on the plot. Assess the distribution Assess the confidence distribution by looking at the shape of the curve, the min and max values, and the ratio in each bin. Look at the histogram for any subset of the data by using the control panel.","title":"Confidence Histogram"},{"location":"user-guide/exploration-space/prediction-overview/#word-clouds","text":"To the right of the histogram is a word cloud showing the most important words for correct and incorrect predictions . Correct predictions include Correct & Predicted and Correct & Rejected , while incorrect predictions include Incorrect & Predicted and Incorrect & Rejected . The word clouds change depending on the filters and indicate the frequency of each important word across all filtered utterances. Clicking on a word will filter utterances that contain it. Clicking it a second time will clear that search filter, so does the \u2717 in the Search utterances filter. What does the word cloud count? If saliency maps are available, the word clouds show the count of salient words . A word is considered salient in an utterance if the sum of its tokens' saliency values is greater than 60% of the largest saliency value in the utterance. If saliency maps are not available, the word clouds only show the most frequent words which are not part of a pre-defined list of stop words (from nltk.corpus ).","title":"Word Clouds"},{"location":"user-guide/exploration-space/utterance-details/","text":"Utterance Details The utterance detail page shows more information about individual utterances . See the including saliency per token, semantically similar utterances, and model performance on behavioral tests automatically generated from the utterance. Utterance Information The top section shows the utterance details: Utterance : The utterance and its per-token saliency (if available for the model's architecture). Hover over the tokens to the see the associated saliency value. Label : The utterance's labeled class (the target). Predictions : The top 3 predictions for this utterance are shown. The top prediction is highlighted using the prediction outcome color. If the top prediction was converted to the rejection class based on the threshold, it will be added as a 4th element at the top. Smart tags (where applicable): An automatically computed tag highlighting a certain characteristic of the utterance (e.g., long sentences, utterance is missing a verb, utterance contains multiple sentences). For more information, see Smart Tags . Proposed Action (editable): You can add a proposed action to identify further steps required for an incorrectly predicted utterance. For more information, see Proposed Actions . Semantically Similar Utterances The Semantically Similar Utterances tab shows the top 20 most similar utterances in the dataset as calculated using sentence embeddings. For more information, see Similarity Analysis . The toggle buttons control whether to search for similar utterances in the evaluation set or the training set. Utterances that are similar but have different labels or predictions can indicate possible problems with the dataset or the model. An icon indicates utterances that are from a different class than the base utterance. Clicking on the row of an utterance in the table will open the details page for that utterance. Behavioral Tests The Behavioral Tests tab displays the result of the behavioral tests that were run to test the model's robustness to minor modifications . A failing test occurs when a modified utterance has a different predicted class or when prediction certainty is altered by more than a set threshold. For more information, see Behavioral Tests . Only available when a pipeline is defined Behavioral tests are only available when a pipeline is available. As such, the tab will be empty for file-based configurations.","title":"Utterance Details"},{"location":"user-guide/exploration-space/utterance-details/#utterance-details","text":"The utterance detail page shows more information about individual utterances . See the including saliency per token, semantically similar utterances, and model performance on behavioral tests automatically generated from the utterance.","title":"Utterance Details"},{"location":"user-guide/exploration-space/utterance-details/#utterance-information","text":"The top section shows the utterance details: Utterance : The utterance and its per-token saliency (if available for the model's architecture). Hover over the tokens to the see the associated saliency value. Label : The utterance's labeled class (the target). Predictions : The top 3 predictions for this utterance are shown. The top prediction is highlighted using the prediction outcome color. If the top prediction was converted to the rejection class based on the threshold, it will be added as a 4th element at the top. Smart tags (where applicable): An automatically computed tag highlighting a certain characteristic of the utterance (e.g., long sentences, utterance is missing a verb, utterance contains multiple sentences). For more information, see Smart Tags . Proposed Action (editable): You can add a proposed action to identify further steps required for an incorrectly predicted utterance. For more information, see Proposed Actions .","title":"Utterance Information"},{"location":"user-guide/exploration-space/utterance-details/#semantically-similar-utterances","text":"The Semantically Similar Utterances tab shows the top 20 most similar utterances in the dataset as calculated using sentence embeddings. For more information, see Similarity Analysis . The toggle buttons control whether to search for similar utterances in the evaluation set or the training set. Utterances that are similar but have different labels or predictions can indicate possible problems with the dataset or the model. An icon indicates utterances that are from a different class than the base utterance. Clicking on the row of an utterance in the table will open the details page for that utterance.","title":"Semantically Similar Utterances"},{"location":"user-guide/exploration-space/utterance-details/#behavioral-tests","text":"The Behavioral Tests tab displays the result of the behavioral tests that were run to test the model's robustness to minor modifications . A failing test occurs when a modified utterance has a different predicted class or when prediction certainty is altered by more than a set threshold. For more information, see Behavioral Tests . Only available when a pipeline is defined Behavioral tests are only available when a pipeline is available. As such, the tab will be empty for file-based configurations.","title":"Behavioral Tests"},{"location":"user-guide/exploration-space/utterance-table/","text":"Utterance Table The utterance table view contains all the utterances with their predictions . The table also includes information such as smart tags and proposed actions, if applicable. To see more details on an utterance , click any row, which will open the Utterance Details page. Open in a new tab Open the utterance details in a new tab or in a new window using the Ctrl or Cmd + click or right-click menu. Table Content Sort the table Click a column header to sort the table by the column values. Each click rotates between ascending order, descending order, and no sorting. ID A unique ID for each utterance is created for referencing purposes. When exporting the utterances, the utterance ID refers to the column row_idx . Utterance Utterances appear as available in the dataset provided. Hover over the utterances to display the copy button . If available, the utterances are overlaid with saliency maps , highlighting the most important tokens for the model's prediction. You can see the raw saliency values when hovering on the utterance tokens in the Utterance Details only. For more information on how these values are calculated see Saliency Map . Prediction Information The table shows the labels , predicted classes , and confidences for the utterances. If the confidence is below the prediction threshold, the table reads NO_PREDICTION and the original prediction in parentheses. Smart Tags The table shows smart tag families, as well as the specific smart tag values on hover. For more information, see Smart Tags . Proposed Action For each data point, the user can specify if an action needs to be taken. Proposed Actions are explained in the Key Concepts section. The actions are done outside the app. Export the proposed actions in a .csv file and use the list to resolve the utterance issues. The exported file also contains the smart tags. Apply in batch Proposed actions can be applied in batches by selecting multiple rows (or selecting all based on the current search) and applying the change.","title":"Utterance Table"},{"location":"user-guide/exploration-space/utterance-table/#utterance-table","text":"The utterance table view contains all the utterances with their predictions . The table also includes information such as smart tags and proposed actions, if applicable. To see more details on an utterance , click any row, which will open the Utterance Details page. Open in a new tab Open the utterance details in a new tab or in a new window using the Ctrl or Cmd + click or right-click menu.","title":"Utterance Table"},{"location":"user-guide/exploration-space/utterance-table/#table-content","text":"Sort the table Click a column header to sort the table by the column values. Each click rotates between ascending order, descending order, and no sorting.","title":"Table Content"},{"location":"user-guide/exploration-space/utterance-table/#id","text":"A unique ID for each utterance is created for referencing purposes. When exporting the utterances, the utterance ID refers to the column row_idx .","title":"ID"},{"location":"user-guide/exploration-space/utterance-table/#utterance","text":"Utterances appear as available in the dataset provided. Hover over the utterances to display the copy button . If available, the utterances are overlaid with saliency maps , highlighting the most important tokens for the model's prediction. You can see the raw saliency values when hovering on the utterance tokens in the Utterance Details only. For more information on how these values are calculated see Saliency Map .","title":"Utterance"},{"location":"user-guide/exploration-space/utterance-table/#prediction-information","text":"The table shows the labels , predicted classes , and confidences for the utterances. If the confidence is below the prediction threshold, the table reads NO_PREDICTION and the original prediction in parentheses.","title":"Prediction Information"},{"location":"user-guide/exploration-space/utterance-table/#smart-tags","text":"The table shows smart tag families, as well as the specific smart tag values on hover. For more information, see Smart Tags .","title":"Smart Tags"},{"location":"user-guide/exploration-space/utterance-table/#proposed-action","text":"For each data point, the user can specify if an action needs to be taken. Proposed Actions are explained in the Key Concepts section. The actions are done outside the app. Export the proposed actions in a .csv file and use the list to resolve the utterance issues. The exported file also contains the smart tags. Apply in batch Proposed actions can be applied in batches by selecting multiple rows (or selecting all based on the current search) and applying the change.","title":"Proposed Action"}]}